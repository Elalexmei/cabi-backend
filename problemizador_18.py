import re
import json
import os
import pandas
from typing import Dict, List, Optional, Set, Tuple, Union
from dataclasses import dataclass, field, asdict
from datetime import datetime, timedelta
from pathlib import Path
from enum import Enum
from pathlib import Path




# -----------------------------------------------------------
# ---------------- PROBLEMIZADOR (NLP) ----------------------
# -----------------------------------------------------------

# codigo encargado de procesar el input del usuario, interpreta  la pregunta 
# escrita en lenguaje natural, funciona como un NLP (Natural Lenguaje Proccesing) 
# de tipo rule based ya que depende de reglas establecidas.



# ------ Rule Based NLP (Natural Lenguaje Proccesor ) -------

# 1 - ENTRADA Y VALIDACI√ìN -----‚Üí Recibe y valida input
# 2 - NORMALIZACI√ìN ------------‚Üí Limpia y estandariza texto
# 3 - TOKENIZACI√ìN Y PATRONES --‚Üí Divide y detecta patrones
# 4 - CLASIFICACI√ìN ------------‚Üí Identifica tipos sem√°nticos
# 5 - ESTRUCTURA SEM√ÅNTICA -----‚Üí Organiza componentes
# 6 - VALIDACI√ìN Y PATRONES ----‚Üí Verifica y clasifica intenci√≥n
# 7 - GENERACI√ìN SQL -----------‚Üí Convierte a c√≥digo ejecutable
# 8 - RESULTADO ----------------‚Üí Formatea respuesta final



# -------------------------
# ------ CONEXIONES -------
# -------------------------


# ------ Conexion con diccionario de sinonimos -------
# importamos las clases que contienen los diccionarios necesarios

class ComponentType(Enum):
    DIMENSION = "dimension"
    METRIC = "metric" 
    OPERATION = "operation"
    COLUMN_VALUE = "column_value"
    TEMPORAL = "temporal"
    VALUE = "value"
    CONNECTOR = "connector"
    UNKNOWN = "unknown"


class OperationType(Enum):
    MAXIMUM = "m√°ximo"
    MINIMUM = "m√≠nimo"
    SUM = "suma"
    AVERAGE = "promedio"
    COUNT = "conteo"


class TemporalUnit(Enum):
    DAYS = "days"
    WEEKS = "weeks"
    MONTHS = "months"
    YEARS = "years"
    QUARTERS = "quarters"


class QueryPattern(Enum):
    """Patrones de Consulta Identificados"""
    UNKNOWN = "unknown"
    AGGREGATION = "aggregation"
    REFERENCED = "referenced"
    TOP_N = "top_n"
    TEMPORAL_CONDITIONAL = "temporal_conditional"
    LIST_ALL = "list_all"
    SHOW_ROWS = "show_rows"
    MULTI_DIMENSION = "multi_dimension"
    MULTI_METRIC = "multi_metric"  


class RankingDirection(Enum):
    TOP = "top"
    BOTTOM = "bottom"
    UNKNOWN = "unknown"


class RankingUnit(Enum):
    COUNT = "count"
    PERCENTAGE = "percentage"


class ExclusionType(Enum):
    EQUALS = "equals"
    NOT_EQUALS = "not_equals"
    GREATER_THAN = "greater_than"
    LESS_THAN = "less_than"



# ----- Dataclasses necesarias -----

@dataclass
class QueryComponent:
    """Componente de consulta identificado"""
    text: str
    type: ComponentType
    confidence: float
    subtype: Optional[str] = None
    value: Optional[Union[str, int, float]] = None
    column_name: Optional[str] = None
    linguistic_info: Dict = field(default_factory=dict)


@dataclass
class ColumnValuePair:
    """Par columna-valor identificado"""
    column_name: str
    value: str
    confidence: float
    raw_text: str


@dataclass
class TemporalFilter:
    indicator: str
    quantity: Optional[int] = None
    unit: TemporalUnit = TemporalUnit.DAYS
    confidence: float = 0.8
    filter_type: str = "range"
    
    # Campos adicionales necesarios
    start_value: Optional[int] = None  # Para BETWEEN X AND Y
    end_value: Optional[int] = None    # Para BETWEEN X AND Y
    week_number: Optional[int] = None   # Para "week 5"
    year: Optional[int] = None         # Para especificar a√±o


@dataclass
class CompoundCriteria:
    """Criterio individual dentro de una consulta compuesta"""
    operation: QueryComponent  # mas, menor, mayor, etc.
    metric: QueryComponent     # inventario, venta, etc.
    confidence: float
    raw_tokens: List[str]      # tokens originales que forman este criterio


@dataclass 
class RankingCriteria:
    """Criterios de ranking detectados"""
    direction: RankingDirection  # top/bottom
    unit: RankingUnit           # count/percentage  
    value: Union[int, float]    # 5, 10, 25.5
    metric: Optional[QueryComponent] = None      # ventas, margen, inventario
    operation: Optional[QueryComponent] = None   # m√°ximo, suma, promedio
    confidence: float = 0.0
    raw_tokens: List[str] = field(default_factory=list)


@dataclass 
class ExclusionFilter:
    """Filtros de exclusi√≥n detectados"""
    exclusion_type: ExclusionType
    column_name: str
    value: str
    confidence: float
    raw_tokens: List[str] = field(default_factory=list)


@dataclass
class QueryStructure:
    
    """Estructura completa de la consulta - EXPANDIDA para rankings complejos"""
    main_dimension: Optional[QueryComponent]
    operations: List[QueryComponent]
    metrics: List[QueryComponent]
    column_conditions: List[ColumnValuePair]
    temporal_filters: List[TemporalFilter]
    values: List[QueryComponent]
    connectors: List[QueryComponent]
    unknown_tokens: List[QueryComponent]
    
    # Campos existentes para consultas compuestas
    compound_criteria: List[CompoundCriteria] = field(default_factory=list)
    is_compound_query: bool = False
    
    # Campos para rankings complejos
    ranking_criteria: Optional[RankingCriteria] = None
    exclusion_filters: List[ExclusionFilter] = field(default_factory=list)
    is_ranking_query: bool = False
    
    # Canpos para Multidimensiones
    main_dimensions: List[QueryComponent] = field(default_factory=list)  
    is_multi_dimension_query: bool = False 
    
    # Campos de control
    query_pattern: QueryPattern = QueryPattern.AGGREGATION
    reference_metric: Optional[QueryComponent] = None
    is_single_result: bool = False
    limit_value: Optional[int] = 1
    confidence_score: float = 0.0
    


    # ====================================
    # AGREGAR Intent sem√°ntico (pre-mapeo)
    # ====================================
    
    original_semantic_intent: str = 'DEFAULT'
    
    def get_complexity_level(self) -> str:
        """Calcula el nivel de complejidad"""
        complexity_score = 0
        
        complexity_score += len(self.column_conditions) * 2
        complexity_score += len(self.temporal_filters) * 3
        complexity_score += len(self.operations) * 1
        complexity_score += len(self.unknown_tokens) * -1
        
        # Complejidad por consultas compuestas
        if self.is_compound_query:
            complexity_score += len(self.compound_criteria) * 2
        
        # Complejidad por rankings
        if self.is_ranking_query:
            complexity_score += 3  # Base por ser ranking
            if self.ranking_criteria and self.ranking_criteria.unit == RankingUnit.PERCENTAGE:
                complexity_score += 2  # Extra por porcentajes
            complexity_score += len(self.exclusion_filters) * 2  # Por exclusiones
        
        # Agregar complejidad por patr√≥n
        if self.query_pattern == QueryPattern.REFERENCED:
            complexity_score += 2
        elif self.query_pattern == QueryPattern.LIST_ALL:
            complexity_score += 1
            
        if complexity_score <= 0:
            return "simple"
        elif complexity_score <= 3:
            return "moderada"
        elif complexity_score <= 6:
            return "compleja"
        elif complexity_score <= 10:
            return "muy_compleja"
        else:
            return "extrema"



# ----- Dataclass para palabras desconocidas -----

@dataclass
class UnknownWord:
    """Informaci√≥n de palabra desconocida"""
    word: str
    position: int
    context_before: List[str]
    context_after: List[str]
    suggested_type: str
    confidence: float
    timestamp: str
    full_query: str


# ----- Descripcion Consultas fallidas -----

@dataclass
class QueryFailure:
    """Informaci√≥n de consulta fallida"""
    original_query: str
    unknown_words: List[UnknownWord]
    timestamp: str
    session_id: str
    user_feedback: Optional[str] = None
    resolved: bool = False


# ----- Condiciones temporales -----

@dataclass
class AdvancedTemporalInfo:
    """Informaci√≥n temporal avanzada - complementa TemporalFilter existente"""
    original_filter: TemporalFilter
    is_range_from: bool = False    # "desde semana 8"
    is_range_between: bool = False # "de semana 8 a 4"  
    is_range_to: bool = False      # "hasta semana 5"
    start_value: Optional[int] = None
    end_value: Optional[int] = None
    raw_tokens: List[str] = field(default_factory=list)
    
    def to_sql_condition(self) -> str:
        """Convierte a condici√≥n SQL avanzada"""
        if self.is_range_from:
            if self.original_filter.unit == TemporalUnit.WEEKS:
                return f"week_number >= {self.start_value}"
            elif self.original_filter.unit == TemporalUnit.MONTHS:
                return f"month_number >= {self.start_value}"
            elif self.original_filter.unit == TemporalUnit.DAYS:
                return f"day_number >= {self.start_value}"
                
        elif self.is_range_between:
            if self.original_filter.unit == TemporalUnit.WEEKS:
                return f"week_number BETWEEN {min(self.start_value, self.end_value)} AND {max(self.start_value, self.end_value)}"
            elif self.original_filter.unit == TemporalUnit.MONTHS:
                return f"month_number BETWEEN {min(self.start_value, self.end_value)} AND {max(self.start_value, self.end_value)}"
            elif self.original_filter.unit == TemporalUnit.DAYS:
                return f"day_number BETWEEN {min(self.start_value, self.end_value)} AND {max(self.start_value, self.end_value)}"
                
        elif self.is_range_to:
            if self.original_filter.unit == TemporalUnit.WEEKS:
                return f"week_number <= {self.end_value}"
            elif self.original_filter.unit == TemporalUnit.MONTHS:
                return f"month_number <= {self.end_value}"
            elif self.original_filter.unit == TemporalUnit.DAYS:
                return f"day_number <= {self.end_value}"
        
        
        # Si no es ning√∫n patr√≥n avanzado, usar l√≥gica original
        if self.original_filter.filter_type == "specific":
            if self.original_filter.unit == TemporalUnit.WEEKS:
                return f"week_number = {self.original_filter.quantity}"
            elif self.original_filter.unit == TemporalUnit.MONTHS:
                return f"month_number = {self.original_filter.quantity}"
            elif self.original_filter.unit == TemporalUnit.DAYS:
                return f"day_number = {self.original_filter.quantity}"
        else:
            # Rangos tradicionales (ultimas X semanas)
            if self.original_filter.unit == TemporalUnit.WEEKS:
                days = self.original_filter.quantity * 7
                return f"fecha >= DATE('now', '-{days} days')"
            elif self.original_filter.unit == TemporalUnit.DAYS:
                return f"fecha >= DATE('now', '-{self.original_filter.quantity} days')"
            elif self.original_filter.unit == TemporalUnit.MONTHS:
                return f"fecha >= DATE('now', '-{self.original_filter.quantity} months')"
        
        return "1=1"



@dataclass
class SuperlativePattern:
    """Patr√≥n superlativo detectado (sold the most, had the least, etc.)"""
    question_word: str          # "which", "who", "what"
    target_dimension: str       # "account", "store", "product"
    action_verb: str           # "sold", "had", "generated"
    superlative_type: str      # "most", "least", "highest", "lowest"
    direction: str             # "DESC" o "ASC"
    implied_metric: Optional[str] = None  # "sales", "revenue" (inferido)
    confidence: float = 0.0
    raw_tokens: List[str] = field(default_factory=list)
    
    
@dataclass
class MultiMetricPattern:
    """üìä PATR√ìN PARA M√öLTIPLES M√âTRICAS"""
    metrics: List[str]  # Lista de nombres de m√©tricas
    operations: List[str]  # Lista de operaciones
    has_dimension: bool
    dimension: Optional[str]
    has_filters: bool
    filters: List[Dict]
    confidence: float
    raw_tokens: List[str]


@dataclass
class ThisWeekPattern:
    """Patr√≥n 'this week' detectado - √∫ltima semana disponible"""
    indicator_text: str        # "this week"
    position_start: int        # Posici√≥n donde empieza0p
    position_end: int          # Posici√≥n donde termina
    confidence: float = 0.0
    raw_tokens: List[str] = field(default_factory=list)



@dataclass
class YNColumnPattern:  # Renombrar de StockOutPattern
    """Patr√≥n para columnas Y/N detectado"""
    column_name: str           # 'Stock_Out' o 'Dead_Inventory'
    value: str                 # 'Y' o 'N'
    negation_detected: bool    # Si hay "not" o "aren't"
    indicator_text: str        # "in stock out", "not in stock out"
    position_start: int        # Posici√≥n donde empieza
    position_end: int          # Posici√≥n donde termina
    confidence: float = 0.0
    raw_tokens: List[str] = field(default_factory=list)



# ----- Cargador de diccionarios desde JSON -----

class JSONDictionaryLoader:
    """Carga diccionarios desde archivos JSON manteniendo la misma interfaz"""

    
# --------------------------------------------------------------
# ---------------- ENCONTRAR DICCIONARIOS ----------------------
# --------------------------------------------------------------    
    
# ----- Encontrar diccionario OPERACIONAL -----
    def __init__(self, json_path: str = "diccionarios/simples/"):
        self.json_path = Path(json_path)
        
        
# ----- Encontrar diccionario TEMPORAL -----
        self.temporal_path = Path("diccionarios/temporales/diccionario_temporal_actual.json")  
        self.temporal_dictionary = {}  
        self.load_all_dictionaries()
    
    
    def load_all_dictionaries(self):
        """Carga todos los diccionarios desde JSON"""
        try:
            
            # Cargar archivos core
            self.operaciones = self._load_and_convert_operations()
            self.dimensiones = set(self._load_json_file("core/dimensions.json", []))
            self.metricas = set(self._load_json_file("core/metrics.json", []))
            self.columnas_conocidas = self._load_json_file("core/known_columns.json", {})
            self.valores_comunes = set(self._load_json_file("core/common_values.json", []))
            
            
# ---------------- CARGAR DICCIONARIOS LINGUISTIC EN ESPA√ëOL E INGL√âS ----------------------
            
            # Cargar archivos linguistic - AMBOS IDIOMAS
            self.synonym_groups = self._load_json_file("linguistic/synonym_groups.json", {})

            # ESPA√ëOL
            self.conectores_es = set(self._load_json_file("linguistic/es/connectors.json", []))
            self.numeros_palabras_es = self._load_json_file("linguistic/es/word_numbers.json", {})
            self.correcciones_tipograficas_es = self._load_json_file("linguistic/es/typo_corrections.json", {})

            # INGL√âS  
            self.conectores_en = set(self._load_json_file("linguistic/en/connectors.json", []))
            self.numeros_palabras_en = self._load_json_file("linguistic/en/word_numbers.json", {})
            self.correcciones_tipograficas_en = self._load_json_file("linguistic/en/typo_corrections.json", {})

            # Variable para idioma detectado
            self.detected_language = 'es'  # ESPA√ëOL ES EL IDIOMA DEFAULT
            
            # üîß AGREGAR ESTA L√çNEA AQU√ç:
            self._create_language_aliases()
                
            # Cargar archivos temporal
            self.indicadores_temporales = self._load_json_file("temporal/temporal_indicators.json", {})
            self.unidades_tiempo = self._load_and_convert_temporal_units()
            
            # Construir frases compuestas
            self.frases_compuestas = {}
            self._build_compound_phrases()
            self._load_temporal_dictionary()
            
            print("‚úÖ Diccionarios JSON cargados exitosamente")
            
            # üöÄ Construir √≠ndices optimizados
            self._build_optimized_indices()
            
        except Exception as e:
            print(f"‚ùå Error cargando diccionarios JSON: {e}")
            print("üìö Usando diccionarios b√°sicos de fallback")
            self._load_fallback_dictionaries()
            
    
    def _load_json_file(self, relative_path: str, default_value):
        """Carga un archivo JSON espec√≠fico"""
        file_path = self.json_path / relative_path
        try:
            if file_path.exists():
                with open(file_path, 'r', encoding='utf-8') as f:
                    return json.load(f)
        except Exception as e:
            print(f"‚ö†Ô∏è Error cargando {relative_path}: {e}")
        return default_value
    
        
    def _load_and_convert_operations(self):
        """üîß CARGA OPERACIONES CON PALABRAS ANCLA - VERSI√ìN ACTUALIZADA"""
        operations_data = self._load_json_file("core/operations.json", {})
        
        # Mapeo de operaciones a enums
        string_to_enum = {
            "m√°ximo": OperationType.MAXIMUM,
            "m√≠nimo": OperationType.MINIMUM,
            "suma": OperationType.SUM,
            "promedio": OperationType.AVERAGE,
            "conteo": OperationType.COUNT
        }
        
        # Crear diccionario plano: palabra_ancla -> tipo_operacion
        operations_dict = {}
        
        for operation_key, anchor_words in operations_data.items():
            if isinstance(anchor_words, list):
                # Nuevo formato: lista de palabras ancla
                operation_enum = string_to_enum.get(operation_key, operation_key)
                
                for anchor_word in anchor_words:
                    # Normalizar la palabra ancla
                    normalized_anchor = anchor_word.lower().strip()
                    operations_dict[normalized_anchor] = operation_enum
                    
                    print(f"   üìé '{normalized_anchor}' ‚Üí {operation_key}")
            else:
                # Formato anterior (compatibilidad hacia atr√°s)
                operations_dict[operation_key] = string_to_enum.get(anchor_words, anchor_words)
        
        print(f"‚úÖ Operaciones cargadas: {len(operations_dict)} palabras ancla")
        return operations_dict

    
    def _load_and_convert_temporal_units(self):
        """Carga unidades temporales y convierte strings a enums"""
        units_data = self._load_json_file("temporal/temporal_units.json", {})
        units_dict = {}
        
        string_to_enum = {
            "days": TemporalUnit.DAYS,
            "weeks": TemporalUnit.WEEKS,
            "months": TemporalUnit.MONTHS,
            "years": TemporalUnit.YEARS,
            "quarters": TemporalUnit.QUARTERS
        }
        
        for key, value in units_data.items():
            units_dict[key] = string_to_enum.get(value, value)
        
        return units_dict
    
    
    def _build_compound_phrases(self):
        """Construye frases compuestas desde synonym_groups"""
        for normalized_key, synonyms in self.synonym_groups.items():
            for synonym in synonyms:
                self.frases_compuestas[synonym.lower()] = normalized_key
    
    
    # M√©todos para mantener compatibilidad con el c√≥digo original
    
    def get_component_type(self, word: str) -> ComponentType:
        """üöÄ VERSI√ìN OPTIMIZADA - B√öSQUEDA O(1)"""
        # Regla absoluta para may√∫sculas (m√°s r√°pido)
        if len(word) == 1 and word.isupper() and word.isalpha():
            return ComponentType.VALUE
        
        word_lower = word.lower()
        
        # B√∫squeda directa en √≠ndice principal
        direct_match = self.word_to_type_index.get(word_lower)
        if direct_match:
            return direct_match
        
        # B√∫squeda con prefijo de idioma
        lang_key = f"{self.detected_language}_{word_lower}"
        lang_match = self.word_to_type_index.get(lang_key)
        if lang_match:
            return lang_match
        
        # B√∫squeda temporal optimizada
        if hasattr(self, 'temporal_lookup'):
            temporal_result = self.temporal_lookup.get(word_lower)
            if temporal_result:
                return ComponentType.VALUE
        
        # Fallback a b√∫squeda tradicional si no est√° indexado
        if word_lower in self.indicadores_temporales or word_lower in self.unidades_tiempo:
            return ComponentType.TEMPORAL
        elif word.isdigit() or word_lower in self._get_numeros_palabras_by_language():
            return ComponentType.VALUE
        else:
            return ComponentType.UNKNOWN
    

    def _get_conectores_by_language(self):
        """Retorna conectores seg√∫n idioma detectado"""
        if self.detected_language == 'en':
            return self.conectores_en
        else:
            return self.conectores_es


    def _get_numeros_palabras_by_language(self):
        """Retorna n√∫meros en palabras seg√∫n idioma detectado"""
        if self.detected_language == 'en':
            return self.numeros_palabras_en
        else:
            return self.numeros_palabras_es    
        
        
    def get_operation_type(self, word: str):
        """üîç OBTIENE EL TIPO DE OPERACI√ìN - VERSI√ìN MEJORADA"""
        word_normalized = word.lower().strip()
        
        # B√∫squeda directa en el diccionario plano
        operation_type = self.operaciones.get(word_normalized, None)
        
        if operation_type:
            print(f"   ‚úÖ Operaci√≥n encontrada: '{word}' ‚Üí {operation_type}")
            return operation_type
        
        print(f"   ‚ùå Operaci√≥n no encontrada: '{word}'")
        return None
        
    
    def search_operation_in_phrase(self, phrase: str):
        """üîç BUSCA OPERACIONES EN FRASES COMPLETAS"""
        phrase_normalized = phrase.lower().strip()
        
        # Buscar frases exactas primero (m√°s espec√≠ficas)
        exact_matches = []
        partial_matches = []
        
        for anchor_word, operation_type in self.operaciones.items():
            if len(anchor_word.split()) > 1:  # Es una frase
                if anchor_word in phrase_normalized:
                    exact_matches.append((anchor_word, operation_type))
            else:  # Es una palabra individual
                if anchor_word in phrase_normalized.split():
                    partial_matches.append((anchor_word, operation_type))
        
        # Priorizar frases exactas sobre palabras individuales
        if exact_matches:
            # Ordenar por longitud (frases m√°s largas = m√°s espec√≠ficas)
            exact_matches.sort(key=lambda x: len(x[0]), reverse=True)
            best_match = exact_matches[0]
            print(f"   üéØ Frase encontrada: '{best_match[0]}' ‚Üí {best_match[1]}")
            return best_match[1]
        
        elif partial_matches:
            best_match = partial_matches[0]
            print(f"   üéØ Palabra encontrada: '{best_match[0]}' ‚Üí {best_match[1]}")
            return best_match[1]
        
        print(f"   ‚ùå No se encontraron operaciones en: '{phrase}'")
        return None
      


    def get_operation_suggestions(self, word: str, max_suggestions: int = 3):
        """üí° SUGERENCIAS DE OPERACIONES SIMILARES"""
        from difflib import get_close_matches
        
        word_normalized = word.lower().strip()
        all_anchor_words = list(self.operaciones.keys())
        
        # Buscar palabras similares
        suggestions = get_close_matches(
            word_normalized, 
            all_anchor_words, 
            n=max_suggestions, 
            cutoff=0.6
        )
        
        suggestion_results = []
        for suggestion in suggestions:
            operation_type = self.operaciones[suggestion]
            suggestion_results.append({
                'word': suggestion,
                'operation': operation_type,
                'confidence': 0.8  # Puedes calcular esto basado en similarity
            })
        
        return suggestion_results
    
    
    def get_temporal_unit(self, word: str):
        """Obtiene la unidad temporal"""
        return self.unidades_tiempo.get(word.lower(), None)
    
    
    def normalize_compound_phrases(self, text: str) -> str:
        """Normaliza frases compuestas"""
        text_lower = text.lower()
        sorted_phrases = sorted(self.frases_compuestas.keys(), key=len, reverse=True)
        
        for phrase in sorted_phrases:
            if phrase in text_lower:
                normalized = self.frases_compuestas[phrase]
                text_lower = text_lower.replace(phrase, normalized)
        
        return text_lower
    
    
    def correct_typo(self, word: str) -> str:
        """Corrige errores tipogr√°ficos seg√∫n idioma detectado"""
        if self.detected_language == 'en':
            return self.correcciones_tipograficas_en.get(word.lower(), word)
        else:  # espa√±ol
            return self.correcciones_tipograficas_es.get(word.lower(), word)
        
    
    def get_statistics(self) -> dict:
        """Obtiene estad√≠sticas de los diccionarios"""
        return {
            'total_dimensiones': len(self.dimensiones),
            'total_operaciones': len(self.operaciones),
            'total_metricas': len(self.metricas),
            'source': 'JSON files'
        }


    def _load_temporal_dictionary(self):
        """Carga el diccionario temporal con datos reales de la tabla"""
        try:
            if self.temporal_path.exists():
                with open(self.temporal_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    self.temporal_dictionary = data.get('temporal_dictionary', {})
                    print(f"‚úÖ Diccionario temporal cargado: {len(self.temporal_dictionary)} entradas")
            else:
                print(f"‚ö†Ô∏è Diccionario temporal no encontrado: {self.temporal_path}")
                self.temporal_dictionary = {}
        except Exception as e:
            print(f"‚ùå Error cargando diccionario temporal: {e}")
            self.temporal_dictionary = {}


    def search_in_temporal_dictionary(self, word: str) -> Optional[Dict]:
        """üöÄ B√öSQUEDA TEMPORAL OPTIMIZADA O(1)"""
        if hasattr(self, 'temporal_lookup'):
            return self.temporal_lookup.get(word.lower())
        
        # Fallback al m√©todo original si no hay √≠ndice
        return self._search_temporal_original(word)


    def _search_temporal_original(self, word: str) -> Optional[Dict]:
        """üîÑ M√âTODO ORIGINAL COMO FALLBACK"""
        word_lower = word.lower()
        
        # Buscar coincidencia exacta por clave
        if word_lower in self.temporal_dictionary:
            return self.temporal_dictionary[word_lower]
        
        # Buscar en variants de todas las entradas
        for key, entry in self.temporal_dictionary.items():
            variants = entry.get('variants', [])
            for variant in variants:
                if variant.lower() == word_lower:
                    return entry
        
        return None


    def get_temporal_component_type(self, word: str) -> Optional[ComponentType]:
        """
        Versi√≥n avanzada con logging para debugging
        """
        temporal_entry = self.search_in_temporal_dictionary(word)
        
        if temporal_entry:
            original_column_type = temporal_entry.get('column_type', 'unknown')
            
            # üîß FORZAR COMO VALUE
            print(f"   üóÑÔ∏è TEMPORAL: '{word}' original_type='{original_column_type}' ‚Üí FORZANDO como VALUE")
            
            return ComponentType.VALUE
        
        return None


# ---------------- DETECTAR IDIOMA POR MEDIO DE LA DETECCION EN TOKENS ----------------------

    def detect_language_from_tokens(self, tokens: List[str]) -> str:
        """üîß DETECTOR DE IDIOMA - REGLA ABSOLUTA PARA MAY√öSCULAS"""
        
        # üîß REGLA ABSOLUTA: Filtrar todas las letras may√∫sculas individuales
        filtered_tokens = []
        excluded_tokens = []
        
        for token in tokens:
            if len(token) == 1 and token.isupper() and token.isalpha():
                excluded_tokens.append(token)
            else:
                filtered_tokens.append(token)
        
        print(f"üîç TOKENS ORIGINALES: {tokens}")
        print(f"üîí DATOS EXCLUIDOS: {excluded_tokens} (letras may√∫sculas = DATOS)")
        print(f"üîç TOKENS PARA AN√ÅLISIS: {filtered_tokens}")
        
        if not filtered_tokens:
            print(f"‚ö†Ô∏è No hay tokens para analizar idioma, defaulteando a ingl√©s")
            return 'en'
        
        english_score = 0
        spanish_score = 0
        
        for token in filtered_tokens:
            token_lower = token.lower()
            
            # üîß PALABRAS CLARAMENTE INGLESAS (alta prioridad)
            clear_english = {
                'with', 'and', 'more', 'most', 'top', 'best', 'worst', 'bottom',
                'having', 'where', 'between', 'from', 'excluding', 'except', 
                'without', 'store', 'sales', 'account', 'product', 'customer'
            }
            
            if token_lower in clear_english:
                english_score += 15
                print(f"   üá∫üá∏ Palabra claramente inglesa: '{token}' (+15)")
                continue
            
            # üîß PALABRAS CLARAMENTE ESPA√ëOLAS (alta prioridad)
            clear_spanish = {
                'con', 'mas', 'mayor', 'menor', 'mejor', 'peor', 'primeros',
                'ultimos', 'entre', 'desde', 'hasta', 'suma', 'promedio',
                'tienda', 'ventas', 'cuenta', 'producto', 'cliente'
            }
            
            if token_lower in clear_spanish:
                spanish_score += 15
                print(f"   üá™üá∏ Palabra claramente espa√±ola: '{token}' (+15)")
                continue
            
            # üîß CONECTORES INGLESES
            if hasattr(self, 'conectores_en') and token_lower in self.conectores_en:
                english_score += 10
                print(f"   üá∫üá∏ Conector ingl√©s: '{token}' (+10)")
            
            # üîß CONECTORES ESPA√ëOLES
            elif token_lower in self.conectores:
                spanish_score += 10
                print(f"   üá™üá∏ Conector espa√±ol: '{token}' (+10)")
        
        # üîß RESOLVER EMPATES A FAVOR DEL INGL√âS si hay "with"
        if english_score == spanish_score:
            has_with = any(t.lower() == 'with' for t in filtered_tokens)
            if has_with:
                print(f"   üîß EMPATE: Resolviendo a favor del ingl√©s por 'with'")
                return 'en'
            
            # Si no hay with, usar heur√≠sticas adicionales
            has_snake_case = any('_' in token for token in filtered_tokens)
            if has_snake_case:
                print(f"   üîß EMPATE: Resolviendo a favor del ingl√©s por snake_case")
                return 'en'
            
            print(f"   üîß EMPATE: Defaulteando a ingl√©s")
            return 'en'
        
        result = 'en' if english_score > spanish_score else 'es'
        print(f"   üéØ RESULTADO: {'INGL√âS' if result == 'en' else 'ESPA√ëOL'} (score: {english_score} vs {spanish_score})")

        # ‚úÖ ACTUALIZAR IDIOMA DETECTADO Y RECREAR ALIASES
        self.detected_language = result
        self._create_language_aliases()

        return result


    def _create_language_aliases(self):
        """
        üîß Crear aliases para mantener compatibilidad con c√≥digo existente
        Actualiza los aliases seg√∫n el idioma detectado
        """
        if self.detected_language == 'en':
            self.conectores = self.conectores_en
            self.numeros_palabras = self.numeros_palabras_en
            self.correcciones_tipograficas = self.correcciones_tipograficas_en
            print(f"   üá∫üá∏ Aliases configurados para INGL√âS")
        else:
            self.conectores = self.conectores_es
            self.numeros_palabras = self.numeros_palabras_es
            self.correcciones_tipograficas = self.correcciones_tipograficas_es
            print(f"   üá™üá∏ Aliases configurados para ESPA√ëOL")
        
        # üîß Verificar que los aliases se crearon correctamente
        print(f"   ‚úÖ Conectores activos: {len(self.conectores)} palabras")
        print(f"   ‚úÖ N√∫meros activos: {len(self.numeros_palabras)} palabras") 
        print(f"   ‚úÖ Correcciones activas: {len(self.correcciones_tipograficas)} palabras")


    def _detect_compound_phrases_dictionary_based(self, query: str) -> str:
        """
        üîç DETECCI√ìN AUTOM√ÅTICA CON DEBUGGING ESPEC√çFICO
        """
        print(f"üîç DETECTANDO FRASES COMPUESTAS (Dictionary-Based): '{query}'")
        
# PASO 1: Preservar may√∫sculas individuales
        query_with_placeholders, preserved_tokens = self._preserve_single_uppercase_letters(query)
        print(f"üîç DEBUG: Query con placeholders: '{query_with_placeholders}'")
        
        text_lower = query_with_placeholders.lower()
        print(f"üîç DEBUG: Text en min√∫sculas: '{text_lower}'")
        
# PASO 2: Generar frases compuestas
        compound_phrases = self._generate_all_compound_phrases()
        print(f"   üîç Generadas {len(compound_phrases)} frases compuestas autom√°ticamente")
        
        changes_made = []
        
        
# PASO 3: Aplicar reemplazos con debugging detallado
        for space_version, underscore_version in sorted(compound_phrases.items(), key=lambda x: len(x[0]), reverse=True):
            if space_version in text_lower:
                print(f"   üéØ MATCH ENCONTRADO: '{space_version}' ‚Üí '{underscore_version}'")
                text_lower = text_lower.replace(space_version, underscore_version)
                changes_made.append(f"AUTO: '{space_version}' ‚Üí '{underscore_version}'")

# PASO 4: Restaurar may√∫sculas con debugging
        print(f"üîç DEBUG: Antes de restaurar may√∫sculas: '{text_lower}'")
        final_text = self._restore_preserved_tokens_fixed(text_lower, preserved_tokens)
        print(f"üîç DEBUG: Despu√©s de restaurar may√∫sculas: '{final_text}'")
        
        return final_text


    def _restore_preserved_tokens_fixed(self, text: str, preserved_tokens: Dict[str, str]) -> str:
        """üîì RESTAURAR TOKENS PRESERVADOS - VERSI√ìN CORREGIDA"""
        final_text = text
        
        print(f"üîì RESTAURANDO TOKENS:")
        print(f"   üì• Input: '{text}'")
        print(f"   üîë Tokens preservados: {preserved_tokens}")
        
        for placeholder_lower, original_letter in preserved_tokens.items():
            if placeholder_lower in final_text:
                final_text = final_text.replace(placeholder_lower, original_letter)
                print(f"   ‚úÖ Restaurado: '{placeholder_lower}' ‚Üí '{original_letter}'")
            else:
                print(f"   ‚ùå NO encontrado: '{placeholder_lower}' en '{final_text}'")
                
                # üÜï B√öSQUEDA M√ÅS ROBUSTA
                # Buscar partes del placeholder que puedan estar fragmentadas
                placeholder_parts = placeholder_lower.split('_')
                for i, part in enumerate(placeholder_parts):
                    if part in final_text and len(part) > 3:  # Solo partes significativas
                        print(f"   üîç Encontrada parte del placeholder: '{part}'")
        
        print(f"   üì§ Output: '{final_text}'")
        return final_text


# ---------------- DETECTAR DATOS COMPUESTOS POR 2 PALABRAS ----------------------

    def _generate_all_compound_phrases(self) -> Dict[str, str]:
        """üöÄ VERSI√ìN OPTIMIZADA CON CACHE"""
        if hasattr(self, '_compound_phrases_cache'):
            return self._compound_phrases_cache
        
        # Si no existe cache, construirlo
        self._build_compound_phrases_cache()
        return self._compound_phrases_cache


    def _add_automatic_variations(self, compound_phrases: Dict[str, str]):
        """
        üîÑ AGREGAR VARIACIONES AUTOM√ÅTICAS
        Genera variaciones comunes de las frases encontradas
        """
        # Crear copias para iterar sin modificar el diccionario original
        original_phrases = compound_phrases.copy()
        
        for space_phrase, underscore_phrase in original_phrases.items():
            
# VARIACI√ìN 1: Agregar plurales autom√°ticamente
            if not space_phrase.endswith('s'):
                plural_space = f"{space_phrase}s"
                plural_underscore = f"{underscore_phrase}s" 
                
                # Solo agregar si el plural existe en los diccionarios
                if (plural_underscore in self.dimensiones or 
                    plural_underscore in self.metricas):
                    compound_phrases[plural_space] = plural_underscore
            
# VARIACI√ìN 2: Manejar casos con may√∫sculas mezcladas
            # Esto permite detectar "DEAD INVENTORY", "Dead Inventory", etc.
            words = space_phrase.split()
            if len(words) >= 2:
                # Generar todas las combinaciones de may√∫sculas/min√∫sculas comunes
                variations = [
                    ' '.join(word.upper() for word in words),      # DEAD INVENTORY
                    ' '.join(word.capitalize() for word in words), # Dead Inventory
                    ' '.join([words[0].upper()] + words[1:]),      # DEAD inventory
                ]
                
                for variation in variations:
                    if variation != space_phrase:  # No duplicar la versi√≥n original
                        compound_phrases[variation.lower()] = underscore_phrase


# ----- FUNCION ESPECIAL PARA LA DETECCION DE VALORES LETRAS INDIVIDUALES MAYUSCULA ----
    
    def _preserve_single_uppercase_letters(self, query: str) -> Tuple[str, Dict[str, str]]:
        """üîí PRESERVAR SOLO LETRAS MAY√öSCULAS INDIVIDUALES"""
        preserved_tokens = {}
        words = query.split()
        processed_query = query
        
        for i, word in enumerate(words):
            clean_word = re.sub(r'[^\w]', '', word)
            if len(clean_word) == 1 and clean_word.isupper() and clean_word.isalpha():
                placeholder = f"__UPPERCASE_{i}_{clean_word}__"
                preserved_tokens[placeholder.lower()] = clean_word
                processed_query = processed_query.replace(word, placeholder)
                print(f"   üîí Preservando: '{clean_word}' ‚Üí '{placeholder}'")
        
        return processed_query, preserved_tokens


    def _restore_preserved_tokens(self, text: str, preserved_tokens: Dict[str, str]) -> str:
        """üîì RESTAURAR TOKENS PRESERVADOS"""
        final_text = text
        
        for placeholder_lower, original_letter in preserved_tokens.items():
            if placeholder_lower in final_text:
                final_text = final_text.replace(placeholder_lower, original_letter)
                print(f"   üîì Restaurando: '{placeholder_lower}' ‚Üí '{original_letter}'")
        
        return final_text


    def _process_synonym_groups(self, text_lower: str) -> List[str]:
        """üìö PROCESAR SYNONYM GROUPS EXISTENTES"""
        changes_made = []
        
        sorted_phrases = sorted(self.synonym_groups.keys(), key=len, reverse=True)
        for phrase in sorted_phrases:
            if phrase in text_lower:
                normalized = self.synonym_groups[phrase]
                text_lower = text_lower.replace(phrase, normalized)
                changes_made.append(f"SYNONYM: '{phrase}' ‚Üí '{normalized}'")
        
        return changes_made



    def _build_optimized_indices(self):
        """üöÄ CONSTRUIR √çNDICES PARA B√öSQUEDAS R√ÅPIDAS"""
        import time
        start_time = time.time()
        
        print("üîß Construyendo √≠ndices de b√∫squeda optimizada...")
        
        # √çNDICE 1: Palabra -> Tipo de componente
        self.word_to_type_index = {}
        
        # Indexar dimensiones
        for dim in self.dimensiones:
            self.word_to_type_index[dim.lower()] = ComponentType.DIMENSION
        
        # Indexar m√©tricas
        for metric in self.metricas:
            self.word_to_type_index[metric.lower()] = ComponentType.METRIC
        
        # Indexar operaciones
        for op_word in self.operaciones.keys():
            self.word_to_type_index[op_word.lower()] = ComponentType.OPERATION
        
        # Indexar conectores espa√±ol
        for connector in self.conectores_es:
            self.word_to_type_index[f"es_{connector.lower()}"] = ComponentType.CONNECTOR
        
        # Indexar conectores ingl√©s
        for connector in self.conectores_en:
            self.word_to_type_index[f"en_{connector.lower()}"] = ComponentType.CONNECTOR
        
        # √çNDICE 2: Diccionario temporal optimizado
        self._build_temporal_index()
        
        # √çNDICE 3: Frases compuestas en cache
        self._build_compound_phrases_cache()
        
        end_time = time.time()
        print(f"‚úÖ √çndices construidos: {len(self.word_to_type_index)} palabras en {end_time - start_time:.3f}s")


    def _build_temporal_index(self):
        """üöÄ OPTIMIZAR DICCIONARIO TEMPORAL"""
        self.temporal_lookup = {}
        
        if not hasattr(self, 'temporal_dictionary') or not self.temporal_dictionary:
            print("‚ö†Ô∏è No hay diccionario temporal para indexar")
            return
        
        for key, entry in self.temporal_dictionary.items():
            # Indexar clave principal
            self.temporal_lookup[key.lower()] = entry
            
            # Indexar todas las variantes
            variants = entry.get('variants', [])
            for variant in variants:
                self.temporal_lookup[variant.lower()] = entry
        
        print(f"‚úÖ √çndice temporal: {len(self.temporal_lookup)} entradas")


    def _build_compound_phrases_cache(self):
        """üöÄ CACHE DE FRASES COMPUESTAS"""
        self._compound_phrases_cache = {}
        
        # Solo procesar palabras con guiones bajos
        underscore_items = []
        
        for dim in self.dimensiones:
            if '_' in dim:
                underscore_items.append(dim)
        
        for metric in self.metricas:
            if '_' in metric:
                underscore_items.append(metric)
        
        for item in underscore_items:
            space_version = item.replace('_', ' ')
            self._compound_phrases_cache[space_version] = item
        
        print(f"‚úÖ Frases compuestas en cache: {len(self._compound_phrases_cache)} entradas")


# --------------------------
# ------ DETECCIONES -------
# --------------------------
    
# primer filtro antes de generar los procesos, identificamos cuales son las palabaras que 
# contienen los inputs del usuario, si se detecta alguan palabra desconocida se convertir√°
# en desconocida y se agregar√° en un diccionario json


# detector de palabras desconocidas
@dataclass
class UnknownWord:
    """Informaci√≥n de palabra desconocida"""
    word: str
    position: int
    context_before: List[str]
    context_after: List[str]
    suggested_type: str
    confidence: float
    timestamp: str
    full_query: str


# si se detecta alguna palabra que no se conoce la consulta fallar√° y no se forazar√° el proceso
@dataclass
class QueryFailure:
    """Informaci√≥n de consulta fallida"""
    original_query: str
    unknown_words: List[UnknownWord]
    timestamp: str
    session_id: str
    user_feedback: Optional[str] = None
    resolved: bool = False


@dataclass
class AdvancedTemporalInfo:
    """Informaci√≥n temporal avanzada - complementa TemporalFilter existente"""
    original_filter: TemporalFilter
    is_range_from: bool = False    # "desde semana 8"
    is_range_between: bool = False # "de semana 8 a 4"  
    is_range_to: bool = False      # "hasta semana 5"
    start_value: Optional[int] = None
    end_value: Optional[int] = None
    raw_tokens: List[str] = field(default_factory=list)
    
    
    
    def to_sql_condition(self) -> str:
        """Convierte a condici√≥n SQL avanzada - VERSI√ìN CORREGIDA"""
        if self.is_range_from:
            if self.original_filter.unit == TemporalUnit.WEEKS:
                return f"week >= {self.start_value}"
            elif self.original_filter.unit == TemporalUnit.MONTHS:
                return f"month >= {self.start_value}"
            elif self.original_filter.unit == TemporalUnit.DAYS:
                return f"day >= {self.start_value}"
                
        elif self.is_range_between:
            if self.original_filter.unit == TemporalUnit.WEEKS:
                return f"week BETWEEN {min(self.start_value, self.end_value)} AND {max(self.start_value, self.end_value)}"
            elif self.original_filter.unit == TemporalUnit.MONTHS:
                return f"month BETWEEN {min(self.start_value, self.end_value)} AND {max(self.start_value, self.end_value)}"
            elif self.original_filter.unit == TemporalUnit.DAYS:
                return f"day BETWEEN {min(self.start_value, self.end_value)} AND {max(self.start_value, self.end_value)}"
                
        elif self.is_range_to:
            if self.original_filter.unit == TemporalUnit.WEEKS:
                return f"week <= {self.end_value}"
            elif self.original_filter.unit == TemporalUnit.MONTHS:
                return f"month <= {self.end_value}"
            elif self.original_filter.unit == TemporalUnit.DAYS:
                return f"day <= {self.end_value}"
        
        # Si no es ning√∫n patr√≥n avanzado, usar l√≥gica original
        if self.original_filter.filter_type == "specific":
            if self.original_filter.unit == TemporalUnit.WEEKS:
                return f"week = {self.original_filter.quantity}"
            elif self.original_filter.unit == TemporalUnit.MONTHS:
                return f"month = {self.original_filter.quantity}"
            elif self.original_filter.unit == TemporalUnit.DAYS:
                return f"day = {self.original_filter.quantity}"
        else:
            # Rangos tradicionales (ultimas X semanas)
            if self.original_filter.unit == TemporalUnit.WEEKS:
                days = self.original_filter.quantity * 7
                return f"fecha >= DATE('now', '-{days} days')"
            elif self.original_filter.unit == TemporalUnit.DAYS:
                return f"fecha >= DATE('now', '-{self.original_filter.quantity} days')"
            elif self.original_filter.unit == TemporalUnit.MONTHS:
                return f"fecha >= DATE('now', '-{self.original_filter.quantity} months')"
        
        return "1=1"

        
    
# ----------------------------------------------------------------
# ------ DETECCION DE COMPLEJIDAD (evauluador inteligente) -------
# ----------------------------------------------------------------

# asignamos una categoria de complejidad evaluando la dificultad computacional y logica para procesar la consulta del usuario
# por medio de un sistema de puntuaciones basado en el tipo de datos que contiene la consulta.
# el valor de estas categorias se determina por medio del costo computacional que requiere completar la tarea.


# ------ Definir complejidad de consultas -------

    def get_complexity_level(self) -> str:
        """Calcula nivel de complejidad Y detecta errores cr√≠ticos"""
        
        # Detecci√≥n temprana de errores
        if len(self.unknown_tokens) > 0:
            return self._handle_unknown_tokens()
        
        # Procesamiento normal si no hay errores
        complexity_score = 0
        complexity_score += len(self.column_conditions) * 2
        complexity_score += len(self.temporal_filters) * 3
        complexity_score += len(self.operations) * 1
        
        if self.is_compound_query:
            complexity_score += len(self.compound_criteria) * 2
        
        if self.query_pattern == QueryPattern.REFERENCED:
            complexity_score += 2
        elif self.query_pattern == QueryPattern.LIST_ALL:
            complexity_score += 1
        
        # Clasificaci√≥n normal
        if complexity_score <= 0:
            return "simple"
        elif complexity_score <= 3:
            return "moderada"
        elif complexity_score <= 6:
            return "compleja"
        else:
            return "muy_compleja"


    # ------ Detectar tokens invalidos -------

    def handle_unknown_tokens(self) -> dict:
        """Maneja tokens desconocidos - falla si encuentra alguno"""
        
        # Si no hay tokens desconocidos, todo bien
        if not self.unknown_tokens:
            return {
                'valid': True,
                'should_fail': False
            }
        
        # Si hay tokens desconocidos, fallar
        unknown_words = [token.text for token in self.unknown_tokens]
        
        print(f"üö® TOKENS DESCONOCIDOS DETECTADOS:")
        print(f"   ‚ùå Palabras no reconocidas: {unknown_words}")
        
        return {
            'valid': False,
            'should_fail': True,
            'error': f'Palabras no reconocidas: {", ".join(unknown_words)}',
            'unknown_tokens': unknown_words
        }



# =====================================================
# ===== PROCESADORES ESPECIALIZADOS POR IDIOMA ========
# =====================================================


class BaseLanguageProcessor:
    """Clase base para procesadores de idioma espec√≠ficos"""
    
    
    def __init__(self, dictionaries):
        self.dictionaries = dictionaries
    
    
    def detect_temporal_patterns(self, tokens: List[str]) -> List[TemporalFilter]:
        """M√©todo abstracto - debe ser implementado por cada idioma"""
        raise NotImplementedError
    
    
    def detect_column_value_patterns(self, tokens: List[str], temporal_filters: List[TemporalFilter]) -> List[ColumnValuePair]:
        """M√©todo abstracto - debe ser implementado por cada idioma"""
        raise NotImplementedError
    
    
    def detect_ranking_patterns(self, tokens: List[str], classified_components: Dict) -> Optional[RankingCriteria]:
        """M√©todo abstracto - debe ser implementado por cada idioma"""
        raise NotImplementedError




# -----------------------------------------------------------------------------------------

# ================================================================================
# =========== PIPELINE DE PROCESAMIENTO COMPLETO (INGL√âS Y ESPA√ëOL) ==============
# ================================================================================

# Se dividen los pipelines para las consultas en ingl√©s y en espa√±ol, los PIPELINES
# comparten funcionalidades pero cada uno se encuentra adaptado para las reglas de 
# su respectivo idioma a tratar, las funcionalidades que se agreguen a uno no afectaran
# al otro por lo que hay que crear funciones para cada uno.  

# -----------------------------------------------------------------------------------------




# =========================================================        
# =========== PIPELINE PARA CONSULTAS EN INGL√âS ===========
# =========================================================     


class EnglishNLPParser:
    """üá∫üá∏ PARSER NLP ESPEC√çFICO PARA CONSULTAS EN INGL√âS"""
    

# ---------------- CORDINADOR DE CLASES INVOLUCRADAS EN EL PIPELINE ---------------------
    
    def __init__(self, dictionaries):
        """Inicializar parser ingl√©s con diccionarios compartidos"""
        self.dictionaries = dictionaries
        self.pre_mapping_analyzer = PreMappingSemanticAnalyzer()
        
        # ‚úÖ INICIALIZAR SQL MAPPER CON MANEJO DE ERRORES
        try:
            self.sql_mapper = SQLSchemaMapper()
            print("üá∫üá∏ English NLP Parser initialized with SQL Schema Mapper")
        except Exception as e:
            print(f"‚ö†Ô∏è Warning: SQLSchemaMapper failed to initialize: {e}")
            print("üìã Continuing without schema mapping (using conceptual SQL)")


    def format_temporal_dimension(self, dimension_name: str) -> str:
        """Formatea dimensiones temporales SOLO para SELECT"""
        if not dimension_name:
            return dimension_name
            
        temporal_dims = {
            'week': 'Week',
            'month': 'Month', 
            'year': 'Year',
            'day': 'Day',
            'quarter': 'Quarter'
        }
        
        # Obtener nombre normalizado
        normalized_name = temporal_dims.get(dimension_name.lower(), dimension_name)
        
        if dimension_name.lower() in temporal_dims:
            return f"CAST({normalized_name} AS CHAR) as {normalized_name}"
        
        return dimension_name



# ---------------- PROCESOS DEL PIPELINE PARA CONSUTLAS EN INGL√âS ---------------------
            
    def process_query(self, query: str, pre_normalized_query: str, preliminary_tokens: List[str]) -> Dict:
        """üá∫üá∏ PIPELINE PRINCIPAL PARA INGL√âS - VERSI√ìN COMPLETA"""
        
        print(f"\nüá∫üá∏ PROCESSING ENGLISH QUERY: '{query}'")
        
    # STEP 1: NORMALIZATION (English-specific)
        normalized_query = self.normalize_english_query(pre_normalized_query)
        tokens = normalized_query.split()
        
        print(f"üß™ DEBUGGING TEMPORAL DICTIONARY:")
        if hasattr(self.dictionaries, 'temporal_dictionary'):
            # Test directo
            test_cases = ["palacio de hierro", "palaciodehierro", "palacio_de_hierro", "liverpool"]
            for test in test_cases:
                result = self.dictionaries.search_in_temporal_dictionary(test)
                if result:
                    print(f"   ‚úÖ '{test}' ‚Üí {result.get('original_value')} (column: {result.get('column_name')})")
                else:
                    print(f"   ‚ùå '{test}' ‚Üí NOT FOUND")
        else:
            print(f"   ‚ùå No temporal dictionary loaded")
        
        print(f"üî§ English tokens: {tokens}")   
        
    # STEP 2: SEMANTIC ANALYSIS (reuse existing)
        original_intent = self.pre_mapping_analyzer.analyze_original_intent(tokens)
        print(f"üß† English semantic intent: {original_intent}")
            
    # STEP 3: ENGLISH-SPECIFIC PATTERN DETECTION
        temporal_filters = self.detect_temporal_patterns_english(tokens)
        
        # Usar la nueva funci√≥n con detecci√≥n impl√≠cita
        column_value_pairs = self.detect_column_value_patterns_english_with_implicit(tokens, temporal_filters)  
                
    # STEP 3.5: TEMPORAL CONDITIONAL PATTERN DETECTION
        temporal_conditional_pattern = self.detect_temporal_conditional_pattern_english(tokens)
        print(f"üïê DEBUG: temporal_conditional_pattern = {temporal_conditional_pattern is not None}")
                
    # STEP 3.6: LIST ALL PATTERN DETECTION
        list_all_pattern = self.detect_list_all_pattern_english(tokens)
        print(f"üìã DEBUG: list_all_pattern = {list_all_pattern is not None}")
        
    # STEP 3.7: SHOW ROWS PATTERN DETECTION
        show_rows_pattern = self.detect_show_rows_pattern_english(tokens)
        print(f"üìä DEBUG: show_rows_pattern = {show_rows_pattern is not None}")
                                
    # STEP 4: COMPONENT CLASSIFICATION (reuse with adaptations)
        classified_components = self.classify_components_english(tokens, column_value_pairs)
        
    # STEP 5: STRUCTURE BUILDING (usar el m√©todo existente)
        query_structure = self.build_english_structure(classified_components, column_value_pairs, temporal_filters, tokens, original_intent)
        
        print(f"üîß DEBUG: Llegando a validaci√≥n...")
        
    # STEP 6: VALIDATION (reuse existing)
        validation_result = self.validate_english_structure(query_structure)
        print(f"üîß DEBUG: Validaci√≥n result = {validation_result}")
        
        if not validation_result['valid']:
            return {
                'success': False,
                'error': validation_result['error'],
                'original_input': query,
                'suggestions': validation_result['suggestions'],
                'language': 'english'
            }
        
        
    # STEP 7: SQL GENERATION
        print(f"üîß DEBUG: Antes de generar SQL...")
        print(f"üîß DEBUG: hasattr list_all_pattern = {hasattr(query_structure, 'list_all_pattern')}")
        
        if hasattr(query_structure, 'list_all_pattern'):
            print(f"üîß DEBUG: list_all_pattern value = {query_structure.list_all_pattern}")
        
        conceptual_sql = self.generate_optimized_sql_english(query_structure)
        
        print(f"üîß DEBUG: SQL conceptual generado = '{conceptual_sql}'")

    # STEP 8: SQL SCHEMA NORMALIZATION (con fallback)
        if self.sql_mapper:
            try:
                normalized_sql = self.sql_mapper.normalize_sql(conceptual_sql)
                print(f"‚úÖ Schema mapping applied successfully")
            except Exception as e:
                print(f"‚ö†Ô∏è Schema mapping failed: {e}")
                print(f"üìã Using conceptual SQL as fallback")
                normalized_sql = conceptual_sql
        else:
            print(f"üìã No schema mapper available, using conceptual SQL")
            normalized_sql = conceptual_sql  # L√çNEA CR√çTICA
            
        print(f"üîß DEBUG: SQL final = '{normalized_sql}'")
            
        # STEP 9: FINAL RESULT
        try:
            # üîß DEBUG: Validar que query_structure existe y tiene los atributos necesarios
            print(f"üîß DEBUG: Creating final result...")
            print(f"üîß DEBUG: query_structure type: {type(query_structure)}")
            print(f"üîß DEBUG: Has get_complexity_level: {hasattr(query_structure, 'get_complexity_level')}")
            
            # Calcular complejidad de forma segura
            try:
                complexity_level = query_structure.get_complexity_level() if hasattr(query_structure, 'get_complexity_level') else 'unknown'
            except Exception as e:
                print(f"‚ö†Ô∏è Error getting complexity level: {e}")
                complexity_level = 'unknown'
            
            # Calcular confianza de forma segura
            try:
                confidence_score = self.calculate_overall_confidence_english(query_structure)
            except Exception as e:
                print(f"‚ö†Ô∏è Error calculating confidence: {e}")
                confidence_score = 0.8
            
            # Generar estructura de diccionario de forma segura
            try:
                structure_dict = self.structure_to_dict_english(query_structure)
            except Exception as e:
                print(f"‚ö†Ô∏è Error converting structure to dict: {e}")
                structure_dict = {}
            
            # Generar estructura jer√°rquica de forma segura
            try:
                hierarchical_structure = self.generate_hierarchical_structure_english(query_structure)
            except Exception as e:
                print(f"‚ö†Ô∏è Error generating hierarchical structure: {e}")
                print(f"‚ö†Ô∏è Error details: {str(e)}")
                import traceback
                traceback.print_exc()
                hierarchical_structure = "error_generating_structure"
            
            # Generar interpretaci√≥n de forma segura
            try:
                interpretation = self.generate_natural_interpretation_english(query_structure)
            except Exception as e:
                print(f"‚ö†Ô∏è Error generating interpretation: {e}")
                interpretation = "Query processed"
            
            # Obtener estad√≠sticas de mapeo de forma segura
            try:
                mapping_stats = self.sql_mapper.get_mapping_statistics() if self.sql_mapper else {}
            except Exception as e:
                print(f"‚ö†Ô∏è Error getting mapping stats: {e}")
                mapping_stats = {}
            
            final_result = {
                'success': True,
                'language': 'english',
                'original_input': query,
                'normalized_query': normalized_query,
                'tokens': tokens,
                'conceptual_sql': conceptual_sql,
                'sql_query': normalized_sql,
                'complexity_level': complexity_level,
                'processing_method': 'english_pipeline_with_schema_mapping',
                'note': 'üá∫üá∏ Processed with English-specific patterns + Schema Mapping',
                'query_structure': structure_dict,
                'hierarchical_structure': hierarchical_structure,
                'interpretation': interpretation,
                'confidence': confidence_score,
                'schema_mapping_stats': mapping_stats
            }
            
            print(f"üîß DEBUG: Final result created successfully")
            print(f"üîß DEBUG: SQL in result = '{final_result['sql_query']}'")
            
            return final_result
            
        except Exception as e:
            print(f"‚ùå ERROR creating final result: {e}")
            import traceback
            print(f"‚ùå FULL TRACEBACK:")
            traceback.print_exc()
            
            # Retornar resultado m√≠nimo funcional
            return {
                'success': True,  # Cambiar a True ya que el SQL se gener√≥ correctamente
                'language': 'english',
                'original_input': query,
                'normalized_query': normalized_query,
                'tokens': tokens,
                'sql_query': normalized_sql,  # Lo importante es que el SQL est√© disponible
                'conceptual_sql': conceptual_sql,
                'error_in_metadata': str(e),
                'note': 'SQL generated successfully but metadata generation had errors'
            }


    # M√©todos auxiliares simples:
    
    def _structure_to_dict(self, structure: QueryStructure) -> Dict:
        """Convertidor simple de estructura"""
        return {
            'main_dimension': structure.main_dimension.text if structure.main_dimension else None,
            'operations': [op.text for op in structure.operations],
            'metrics': [m.text for m in structure.metrics],
            'query_pattern': structure.query_pattern.value if hasattr(structure, 'query_pattern') else 'unknown'
        }


    def _calculate_confidence(self, structure: QueryStructure) -> float:
        """Calculador simple de confianza"""
        return getattr(structure, 'confidence_score', 0.85)
            
    
# ---------------- PROCESOS DEL PIPELINE PARA NORMALIZACION DE CONSUTLAS EN INGL√âS ---------------------

    def normalize_english_query(self, query: str) -> str:
        """üá∫üá∏ NORMALIZACI√ìN ESPEC√çFICA PARA INGL√âS"""
        
        print(f"üîß Normalizing English query: '{query}'")
        
# STEP 1: Apply English typo corrections
        words = query.split()
        corrected_words = []
        
        for word in words:
            # Preserve single uppercase letters
            if len(word) == 1 and word.isupper() and word.isalpha():
                corrected_words.append(word)
                print(f"üîí Preserving uppercase: '{word}'")
            else:
                corrected_word = self.dictionaries.correct_typo(word)
                corrected_words.append(corrected_word)
                if corrected_word != word:
                    print(f"üîß English correction: '{word}' ‚Üí '{corrected_word}'")
        
        query = ' '.join(corrected_words)
        
# STEP 2: Clean special characters
        query = re.sub(r'[^\w\s_/^a-zA-Z0-9\s\.\,\-\(\)\/]', '', query)
    
# STEP 3: Normalize spaces
        query = re.sub(r'\s+', ' ', query).strip()
        
        print(f"‚úÖ English normalized: '{query}'")
        return query
    
        
        

# =========== PROCESAMIENTO DE PATRONES TEMPORALES ===========
                    
    def detect_temporal_patterns_english(self, tokens: List[str]) -> List[TemporalFilter]:
        """üá∫üá∏ DETECCI√ìN DE PATRONES TEMPORALES EN INGL√âS - VERSI√ìN GEN√âRICA MEJORADA"""
        
        print(f"‚è∞ DETECTING ENGLISH TEMPORAL PATTERNS:")
        print(f"   üî§ Full tokens list: {tokens}")  # VER TODOS LOS TOKENS
        print(f"   üìè Total tokens: {len(tokens)}")
        
        # Buscar si existe "between" en los tokens
        between_positions = [i for i, t in enumerate(tokens) if t.lower() == 'between']
        print(f"   üîç 'between' found at positions: {between_positions}")
        
        temporal_filters = []
        advanced_temporal_info = []
        processed_positions = set()
        i = 0
        
        while i < len(tokens):
            # Saltar posiciones ya procesadas
            if i in processed_positions:
                print(f"   ‚è≠Ô∏è Position {i} already processed, skipping")  # AGREGAR
                i += 1
                continue
                    
            token_lower = tokens[i].lower()
                            
            # PATTERN 1: between weeks/months/days X and Y - VERSI√ìN MEJORADA PARA "IN BETWEEN"
            if token_lower == 'between' or (token_lower == 'in' and i + 1 < len(tokens) and tokens[i + 1].lower() == 'between'):
                print(f"üîß DEBUG: Found 'between' pattern at position {i}")
                
                # Ajustar el √≠ndice inicial si hay "in" antes
                start_idx = i
                if token_lower == 'in':
                    start_idx = i + 1  # Saltar "in" para empezar desde "between"
                
                # Buscar componentes de forma FLEXIBLE
                components = {
                    'unit': None,
                    'unit_pos': -1,
                    'numbers': [],
                    'and_pos': -1
                }
                
                # Buscar en las pr√≥ximas 10 posiciones desde "between"
                search_range = min(start_idx + 10, len(tokens))
                
                for j in range(start_idx + 1, search_range):
                    if j in processed_positions:
                        continue
                        
                    current_token = tokens[j].lower()
                    print(f"      üîç Checking position {j}: '{tokens[j]}' (lower: '{current_token}')")
                    
                    # Buscar unidad temporal
                    if not components['unit'] and current_token in ['week', 'weeks', 'month', 'months', 'day', 'days', 'year', 'years']:
                        components['unit'] = current_token
                        components['unit_pos'] = j
                        print(f"   ‚úÖ Found unit '{current_token}' at position {j}")
                    
                    # Buscar n√∫meros
                    elif tokens[j].isdigit():
                        components['numbers'].append((j, int(tokens[j])))
                        print(f"   ‚úÖ Found number '{tokens[j]}' at position {j}")
                    
                    # Buscar 'and'
                    elif current_token == 'and' and len(components['numbers']) == 1:
                        components['and_pos'] = j
                        print(f"   ‚úÖ Found 'and' at position {j}")
                
                # Debug de componentes encontrados
                print(f"   üìä Components found:")
                print(f"      unit: {components['unit']} at pos {components['unit_pos']}")
                print(f"      numbers: {components['numbers']}")
                print(f"      and_pos: {components['and_pos']}")
                
                # Validar que tenemos todos los componentes necesarios
                if (components['unit'] and 
                    len(components['numbers']) >= 2 and 
                    components['and_pos'] > -1):
                    
                    # Extraer valores
                    num1_pos, week_num1 = components['numbers'][0]
                    num2_pos, week_num2 = components['numbers'][1]
                    
                    # Verificar que 'and' est√° entre los n√∫meros
                    if num1_pos < components['and_pos'] < num2_pos:
                        current_year = 2025
                        
                        # Mapear unidad
                        unit_map = {
                            'week': TemporalUnit.WEEKS, 'weeks': TemporalUnit.WEEKS,
                            'month': TemporalUnit.MONTHS, 'months': TemporalUnit.MONTHS,
                            'day': TemporalUnit.DAYS, 'days': TemporalUnit.DAYS,
                            'year': TemporalUnit.YEARS, 'years': TemporalUnit.YEARS
                        }
                        temporal_unit = unit_map.get(components['unit'], TemporalUnit.WEEKS)
                        
                        # Convertir valores seg√∫n la unidad
                        if temporal_unit == TemporalUnit.WEEKS:
                            start_value = int(f"{current_year}{str(week_num1).zfill(2)}") if week_num1 < 100 else week_num1
                            end_value = int(f"{current_year}{str(week_num2).zfill(2)}") if week_num2 < 100 else week_num2
                        else:
                            start_value = week_num1
                            end_value = week_num2
                        
                        # Crear TemporalFilter con valores correctos
                        temporal_filter = TemporalFilter(
                            indicator='between',
                            quantity=None,
                            unit=temporal_unit,
                            confidence=0.95,
                            filter_type='range_between',
                            start_value=start_value,
                            end_value=end_value
                        )
                        
                        # Crear informaci√≥n avanzada
                        advanced_info = AdvancedTemporalInfo(
                            original_filter=temporal_filter,
                            is_range_between=True,
                            start_value=start_value,
                            end_value=end_value,
                            raw_tokens=tokens[i:num2_pos + 1]
                        )
                        
                        temporal_filters.append(temporal_filter)
                        advanced_temporal_info.append(advanced_info)
                        
                        print(f"   ‚úÖ BETWEEN PATTERN COMPLETE: {components['unit']} {week_num1} and {week_num2}")
                        print(f"      start_value={start_value}, end_value={end_value}")
                        
                        # Marcar todas las posiciones como procesadas (incluyendo "in" si existe)
                        for pos in range(i, num2_pos + 1):
                            processed_positions.add(pos)
                        
                        i = num2_pos + 1
                        continue
                else:
                    print(f"   ‚ùå Missing components for between pattern")
                    print(f"      unit: {components['unit']}, numbers: {len(components['numbers'])}, and: {components['and_pos']}")
                        
            # PATTERN 2: "from week/month X to Y" - VERSI√ìN GEN√âRICA
            elif token_lower == 'from':
                print(f"üîß DEBUG: Found 'from' at position {i}")
                
                components = {
                    'unit': None,
                    'first_number': None,
                    'to_pos': -1,
                    'second_number': None
                }
                
                # Buscar en las pr√≥ximas 8 posiciones
                search_range = min(i + 8, len(tokens))
                
                for j in range(i + 1, search_range):
                    if j in processed_positions:
                        continue
                        
                    current_token = tokens[j].lower()
                    
                    # Buscar unidad
                    if not components['unit'] and current_token in ['week', 'weeks', 'month', 'months', 'day', 'days', 'year', 'years']:
                        components['unit'] = current_token
                    
                    # Buscar primer n√∫mero
                    elif not components['first_number'] and tokens[j].isdigit():
                        components['first_number'] = (j, int(tokens[j]))
                    
                    # Buscar 'to'
                    elif current_token == 'to' and components['first_number']:
                        components['to_pos'] = j
                    
                    # Buscar segundo n√∫mero
                    elif components['to_pos'] > -1 and not components['second_number'] and tokens[j].isdigit():
                        components['second_number'] = (j, int(tokens[j]))
                        break
                
                # Validar componentes
                if all([components['unit'], components['first_number'], components['second_number'], components['to_pos'] > -1]):
                    num1_pos, num1 = components['first_number']
                    num2_pos, num2 = components['second_number']
                    
                    # Mapear unidad
                    unit_map = {
                        'week': TemporalUnit.WEEKS, 'weeks': TemporalUnit.WEEKS,
                        'month': TemporalUnit.MONTHS, 'months': TemporalUnit.MONTHS,
                        'day': TemporalUnit.DAYS, 'days': TemporalUnit.DAYS,
                        'year': TemporalUnit.YEARS, 'years': TemporalUnit.YEARS
                    }
                    temporal_unit = unit_map.get(components['unit'], TemporalUnit.WEEKS)
                    
                    # Convertir valores
                    current_year = 2025
                    if temporal_unit == TemporalUnit.WEEKS:
                        start_value = int(f"{current_year}{str(num1).zfill(2)}") if num1 < 100 else num1
                        end_value = int(f"{current_year}{str(num2).zfill(2)}") if num2 < 100 else num2
                    else:
                        start_value = num1
                        end_value = num2
                    
                    temporal_filter = TemporalFilter(
                        indicator="from_to",
                        quantity=None,
                        unit=temporal_unit,
                        confidence=0.95,
                        filter_type="range_between",
                        start_value=start_value,
                        end_value=end_value
                    )
                    
                    temporal_filters.append(temporal_filter)
                    
                    print(f"   ‚úÖ FROM-TO PATTERN: from {components['unit']} {num1} to {num2}")
                    
                    # Marcar posiciones procesadas
                    for pos in range(i, num2_pos + 1):
                        processed_positions.add(pos)
                    
                    i = num2_pos + 1
                    continue
            
            # PATTERN 3: "last X weeks/months" - GEN√âRICO
            elif token_lower == 'last':
                # Buscar n√∫mero y unidad en las pr√≥ximas posiciones
                number_found = None
                unit_found = None
                
                for j in range(i + 1, min(i + 4, len(tokens))):
                    if j in processed_positions:
                        continue
                        
                    # Buscar n√∫mero
                    if not number_found and tokens[j].isdigit():
                        number_found = int(tokens[j])
                    
                    # Buscar unidad
                    elif tokens[j].lower() in ['weeks', 'months', 'days', 'years', 'week', 'month', 'day', 'year']:
                        unit_found = tokens[j].lower()
                        
                        if number_found and unit_found:
                            unit_map = {
                                'weeks': TemporalUnit.WEEKS, 'week': TemporalUnit.WEEKS,
                                'months': TemporalUnit.MONTHS, 'month': TemporalUnit.MONTHS,
                                'days': TemporalUnit.DAYS, 'day': TemporalUnit.DAYS,
                                'years': TemporalUnit.YEARS, 'year': TemporalUnit.YEARS
                            }
                            
                            temporal_filter = TemporalFilter(
                                indicator="last",
                                quantity=number_found,
                                unit=unit_map[unit_found],
                                confidence=0.95,
                                filter_type="range"
                            )
                            
                            temporal_filters.append(temporal_filter)
                            
                            print(f"   ‚úÖ LAST PATTERN: last {number_found} {unit_found}")
                            
                            # Marcar posiciones procesadas
                            for pos in range(i, j + 1):
                                processed_positions.add(pos)
                            
                            i = j + 1
                            break
            
            # PATTERN 4: "week/month X" (espec√≠fico) - GEN√âRICO
            elif token_lower in ['week', 'weeks', 'month', 'months', 'day', 'days', 'year', 'years']:
                # Buscar n√∫mero en las pr√≥ximas 3 posiciones
                number_found = None
                number_pos = -1
                
                for j in range(i + 1, min(i + 4, len(tokens))):
                    if j in processed_positions:
                        continue
                        
                    if tokens[j].isdigit():
                        number_found = int(tokens[j])
                        number_pos = j
                        break
                
                if number_found:
                    # Normalizar unidad
                    unit_map = {
                        'week': TemporalUnit.WEEKS, 'weeks': TemporalUnit.WEEKS,
                        'month': TemporalUnit.MONTHS, 'months': TemporalUnit.MONTHS,
                        'day': TemporalUnit.DAYS, 'days': TemporalUnit.DAYS,
                        'year': TemporalUnit.YEARS, 'years': TemporalUnit.YEARS
                    }
                    temporal_unit = unit_map.get(token_lower, TemporalUnit.WEEKS)
                    
                    # Para semanas, convertir a formato YYYYWW
                    if temporal_unit == TemporalUnit.WEEKS and number_found < 100:
                        current_year = 2025
                        quantity = int(f"{current_year}{str(number_found).zfill(2)}")
                    else:
                        quantity = number_found
                    
                    temporal_filter = TemporalFilter(
                        indicator="specific",
                        quantity=quantity,
                        unit=temporal_unit,
                        confidence=0.90,
                        filter_type="specific"
                    )
                    
                    temporal_filters.append(temporal_filter)
                    
                    print(f"   ‚úÖ SPECIFIC PATTERN: {token_lower} {number_found} ‚Üí {quantity}")
                    
                    # Marcar posiciones procesadas
                    processed_positions.add(i)
                    processed_positions.add(number_pos)
                    
                    i = number_pos + 1
                    continue
            
            # PATTERN 5: "this week/month"
            elif token_lower == 'this':
                if i + 1 < len(tokens):
                    next_token = tokens[i + 1].lower()
                    if next_token in ['week', 'month', 'day', 'year']:
                        unit_map = {
                            'week': TemporalUnit.WEEKS,
                            'month': TemporalUnit.MONTHS,
                            'day': TemporalUnit.DAYS,
                            'year': TemporalUnit.YEARS
                        }
                        
                        temporal_filter = TemporalFilter(
                            indicator="this",
                            quantity=1,
                            unit=unit_map[next_token],
                            confidence=0.95,
                            filter_type="current_week" if next_token == 'week' else "current"
                        )
                        
                        temporal_filters.append(temporal_filter)
                        
                        print(f"   ‚úÖ THIS PATTERN: this {next_token}")
                        
                        processed_positions.add(i)
                        processed_positions.add(i + 1)
                        
                        i += 2
                        continue
            
            # PATTERN 6: "since week X"
            elif token_lower == 'since':
                components = {'unit': None, 'number': None}
                
                for j in range(i + 1, min(i + 4, len(tokens))):
                    if tokens[j].lower() in ['week', 'weeks', 'month', 'months']:
                        components['unit'] = tokens[j].lower()
                    elif tokens[j].isdigit():
                        components['number'] = int(tokens[j])
                        
                    if components['unit'] and components['number']:
                        unit_map = {
                            'week': TemporalUnit.WEEKS, 'weeks': TemporalUnit.WEEKS,
                            'month': TemporalUnit.MONTHS, 'months': TemporalUnit.MONTHS
                        }
                        temporal_unit = unit_map.get(components['unit'], TemporalUnit.WEEKS)
                        
                        if temporal_unit == TemporalUnit.WEEKS:
                            current_year = 2025
                            since_value = int(f"{current_year}{str(components['number']).zfill(2)}") if components['number'] < 100 else components['number']
                        else:
                            since_value = components['number']
                        
                        temporal_filter = TemporalFilter(
                            indicator='since',
                            quantity=None,
                            unit=temporal_unit,
                            confidence=0.95,
                            filter_type='since',
                            start_value=since_value
                        )
                        
                        temporal_filters.append(temporal_filter)
                        print(f"   ‚úÖ SINCE PATTERN: since {components['unit']} {components['number']}")
                        
                        for pos in range(i, j + 1):
                            processed_positions.add(pos)
                        
                        i = j + 1
                        break
            
            i += 1
        
        # Guardar informaci√≥n para uso posterior
        self.advanced_temporal_info = advanced_temporal_info
        self.temporal_processed_positions = processed_positions
        
        print(f"‚è∞ TOTAL ENGLISH TEMPORAL FILTERS: {len(temporal_filters)}")
        for tf in temporal_filters:
            print(f"   üìÖ Filter: {tf.filter_type} - {tf.indicator}")
            if hasattr(tf, 'start_value'):
                print(f"      start_value: {tf.start_value}")
            if hasattr(tf, 'end_value'):
                print(f"      end_value: {tf.end_value}")
        
        return temporal_filters


# =========== PROCESAMIENTO DE PATRONES COLUMNA VALOR EN INGL√âS ===========
            
    def detect_column_value_patterns_english(self, tokens: List[str], temporal_filters: List[TemporalFilter]) -> List[ColumnValuePair]:
        """üá∫üá∏ DETECCI√ìN CON CONTROL DE DUPLICADOS CORREGIDO"""
        
        print(f"üéØ DETECTING ENGLISH COLUMN-VALUE PATTERNS:")
        
        # VERIFICAR DICCIONARIO TEMPORAL
        if hasattr(self.dictionaries, 'temporal_dictionary'):
            temp_dict_size = len(self.dictionaries.temporal_dictionary)
            print(f"üìö Temporal dictionary loaded: {temp_dict_size} entries")
            test_search = self.dictionaries.search_in_temporal_dictionary("palacio de hierro")
            if test_search:
                print(f"‚úÖ Test: 'palacio de hierro' ‚Üí {test_search.get('original_value')}")
        
        column_value_pairs = []
        
        # Identificar columnas temporales
        temporal_columns = set()
        for tf in temporal_filters:
            if tf.unit == TemporalUnit.WEEKS:
                temporal_columns.update(['week', 'weeks', 'semana', 'semanas'])
            elif tf.unit == TemporalUnit.MONTHS:
                temporal_columns.update(['month', 'months', 'mes', 'meses'])
            elif tf.unit == TemporalUnit.DAYS:
                temporal_columns.update(['day', 'days', 'dia', 'dias'])
            elif tf.unit == TemporalUnit.YEARS:
                temporal_columns.update(['year', 'years', 'a√±o', 'a√±os'])
        
        print(f"‚è∞ Columnas temporales a excluir: {temporal_columns}")
        
    # CONTROL ESTRICTO DE DUPLICADOS
        processed_positions = set()
        created_filters = set()  # Para evitar filtros duplicados por contenido
        
    # PASO 1: DETECTAR PATRONES DIRECTOS CON DICCIONARIO TEMPORAL (PRIORIDAD M√ÅXIMA)
        for i in range(len(tokens) - 1):
            if i in processed_positions:
                continue
                
            current_token = tokens[i]
            
            # Verificar si es columna potencial
            column_info = self._identify_potential_column_english(current_token)
            if not column_info['is_column'] or column_info['normalized_name'] in temporal_columns:
                continue
            
            print(f"üîç Testing TEMPORAL DICT pattern: '{current_token}' + [value from dictionary]")
            
            dict_result = self._extract_value_from_temporal_dict(tokens, i + 1, column_info['normalized_name'])
            
            if dict_result:
                # üÜï CREAR CLAVE √öNICA PARA EVITAR DUPLICADOS
                filter_key = f"{column_info['normalized_name']}={dict_result['normalized_value']}"
                
                if filter_key not in created_filters:
                    column_value_pairs.append(ColumnValuePair(
                        column_name=column_info['normalized_name'],
                        value=dict_result['normalized_value'],
                        confidence=dict_result['confidence'],
                        raw_text=f"{current_token} {dict_result['raw_text']}"
                    ))
                    
                    created_filters.add(filter_key)
                    print(f"‚úÖ TEMPORAL DICT SUCCESS: {current_token} = '{dict_result['normalized_value']}'")
                    
                    # üö® MARCAR TODAS LAS POSICIONES COMO PROCESADAS
                    for pos in range(i, i + 1 + dict_result['tokens_consumed']):
                        processed_positions.add(pos)
                    
                    print(f"üîí POSITIONS LOCKED: {list(range(i, i + 1 + dict_result['tokens_consumed']))}")
                else:
                    print(f"üîÑ TEMPORAL DICT DUPLICATE AVOIDED: {filter_key}")
        
    # PASO 2: PATRONES CON PREPOSICIONES (respetando posiciones procesadas)
        i = 0
        while i < len(tokens) - 2:
            # üö® VERIFICAR SI LA POSICI√ìN YA FUE PROCESADA
            if i in processed_positions:
                print(f"‚è≠Ô∏è SKIPPING position {i} (already processed)")
                i += 1
                continue
            
            # PATTERN ESPECIAL: "total [word] of [metric]" ‚Üí Agregaci√≥n global
            if (i < len(tokens) - 3 and
                tokens[i].lower() == 'total' and
                tokens[i + 2].lower() == 'of' and
                self._is_potential_metric_english(tokens[i + 3])):
                
                print(f"   üåê ENGLISH AGGREGATION PATTERN: total {tokens[i + 1]} of {tokens[i + 3]} (no filter created)")
                i += 4
                continue
            
        # PATTERN 1: [preposition] [column] [value]
            if i < len(tokens) - 2:
                # üö® VERIFICAR QUE NINGUNA DE LAS 3 POSICIONES EST√â PROCESADA
                positions_needed = {i, i + 1, i + 2}
                if positions_needed.intersection(processed_positions):
                    print(f"‚è≠Ô∏è PREPOSITION PATTERN: positions {positions_needed} overlap with processed {processed_positions}")
                    i += 1
                    continue
                
                pattern_result = self._detect_preposition_column_value_pattern_english(
                    tokens, i, temporal_columns, processed_positions
                )
                
                if pattern_result:
                    # üÜï VERIFICAR DUPLICADOS POR CONTENIDO
                    filter_key = f"{pattern_result['pair'].column_name}={pattern_result['pair'].value}"
                    
                    if filter_key not in created_filters:
                        column_value_pairs.append(pattern_result['pair'])
                        created_filters.add(filter_key)
                        print(f"‚úÖ ENGLISH FILTER CREATED (preposition): {pattern_result['raw_text']}")
                        
                        # Marcar posiciones como procesadas
                        for pos in range(i, i + pattern_result['tokens_consumed']):
                            processed_positions.add(pos)
                    else:
                        print(f"üîÑ PREPOSITION DUPLICATE AVOIDED: {filter_key}")
                    
                    i += pattern_result['tokens_consumed']
                    continue
            
            i += 1
        
        print(f"üéØ Total English filters detected: {len(column_value_pairs)}")
        print(f"üîÑ Unique filters created: {created_filters}")
        print(f"üîí Final processed positions: {sorted(processed_positions)}")
        
        return column_value_pairs
                        

    # =====================================================================
    # =========== DETECTOR DE PATRONES DE VALORES IMPL√çCITOS =============
    # =====================================================================

    def detect_implicit_value_patterns_english(self, tokens: List[str]) -> Tuple[List[ColumnValuePair], List[int]]:
        implicit_filters = []
        processed_positions = set()
        
        
        # STEP 1: Buscar combinaciones de m√∫ltiples tokens PRIMERO
        for start_idx in range(len(tokens)):
            if start_idx in processed_positions:
                continue
                
            # üîß CAMBIO CR√çTICO: Empezar desde las combinaciones m√°s largas
            for length in range(min(15, len(tokens) - start_idx), 0, -1):  # De m√°s largo a m√°s corto
                end_idx = start_idx + length
                
                # Verificar que no haya posiciones ya procesadas en este rango
                if any(pos in processed_positions for pos in range(start_idx, end_idx)):
                    continue
                
                candidate_tokens = tokens[start_idx:end_idx]
                
                print(f"      üîç Testing combination: {candidate_tokens} (positions {start_idx}-{end_idx-1})")
                
                # Buscar en diccionario temporal
                implicit_result = self._search_implicit_value_in_temporal_dict(candidate_tokens)
                
                if implicit_result:
                    # Determinar el contexto de la consulta para validar si tiene sentido
                    context_info = self._analyze_query_context_for_implicit_value(
                        tokens, start_idx, end_idx, implicit_result
                    )
                    
                    if context_info['is_valid_context']:
                        # Crear ColumnValuePair autom√°ticamente
                        column_value_pair = ColumnValuePair(
                            column_name=implicit_result['column_name'].lower(),
                            value=implicit_result['original_value'],
                            confidence=implicit_result['confidence'] * context_info['context_confidence'],
                            raw_text=' '.join(candidate_tokens)
                        )
                        
                        implicit_filters.append(column_value_pair)
                        
                        # Marcar TODAS las posiciones como procesadas
                        for pos in range(start_idx, end_idx):
                            processed_positions.add(pos)
                        
                        print(f"      ‚úÖ IMPLICIT VALUE DETECTED:")
                        print(f"         üìç Value: '{implicit_result['original_value']}'")
                        print(f"         üìã Column: {implicit_result['column_name']}")
                        print(f"         üéØ Context: {context_info['context_type']}")
                        print(f"         ‚≠ê Confidence: {column_value_pair.confidence:.2f}")
                        print(f"         üîí Positions processed: {list(range(start_idx, end_idx))}")
                        
                        # üîß IMPORTANTE: Salir del loop de longitud para esta posici√≥n
                        break  # Procesar solo la combinaci√≥n m√°s larga encontrada
                    else:
                        print(f"      ‚ùå Invalid context for implicit value: {context_info['reason']}")
        
        # üîß CAMBIO: Ya NO buscar tokens individuales por separado
        # porque ya est√°n incluidos en el loop anterior (cuando length=1)
        
        print(f"üîç TOTAL IMPLICIT VALUES DETECTED: {len(implicit_filters)}")
        print(f"üîí TOTAL POSITIONS PROCESSED: {sorted(processed_positions)}")
        
        return implicit_filters, list(processed_positions)


    def _search_implicit_value_in_temporal_dict(self, candidate_tokens: List[str]) -> Optional[Dict]:
        """
        üóÑÔ∏è B√öSQUEDA DE VALOR EN DICCIONARIO TEMPORAL
        Prueba m√∫ltiples variantes de los tokens candidatos
        """
        
        # Generar variantes para buscar
        test_variants = self._generate_search_variants(candidate_tokens)
        
        for variant in test_variants:
            print(f"         üîç Testing variant: '{variant}'")
            
            temporal_entry = self.dictionaries.search_in_temporal_dictionary(variant)
            
            if temporal_entry:
                print(f"         ‚úÖ MATCH FOUND: '{variant}' ‚Üí {temporal_entry}")
                return temporal_entry
        
        return None


    def _generate_search_variants(self, tokens: List[str]) -> List[str]:
        """
        üîß GENERADOR DE VARIANTES DE B√öSQUEDA
        Crea todas las combinaciones posibles para buscar en el diccionario
        """
        
        base_text = ' '.join(tokens)
        
        variants = [
            base_text.lower(),                          # "palacio de hierro"
            base_text.upper(),                          # "PALACIO DE HIERRO"
            ''.join(tokens).lower(),                    # "palaciodehierro"
            '('.join(tokens).lower(),
            ''.join(tokens).upper(),                    # "PALACIODEHIERRO"
            '_'.join(tokens).lower(),                   # "palacio_de_hierro"
            '_'.join(tokens).upper(),                   # "PALACIO_DE_HIERRO"
            base_text.title(),                          # "Palacio De Hierro"
        ]
        
        # Para tokens individuales, agregar variantes adicionales
        if len(tokens) == 1:
            token = tokens[0]
            variants.extend([
                token,                                  # Original
                token.lower(),                          # lowercase
                token.upper(),                          # UPPERCASE
                token.capitalize()                      # Capitalized
            ])
        
        # Remover duplicados manteniendo orden
        seen = set()
        unique_variants = []
        for variant in variants:
            if variant not in seen:
                seen.add(variant)
                unique_variants.append(variant)
        
        return unique_variants


    def _analyze_query_context_for_implicit_value(self, tokens: List[str], start_idx: int, end_idx: int, implicit_result: Dict) -> Dict:
        """
        üß† ANALIZADOR DE CONTEXTO PARA VALORES IMPL√çCITOS
        Determina si el valor encontrado tiene sentido en el contexto de la consulta
        """
        
        print(f"         üß† Analyzing context for implicit value...")
        
        # Tokens antes y despu√©s del valor encontrado
        before_tokens = tokens[:start_idx]
        after_tokens = tokens[end_idx:]
        value_column = implicit_result['column_name'].lower()
        
        print(f"            üìç Before: {before_tokens}")
        print(f"            üìç After: {after_tokens}")
        print(f"            üìã Value column: {value_column}")
        
        context_patterns = []
        context_confidence = 0.7  # Base confidence
        
    # PATTERN 1: "how many X does [VALUE] have" ‚Üí COUNT query
        if self._matches_count_pattern(before_tokens + after_tokens):
            context_patterns.append('COUNT_PATTERN')
            context_confidence += 0.2
            print(f"            ‚úÖ COUNT pattern detected")
        
    # PATTERN 2: "top N X of [VALUE]" ‚Üí RANKING query  
        if self._matches_ranking_pattern(before_tokens + after_tokens):
            context_patterns.append('RANKING_PATTERN')
            context_confidence += 0.2
            print(f"            ‚úÖ RANKING pattern detected")
        
    # PATTERN 3: "which X of [VALUE]" ‚Üí SELECTION query
        if self._matches_selection_pattern(before_tokens + after_tokens):
            context_patterns.append('SELECTION_PATTERN')
            context_confidence += 0.2
            print(f"            ‚úÖ SELECTION pattern detected")
        
    # PATTERN 4: "[VALUE] sales/revenue/data" ‚Üí METRIC query
        if self._matches_metric_pattern(before_tokens + after_tokens):
            context_patterns.append('METRIC_PATTERN')
            context_confidence += 0.15
            print(f"            ‚úÖ METRIC pattern detected")
        
    # PATTERN 5: Contiene palabras interrogativas
        if self._contains_question_words(before_tokens + after_tokens):
            context_patterns.append('QUESTION_PATTERN')
            context_confidence += 0.1
            print(f"            ‚úÖ QUESTION pattern detected")


    # VALIDATION: Debe tener al menos un patr√≥n v√°lido O ser "X of Y"
        if not context_patterns:
            # Verificar si es patr√≥n "X of Y"
            if (len(before_tokens) >= 2 and 
                before_tokens[-1].lower() == 'of', 'in'):
                return {
                    'is_valid_context': True,
                    'context_type': 'X_OF_Y_PATTERN',
                    'context_confidence': 0.85,
                    'patterns_detected': ['X_OF_Y']
                }
            
            # Si no, rechazar
            return {
                'is_valid_context': False,
                'context_type': 'UNKNOWN',
                'context_confidence': 0.0,
                'reason': 'No recognizable query patterns found'
            }
                
    # VALIDATION: Verificar coherencia con tipo de columna
        column_type = implicit_result.get('column_type', 'unknown')
        if not self._is_coherent_with_column_type(context_patterns, column_type):
            return {
                'is_valid_context': False,
                'context_type': 'INCOHERENT',
                'context_confidence': 0.0,
                'reason': f'Context patterns {context_patterns} not coherent with column type {column_type}'
            }
        
        return {
            'is_valid_context': True,
            'context_type': '_'.join(context_patterns),
            'context_confidence': min(1.0, context_confidence),
            'patterns_detected': context_patterns
        }


    def _matches_count_pattern(self, surrounding_tokens: List[str]) -> bool:
        """üî¢ DETECTOR DE PATR√ìN DE CONTEO"""
        text = ' '.join(surrounding_tokens).lower()
        
        count_indicators = [
            'how many', 'how much', 'total number', 'number of', 'count of',
            'total', 'sum of', 'amount of', 'quantity of'
        ]
        
        for indicator in count_indicators:
            if indicator in text:
                return True
        
        # Buscar palabras individuales tambi√©n
        count_words = {'many', 'much', 'total', 'count', 'number', 'quantity', 'amount'}
        return any(word in [t.lower() for t in surrounding_tokens] for word in count_words)


    def _matches_ranking_pattern(self, surrounding_tokens: List[str]) -> bool:
        """üèÜ DETECTOR DE PATR√ìN DE RANKING"""
        
        ranking_words = {
            'top', 'best', 'highest', 'maximum', 'first', 'greatest', 'most',
            'worst', 'lowest', 'minimum', 'last', 'least', 'bottom'
        }
        
        # Buscar indicadores de ranking
        for token in surrounding_tokens:
            if token.lower() in ranking_words:
                return True
        
        # Buscar n√∫meros que indican ranking (top 5, best 10, etc.)
        for i, token in enumerate(surrounding_tokens):
            if token.lower() in ranking_words and i + 1 < len(surrounding_tokens):
                next_token = surrounding_tokens[i + 1]
                if next_token.isdigit() or next_token.endswith('%'):
                    return True
        
        return False


    def _matches_selection_pattern(self, surrounding_tokens: List[str]) -> bool:
        """üéØ DETECTOR DE PATR√ìN DE SELECCI√ìN"""
        
        selection_words = {
            'which', 'what', 'who', 'where', 'show', 'list', 'display', 
            'get', 'find', 'search', 'lookup', 'identify'
        }
        
        return any(token.lower() in selection_words for token in surrounding_tokens)


    def _matches_metric_pattern(self, surrounding_tokens: List[str]) -> bool:
        """üìä DETECTOR DE PATR√ìN DE M√âTRICAS"""
        
        metric_words = {
            'sales', 'revenue', 'profit', 'margin', 'cost', 'price',
            'inventory', 'stock', 'volume', 'units', 'dollars', 'data'
        }
        
        return any(token.lower() in metric_words for token in surrounding_tokens)


    def _contains_question_words(self, surrounding_tokens: List[str]) -> bool:
        """‚ùì DETECTOR DE PALABRAS INTERROGATIVAS"""
        
        question_words = {
            'how', 'what', 'which', 'who', 'where', 'when', 'why',
            'does', 'do', 'is', 'are', 'can', 'will', 'would'
        }
        
        return any(token.lower() in question_words for token in surrounding_tokens)


    def _is_coherent_with_column_type(self, context_patterns: List[str], column_type: str) -> bool:
        """üîç VERIFICADOR DE COHERENCIA CON TIPO DE COLUMNA"""
        
        # Por ahora, permitir todos los patrones para todos los tipos de columna
        # Se puede refinar m√°s adelante con reglas espec√≠ficas
        
        # Ejemplo de reglas futuras:
        # if column_type == 'dimension' and 'METRIC_PATTERN' in context_patterns:
        #     return False  # Una dimensi√≥n no deber√≠a ser tratada como m√©trica
        
        return True


    # =====================================================================
    # =========== INTEGRACI√ìN CON PIPELINE EXISTENTE ====================
    # =====================================================================

    def detect_column_value_patterns_english_with_implicit(self, tokens: List[str], temporal_filters: List[TemporalFilter]) -> List[ColumnValuePair]:
        """
        üéØ VERSI√ìN MEJORADA QUE INCLUYE: impl√≠citos + especiales (this week, enhanced stock out)
        MANTIENE EL NOMBRE ORIGINAL DEL M√âTODO
        """
        
        print(f"üéØ DETECTING ENGLISH COLUMN-VALUE PATTERNS (WITH ENHANCED STOCK OUT - ORIGINAL METHOD):")
                
        all_column_value_pairs = []
        all_processed_positions = set()

        # üÜï PASO 0.1: Identificar posiciones que son GROUP BY (no filtros)
        groupby_positions = set()
        
        for i, token in enumerate(tokens):
            if token.lower() == 'by' and i + 1 < len(tokens):
                next_token = tokens[i + 1]
                # Usar m√©todo existente que consulta diccionarios
                column_info = self._identify_potential_column_english(next_token)
                
                if column_info['is_column'] and column_info['type'] == 'dimension':
                    groupby_positions.add(i + 1)  # Marcar posici√≥n de la dimensi√≥n
                    print(f"   üìç Excluding position {i + 1} ('{next_token}') from implicit values - it's GROUP BY")

        # STEP 1: DETECTAR PATRONES ESPECIALES PRIMERO (mayor prioridad)

        # 1.1: Detectar patr√≥n THIS WEEK
        this_week_pattern = self.detect_this_week_pattern_english(tokens)
        if this_week_pattern:
            # Agregar como filtro temporal especial
            special_temporal_filter = TemporalFilter(
                indicator="this_week",
                quantity=1,
                unit=TemporalUnit.WEEKS,
                confidence=this_week_pattern.confidence,
                filter_type="current_week"
            )
            temporal_filters.append(special_temporal_filter)
            
            # Marcar posiciones como procesadas
            for pos in range(this_week_pattern.position_start, this_week_pattern.position_end + 1):
                all_processed_positions.add(pos)
            
            print(f"   üìÖ THIS WEEK pattern processed - added to temporal filters")

        # 1.2: Detectar patr√≥n ENHANCED YN PATTERNS
        print(f"   üîß DEBUG: Llamando a detect_enhanced_stock_out_pattern_english...")
        enhanced_yn_pattern = self.detect_enhanced_yn_column_pattern_english(tokens)

        if enhanced_yn_pattern:
            # Crear filtro de columna gen√©rico
            yn_pair = ColumnValuePair(
                column_name=enhanced_yn_pattern.column_name,  # Usa la columna detectada
                value=enhanced_yn_pattern.value,               # Usa el valor Y/N
                confidence=enhanced_yn_pattern.confidence,
                raw_text=enhanced_yn_pattern.indicator_text
            )
            
            all_column_value_pairs.append(yn_pair)
            
            print(f"   üì¶ ENHANCED Y/N pattern processed: {enhanced_yn_pattern.column_name} = '{enhanced_yn_pattern.value}'")

        
        # STEP 1.5: DETECTAR VALORES IMPL√çCITOS (l√≥gica existente)
        implicit_filters, implicit_positions = self.detect_implicit_value_patterns_english(tokens)
        
        # üÜï Filtrar implicit_filters que est√©n en groupby_positions
        filtered_implicit_filters = []
        for filter_item in implicit_filters:
            # Usar raw_text para buscar la posici√≥n original
            filter_text = filter_item.raw_text.lower()
            filter_conflicts_groupby = False
            
            # Verificar si este filtro conflict√∫a con alguna posici√≥n de GROUP BY
            for gb_pos in groupby_positions:
                if gb_pos < len(tokens) and tokens[gb_pos].lower() in filter_text:
                    filter_conflicts_groupby = True
                    print(f"   üö´ Excluding implicit filter: {filter_item.column_name} = {filter_item.value} (GROUP BY conflict with position {gb_pos})")
                    break
            
            if not filter_conflicts_groupby:
                filtered_implicit_filters.append(filter_item)
        
        implicit_filters = filtered_implicit_filters
        
        if implicit_filters:
            all_column_value_pairs.extend(implicit_filters)
            all_processed_positions.update(implicit_positions)
            
            print(f"   ‚úÖ Implicit filters found: {len(implicit_filters)}")
            for filter in implicit_filters:
                print(f"      üîç {filter.column_name} = '{filter.value}' (confidence: {filter.confidence:.2f})")
            
        # STEP 2: DETECTAR PATRONES EXPL√çCITOS EN POSICIONES NO PROCESADAS
        # (Reutilizar la l√≥gica existente pero evitando posiciones ya procesadas)
        
        # Identificar columnas temporales
        temporal_columns = set()
        for tf in temporal_filters:
            if tf.unit == TemporalUnit.WEEKS:
                temporal_columns.update(['week', 'weeks', 'semana', 'semanas'])
            elif tf.unit == TemporalUnit.MONTHS:
                temporal_columns.update(['month', 'months', 'mes', 'meses'])
            elif tf.unit == TemporalUnit.DAYS:
                temporal_columns.update(['day', 'days', 'dia', 'dias'])
            elif tf.unit == TemporalUnit.YEARS:
                temporal_columns.update(['year', 'years', 'a√±o', 'a√±os'])
        
        print(f"‚è∞ Temporal columns to exclude: {temporal_columns}")
        print(f"üîí Positions already processed: {sorted(all_processed_positions)}")
        
        # CONTROL ESTRICTO DE DUPLICADOS
        processed_positions = set(all_processed_positions)  # Copiar posiciones ya procesadas
        created_filters = set()  # Para evitar filtros duplicados por contenido
        
        # PASO 1: DETECTAR PATRONES DIRECTOS CON DICCIONARIO TEMPORAL (PRIORIDAD M√ÅXIMA)
        for i in range(len(tokens) - 1):
            if i in processed_positions:
                continue
                
            current_token = tokens[i]
            
            # Verificar si es columna potencial
            column_info = self._identify_potential_column_english(current_token)
            if not column_info['is_column'] or column_info['normalized_name'] in temporal_columns:
                continue
            
            print(f"üîç Testing TEMPORAL DICT pattern: '{current_token}' + [value from dictionary]")
            
            dict_result = self._extract_value_from_temporal_dict(tokens, i + 1, column_info['normalized_name'])
            
            if dict_result:
                # üÜï CREAR CLAVE √öNICA PARA EVITAR DUPLICADOS
                filter_key = f"{column_info['normalized_name']}={dict_result['normalized_value']}"
                
                if filter_key not in created_filters:
                    column_value_pairs = ColumnValuePair(
                        column_name=column_info['normalized_name'],
                        value=dict_result['normalized_value'],
                        confidence=dict_result['confidence'],
                        raw_text=f"{current_token} {dict_result['raw_text']}"
                    )
                    
                    all_column_value_pairs.append(column_value_pairs)
                    created_filters.add(filter_key)
                    print(f"‚úÖ TEMPORAL DICT SUCCESS: {current_token} = '{dict_result['normalized_value']}'")
                    
                    # üîß FIX: Manejar None en tokens_consumed
                    tokens_consumed = dict_result.get('tokens_consumed')
                    if tokens_consumed is None or not isinstance(tokens_consumed, int):
                        # Calcular tokens_consumed basado en raw_text
                        raw_text = dict_result.get('raw_text', '')
                        if raw_text:
                            tokens_consumed = len(raw_text.split())
                        else:
                            tokens_consumed = 1
                        print(f"‚ö†Ô∏è WARNING: tokens_consumed was None, calculated as {tokens_consumed}")
                    
                    # üö® MARCAR TODAS LAS POSICIONES COMO PROCESADAS
                    end_position = min(i + 1 + tokens_consumed, len(tokens))
                    for pos in range(i, end_position):
                        processed_positions.add(pos)
                    
                    print(f"üîí POSITIONS LOCKED: {list(range(i, end_position))}")
                else:
                    print(f"üîÑ TEMPORAL DICT DUPLICATE AVOIDED: {filter_key}")
        
        # PASO 3: PATRONES EXPL√çCITOS CON PREPOSICIONES (respetando posiciones procesadas)
        i = 0
        while i < len(tokens) - 2:
            # Verificar si la posici√≥n ya fue procesada
            if i in processed_positions:
                print(f"‚≠ï SKIPPING position {i} (processed by special patterns)")
                i += 1
                continue
            
            # PATTERN: [preposition] [column] [value]
            positions_needed = {i, i + 1, i + 2}
            if positions_needed.intersection(processed_positions):
                print(f"‚≠ï PREPOSITION PATTERN: positions {positions_needed} overlap with processed {processed_positions}")
                i += 1
                continue
            
            pattern_result = self._detect_preposition_column_value_pattern_english(
                tokens, i, temporal_columns, processed_positions
            )
            
            if pattern_result:
                # Verificar duplicados por contenido
                filter_key = f"{pattern_result['pair'].column_name}={pattern_result['pair'].value}"
                existing_filters = {f"{cvp.column_name}={cvp.value}" for cvp in all_column_value_pairs}
                
                if filter_key not in existing_filters:
                    all_column_value_pairs.append(pattern_result['pair'])
                    print(f"‚úÖ EXPLICIT FILTER CREATED (preposition): {pattern_result['raw_text']}")
                    
                    # Marcar posiciones como procesadas
                    tokens_consumed = pattern_result.get('tokens_consumed', 3)  # üîß FIX: Default value
                    for pos in range(i, i + tokens_consumed):
                        processed_positions.add(pos)
                else:
                    print(f"üîÑ EXPLICIT DUPLICATE AVOIDED: {filter_key}")
                
                i += pattern_result.get('tokens_consumed', 3)  # üîß FIX: Default value
                continue
            
            i += 1
        
        print(f"üéØ Total filters detected: {len(all_column_value_pairs)}")
        print(f"   üì¶ Enhanced stock out: {len([cvp for cvp in all_column_value_pairs if cvp.column_name == 'Stock_Out'])}")
        print(f"   üîç Implicit: {len(implicit_filters)}")
        print(f"   üîç Explicit: {len(all_column_value_pairs) - len(implicit_filters) - len([cvp for cvp in all_column_value_pairs if cvp.column_name == 'Stock_Out'])}")
        print(f"üîí Final processed positions: {sorted(processed_positions)}")
            
        # üÜï ELIMINAR FILTROS DUPLICADOS
        print(f"üîß REMOVING DUPLICATE FILTERS:")
        seen_filters = {}
        unique_filters = []
        
        for filter_item in all_column_value_pairs:
            filter_key = f"{filter_item.column_name}={filter_item.value}"
            if filter_key not in seen_filters:
                seen_filters[filter_key] = True
                unique_filters.append(filter_item)
            else:
                print(f"   üîÑ Removing duplicate filter: {filter_key}")
        
        all_column_value_pairs = unique_filters
        print(f"üéØ Final unique filters: {len(all_column_value_pairs)}")

        return all_column_value_pairs


    # =====================================================================
    # =========== ACTUALIZACI√ìN DEL PIPELINE PRINCIPAL ==================
    # =====================================================================

    def process_query_with_implicit_values(self, query: str, pre_normalized_query: str, preliminary_tokens: List[str]) -> Dict:
        """
        üá∫üá∏ PIPELINE PRINCIPAL ACTUALIZADO CON SOPORTE PARA VALORES IMPL√çCITOS
        Esta funci√≥n reemplaza o extiende el process_query original
        """
        
        print(f"\nüá∫üá∏ PROCESSING ENGLISH QUERY WITH IMPLICIT VALUES: '{query}'")
        
    # STEP 1: NORMALIZATION (English-specific)
        normalized_query = self.normalize_english_query(pre_normalized_query)
        tokens = normalized_query.split()
        
        print(f"üî§ English tokens: {tokens}")   
        
    # STEP 2: SEMANTIC ANALYSIS (reuse existing)
        original_intent = self.pre_mapping_analyzer.analyze_original_intent(tokens)
        print(f"üß† English semantic intent: {original_intent}")
           
    # STEP 2.5: DETECTAR MULTI-M√âTRICA TEMPRANO
        multi_metric_pattern = self.detect_multi_metric_pattern_english(tokens)
        if multi_metric_pattern and multi_metric_pattern.confidence >= 0.8:
            print(f"üìä MULTI-METRIC pattern detected early - using optimized path")
            
            # Generar SQL directamente para multi-m√©trica
            multi_metric_sql = self.generate_multi_metric_sql_direct(multi_metric_pattern, normalized_query, query)
            if multi_metric_sql:
                return multi_metric_sql      
            
    # STEP 3: ENGLISH-SPECIFIC PATTERN DETECTION CON VALORES IMPL√çCITOS
        temporal_filters = self.detect_temporal_patterns_english(tokens)
        
    # STEP 3.2: USAR LA NUEVA FUNCI√ìN QUE INCLUYE VALORES IMPL√çCITOS
        column_value_pairs = self.detect_column_value_patterns_english_with_implicit(tokens, temporal_filters)  
        
    # STEP 3.5: Otros patrones (temporal conditional, list all, show rows)
        temporal_conditional_pattern = self.detect_temporal_conditional_pattern_english(tokens)
        list_all_pattern = self.detect_list_all_pattern_english(tokens)
        show_rows_pattern = self.detect_show_rows_pattern_english(tokens)
                                
    # STEP 4: COMPONENT CLASSIFICATION (reuse with adaptations)
        classified_components = self.classify_components_english(tokens, column_value_pairs)
        
    # STEP 5: STRUCTURE BUILDING (usar el m√©todo existente)
        query_structure = self.build_english_structure(classified_components, column_value_pairs, temporal_filters, tokens, original_intent)
        
        # Agregar patrones especiales detectados
        if temporal_conditional_pattern:
            query_structure.temporal_conditional_pattern = temporal_conditional_pattern
        
        if list_all_pattern:
            query_structure.list_all_pattern = list_all_pattern
            
        if show_rows_pattern:
            query_structure.show_rows_pattern = show_rows_pattern
        
    # STEP 6: VALIDATION (reuse existing)
        validation_result = self.validate_english_structure(query_structure)
        
        if not validation_result['valid']:
            return {
                'success': False,
                'error': validation_result['error'],
                'original_input': query,
                'suggestions': validation_result['suggestions'],
                'language': 'english'
            }
        
    # STEP 7: SQL GENERATION
        conceptual_sql = self.generate_optimized_sql_english(query_structure)

    # STEP 8: SQL SCHEMA NORMALIZATION (con fallback)
        if self.sql_mapper:
            try:
                normalized_sql = self.sql_mapper.normalize_sql(conceptual_sql)
            except Exception as e:
                print(f"‚ö†Ô∏è Schema mapping failed: {e}")
                normalized_sql = conceptual_sql
        else:
            normalized_sql = conceptual_sql
            
    # STEP 9: FINAL RESULT
        return {
            'success': True,
            'language': 'english',
            'original_input': query,
            'normalized_query': normalized_query,
            'tokens': tokens,
            'conceptual_sql': conceptual_sql,
            'sql_query': normalized_sql,
            'complexity_level': query_structure.get_complexity_level(),
            'processing_method': 'english_pipeline_with_implicit_values',
            'note': 'üá∫üá∏ Processed with English patterns + Implicit Value Detection',
            'query_structure': self.structure_to_dict_english(query_structure),
            'hierarchical_structure': self.generate_hierarchical_structure_english(query_structure),
            'interpretation': self.generate_natural_interpretation_english(query_structure),
            'confidence': self.calculate_overall_confidence_english(query_structure),
            'schema_mapping_stats': self.sql_mapper.get_mapping_statistics() if self.sql_mapper else {},
            'implicit_values_detected': len([cvp for cvp in column_value_pairs if 'implicit' in cvp.raw_text.lower()]) # Estad√≠stica adicional
        }
        
        
    def _extract_value_from_temporal_dict(self, tokens: List[str], start_idx: int, column_name: str) -> Optional[Dict]:
        """
        üóÑÔ∏è EXTRACTOR ESPEC√çFICO PARA DICCIONARIO TEMPORAL
        Prueba combinaciones de tokens contra el diccionario temporal
        """
        if (not hasattr(self.dictionaries, 'temporal_dictionary') or 
            start_idx >= len(tokens)):
            return None
        
        print(f"      üóÑÔ∏è Searching temporal dictionary starting at position {start_idx}")
        
        # Probar combinaciones desde la m√°s larga (6 tokens) hasta 1 token
        max_tokens = min(6, len(tokens) - start_idx)
        
        for length in range(max_tokens, 0, -1):
            if start_idx + length > len(tokens):
                continue
            
            candidate_tokens = tokens[start_idx:start_idx + length]
            
            print(f"         üîç Testing {length} tokens: {candidate_tokens}")
            
            # Generar variantes para buscar en el diccionario
            test_variants = [
                ' '.join(candidate_tokens).lower(),           # "palacio de hierro"
                ''.join(candidate_tokens).lower(),            # "palaciodehierro"  
                '_'.join(candidate_tokens).lower(),           # "palacio_de_hierro"
                ' '.join(candidate_tokens).upper(),           # "PALACIO DE HIERRO"
                ''.join(candidate_tokens).upper(),            # "PALACIODEHIERRO"
                '_'.join(candidate_tokens).upper(),           # "PALACIO_DE_HIERRO"
            ]
            
            for variant in test_variants:
                print(f"            üîç Testing variant: '{variant}'")
                
                temporal_entry = self.dictionaries.search_in_temporal_dictionary(variant)
                
                if temporal_entry:
                    # Verificar que la columna coincida
                    entry_column = temporal_entry.get('column_name', '').lower()
                    if entry_column == column_name.lower():
                        
                        original_value = temporal_entry.get('original_value', variant.upper())
                        confidence = temporal_entry.get('confidence', 0.95)
                        
                        print(f"            ‚úÖ PERFECT MATCH: '{variant}' ‚Üí '{original_value}' (column: {entry_column})")
                        
                        return {
                            'normalized_value': original_value,
                            'raw_text': ' '.join(candidate_tokens),
                            'tokens_consumed': length if length else len(candidate_tokens),  # FIX
                            'confidence': confidence
                        }
                    else:
                        print(f"            ‚ùå Column mismatch: found '{entry_column}', expected '{column_name.lower()}'")
        
        print(f"         ‚ùå No matches found in temporal dictionary")
        return None      
          
                
    def detect_temporal_conditional_pattern_english(self, tokens: List[str]) -> Optional[Dict]:
        """üïê DETECTOR MEJORADO DE PATR√ìN TEMPORAL CONDICIONAL EN INGL√âS"""
        print(f"üïê DETECTING TEMPORAL CONDITIONAL PATTERN:")
        print(f"   üì§ Tokens: {tokens}")
        
        if len(tokens) < 5:  # M√≠nimo: week where store had sales
            return None
        
        # STEP 1: Verificar que empiece con dimensi√≥n temporal (SINGULAR O PLURAL)
        first_token = tokens[0].lower()
        
        # üîß MAPEO DE PLURALES A SINGULARES
        temporal_plural_map = {
            'weeks': 'week',
            'months': 'month', 
            'days': 'day',
            'years': 'year',
            'quarters': 'quarter'
        }
        
        # Normalizar plural a singular si es necesario
        normalized_first = temporal_plural_map.get(first_token, first_token)
        
        temporal_dimensions = {'week', 'month', 'day', 'year', 'quarter'}
        
        if normalized_first not in temporal_dimensions:
            print(f"   ‚ùå No temporal dimension at start: '{first_token}'")
            return None
        
        # STEP 2: Buscar "where" (debe estar en posici√≥n 1 o 2)
        where_pos = -1
        for i in range(1, min(3, len(tokens))):
            if tokens[i].lower() == 'where':
                where_pos = i
                break
        
        if where_pos == -1:
            print(f"   ‚ùå No 'where' found after temporal dimension")
            return None
        
        print(f"   ‚úÖ Temporal dimension: '{normalized_first}' (from '{first_token}')")
        print(f"   ‚úÖ 'where' found at position {where_pos}")
        
        # STEP 3: Extraer componentes despu√©s del where
        remaining_tokens = tokens[where_pos + 1:]
        components = self._extract_enhanced_conditional_components_english(remaining_tokens)
        
        if not components:
            print(f"   ‚ùå Could not extract conditional components")
            return None
        
        # STEP 4: Construir resultado mejorado (usar la forma normalizada)
        pattern_result = {
            'pattern_type': 'TEMPORAL_CONDITIONAL',
            'temporal_dimension': normalized_first,  # Usar singular normalizado
            'entity_column': components['entity_column'],
            'entity_value': components['entity_value'],
            'condition_verb': components['condition_verb'],
            'comparative': components['comparative'],
            'target_metric': components['target_metric'],
            'order_direction': components['order_direction'],
            'confidence': components['confidence'],
            'raw_tokens': tokens
        }
        
        print(f"üïê TEMPORAL CONDITIONAL PATTERN DETECTED:")
        print(f"   ‚è∞ Temporal: {pattern_result['temporal_dimension']}")
        print(f"   üéØ Entity: {pattern_result['entity_column']} = '{pattern_result['entity_value']}'")
        print(f"   üîÑ Verb: {pattern_result['condition_verb']}")
        print(f"   üìä Metric: {pattern_result['target_metric']}")
        print(f"   üîº Direction: {pattern_result['order_direction']}")
        
        return pattern_result
            
                            
                    
    def _extract_enhanced_conditional_components_english(self, tokens: List[str]) -> Optional[Dict]:
        """üîç EXTRACTOR MEJORADO DE COMPONENTES CONDICIONALES"""
        print(f"      üîç Extracting from: {tokens}")
        
        if len(tokens) < 3:  # M√≠nimo: sams had sales
            return None
        
        # Inicializar valores
        entity_column = None
        entity_value = None
        condition_verb = None
        target_metric = None
        comparative = 'more'  # Default
        order_direction = 'DESC'  # Default
        verb_start_pos = -1
        
        # STEP 1: Buscar entidad (columna + valor)
        for i in range(len(tokens) - 2):
            current_token = tokens[i]
            next_token = tokens[i + 1] if i + 1 < len(tokens) else None
            
            # Verificar si es columna potencial (store, account, item, etc.)
            if self._is_potential_column_english(current_token):
                # Verificar si el siguiente es un valor
                if next_token and self._is_potential_value_english(next_token):
                    entity_column = current_token.lower()
                    entity_value = next_token.upper()
                    verb_start_pos = i + 2
                    print(f"         ‚úÖ Entity found: {entity_column} = '{entity_value}'")
                    break
            
            # Verificar si es un valor conocido del diccionario temporal
            if hasattr(self.dictionaries, 'search_in_temporal_dictionary'):
                temp_result = self.dictionaries.search_in_temporal_dictionary(current_token)
                if temp_result:
                    entity_column = temp_result.get('column_name', 'account').lower()
                    entity_value = temp_result.get('original_value', current_token.upper())
                    verb_start_pos = i + 1
                    print(f"         ‚úÖ Entity from dictionary: {entity_column} = '{entity_value}'")
                    break
        
        # Si no encontramos entidad, empezar desde el principio
        if verb_start_pos == -1:
            verb_start_pos = 0
        
        # STEP 2: Buscar verbo temporal
        condition_verbs = {
            'had', 'has', 'got', 'achieved', 'reached', 'obtained',
            'generated', 'produced', 'made', 'recorded', 'showed', 'with'
        }
        
        for i in range(verb_start_pos, min(verb_start_pos + 2, len(tokens))):
            if i < len(tokens) and tokens[i].lower() in condition_verbs:
                condition_verb = tokens[i].lower()
                comparative_start_pos = i + 1
                print(f"         ‚úÖ Verb found: '{condition_verb}'")
                break
        
        # Si no encontramos verbo, buscar desde el inicio
        if not condition_verb:
            for token in tokens:
                if token.lower() in condition_verbs:
                    condition_verb = token.lower()
                    print(f"         ‚úÖ Verb found: '{condition_verb}'")
                    break
        
        # STEP 3: Buscar comparativo + m√©trica
        comparative_map = {
            # Positivos (ORDER BY DESC)
            'more': 'DESC', 'most': 'DESC', 'highest': 'DESC',
            'best': 'DESC', 'maximum': 'DESC', 'greater': 'DESC',
            # Negativos (ORDER BY ASC)
            'less': 'ASC', 'least': 'ASC', 'lowest': 'ASC',
            'worst': 'ASC', 'minimum': 'ASC', 'smaller': 'ASC'
        }
        
        for token in tokens:
            token_lower = token.lower()
            
            # Si encontramos un comparativo
            if token_lower in comparative_map:
                comparative = token_lower
                order_direction = comparative_map[token_lower]
                print(f"         ‚úÖ Comparative: '{comparative}' ‚Üí {order_direction}")
                break
        
        # STEP 4: Buscar m√©trica objetivo
        metric_keywords = {
            'sales', 'revenue', 'inventory', 'profit', 'margin',
            'cost', 'units', 'sell_out', 'stock', 'amount', 'quantity'
        }
        
        for token in tokens:
            if token.lower() in metric_keywords:
                target_metric = token.lower()
                print(f"         ‚úÖ Metric: '{target_metric}'")
                break
        
        # Si no encontramos m√©trica, usar 'sales' como default
        if not target_metric:
            # Mapear m√©tricas comunes seg√∫n el contexto
            if 'sales' in ' '.join(tokens).lower() or 'sell' in ' '.join(tokens).lower():
                target_metric = 'Sell_Out'
            else:
                target_metric = 'Sell_Out'  # Default m√°s com√∫n
            print(f"         ‚ÑπÔ∏è Using default metric: '{target_metric}'")
        
        # Si no encontramos verbo, usar 'had' como default
        if not condition_verb:
            condition_verb = 'had'
            print(f"         ‚ÑπÔ∏è Using default verb: 'had'")
        
        # STEP 5: Calcular confianza
        confidence = 0.6  # Base
        if entity_column and entity_value:
            confidence += 0.2
        if condition_verb:
            confidence += 0.1
        if comparative != 'more':  # Comparativo expl√≠cito
            confidence += 0.1
        
        return {
            'entity_column': entity_column,
            'entity_value': entity_value,
            'condition_verb': condition_verb,
            'comparative': comparative,
            'target_metric': target_metric,
            'order_direction': order_direction,
            'confidence': min(1.0, confidence)
        }
                    


    def _detect_preposition_column_value_pattern_english(self, tokens: List[str], start_idx: int, temporal_columns: set, processed_positions: set = None) -> Optional[Dict]:
        """
        Detecta patrones: [preposition] [column] [value] - CON VERIFICACI√ìN DE POSICIONES
        """
        if processed_positions is None:
            processed_positions = set()
        
        # üÜï VERIFICAR SI YA EST√ÅN PROCESADAS LAS POSICIONES
        positions_to_check = {start_idx, start_idx + 1, start_idx + 2}
        if positions_to_check.intersection(processed_positions):
            print(f"   üîÑ Positions {positions_to_check} already processed, skipping")
            return None
        
        if start_idx + 2 >= len(tokens):
            return None
        
        preposition_token = tokens[start_idx]
        column_token = tokens[start_idx + 1] 
        value_token = tokens[start_idx + 2]
        
        english_prepositions = {'with', 'from', 'for', 'by', 'in', 'on', 'at', 'of'}
        
        if preposition_token.lower() not in english_prepositions:
            return None
        
        print(f"üîç Analyzing English preposition pattern: '{preposition_token}' + '{column_token}' + '{value_token}'")
        
        column_info = self._identify_potential_column_english(column_token)
        print(f"     Column? {column_info}")
        
        if not column_info['is_column']:
            return None
        
        if column_info['normalized_name'] in temporal_columns:
            print(f"‚è∞ Skipping '{column_token}' - already processed as temporal")
            return None
        
        value_info = self._identify_potential_value_english(value_token, start_idx + 2, tokens)
        print(f"     Value? {value_info}")
        
        if not value_info['is_value']:
            return None
        
        confidence_adjustment = 0.95
        final_confidence = min(column_info['confidence'], value_info['confidence']) * confidence_adjustment
        
        pair = ColumnValuePair(
            column_name=column_info['normalized_name'],
            value=value_info['normalized_value'],
            confidence=final_confidence,
            raw_text=f"{preposition_token} {column_token} {value_token}"
        )
        
        return {
            'pair': pair,
            'tokens_consumed': 3,
            'raw_text': f"{preposition_token} {column_token} = '{value_token}'"
        }


    def _identify_potential_column_english(self, token: str) -> Dict:
        """Identificador de Columnas Potenciales para ingl√©s - VERSI√ìN MEJORADA"""
        token_lower = token.lower()
        
        # üîß DESCARTAR: Modificadores de agregaci√≥n que NO son columnas
        aggregate_modifiers = {'total', 'sum', 'average', 'avg', 'max', 'min', 'count'}
        if token_lower in aggregate_modifiers:
            return {
                'is_column': False,
                'normalized_name': None,
                'type': 'aggregate_modifier',
                'confidence': 0.0
            }
        
        # üîß DESCARTAR: Conectores y palabras de enlace
        link_words = {'of', 'for', 'in', 'on', 'at', 'by', 'from', 'to', 'with'}
        if token_lower in link_words:
            return {
                'is_column': False,
                'normalized_name': None,
                'type': 'link_word',
                'confidence': 0.0
            }
        
        # üîß DESCARTAR: Errores tipogr√°ficos comunes que no son columnas
        common_typos = {'ammount', 'amout', 'summ', 'totall', 'avrage'}
        if token_lower in common_typos:
            return {
                'is_column': False,
                'normalized_name': None,
                'type': 'typo',
                'confidence': 0.0
            }
        
        # L√ìGICA ORIGINAL (mantener)
        if token_lower in self.dictionaries.dimensiones:
            return {
                'is_column': True,
                'normalized_name': token_lower,
                'type': 'dimension',
                'confidence': 0.95
            }
        
        if token_lower in self.dictionaries.metricas:
            return {
                'is_column': True,
                'normalized_name': token_lower,
                'type': 'metric',
                'confidence': 0.90
            }
        
        # Buscar en frases compuestas (ej: stock_out)
        if token_lower in self.dictionaries.frases_compuestas:
            normalized = self.dictionaries.frases_compuestas[token_lower]
            return {
                'is_column': True,
                'normalized_name': normalized,
                'type': 'compound',
                'confidence': 0.95
            }
        
        # Detectar nombres de columnas con snake_case
        if self._looks_like_column_name_english(token):
            return {
                'is_column': True,
                'normalized_name': token_lower,
                'type': 'inferred',
                'confidence': 0.70
            }
        
        return {
            'is_column': False,
            'normalized_name': None,
            'type': None,
            'confidence': 0.0
        }
        

    def _identify_potential_value_english(self, token: str, position: int, tokens: List[str]) -> Dict:
        """Identificador de Valores Espec√≠ficos para ingl√©s"""
        
        # PRIORIDAD M√ÅXIMA: Letras individuales may√∫sculas
        if len(token) == 1 and token.isupper() and token.isalpha():
            return {
                'is_value': True,
                'normalized_value': token,
                'confidence': 0.98
            }
        
        token_lower = token.lower()
        token_upper = token.upper()
        
        
        # DESCARTAR: Palabras del lenguaje natural en ingl√©s
        english_language_words = self.dictionaries.conectores.union({
            'between', 'from', 'to', 'with', 'and', 'or', 'but'
        })
        
        if token_lower in english_language_words and token != 'Y':
            return {'is_value': False, 'normalized_value': None, 'confidence': 0.0}
        
        # DESCARTAR: Usar diccionarios para operaciones y m√©tricas
        if token_lower in self.dictionaries.operaciones:
            return {'is_value': False, 'normalized_value': None, 'confidence': 0.0}
        
        if token_lower in self.dictionaries.metricas:
            return {'is_value': False, 'normalized_value': None, 'confidence': 0.0}
        
        if token_lower in self.dictionaries.dimensiones:
            return {'is_value': False, 'normalized_value': None, 'confidence': 0.0}

        # REGLA GEN√âRICA: C√≥digos alfanum√©ricos
        if self._is_generic_code_value_english(token):
            context_confidence = self._calculate_generic_context_confidence_english(token, position, tokens)
            return {
                'is_value': True,
                'normalized_value': token_upper,
                'confidence': context_confidence
            }

        # REGLAS B√ÅSICAS
        if len(token) == 1 and token.isalpha():
            return {
                'is_value': True,
                'normalized_value': token_upper,
                'confidence': 0.90
            }
        
        if token.isdigit():
            return {
                'is_value': True,
                'normalized_value': token,
                'confidence': 0.95
            }
        
        # Estados comunes en ingl√©s
        common_english_states = {
            'active', 'inactive', 'pending', 'completed', 'cancelled',
            'yes', 'no', 'true', 'false', 'on', 'off',
            'high', 'medium', 'low', 'premium', 'basic', 'vip'
        }
        if token_lower in common_english_states:
            return {
                'is_value': True,
                'normalized_value': token_upper,
                'confidence': 0.85
            }
        
        return {'is_value': False, 'normalized_value': None, 'confidence': 0.0}

    def _looks_like_column_name_english(self, token: str) -> bool:
        """Detecta si parece nombre de columna en ingl√©s"""
        token_lower = token.lower()
        
        # Snake case (ej: stock_out, dead_inventory)
        if '_' in token_lower:
            return True
        
        # Nombres comunes de columnas en ingl√©s
        common_column_patterns = {
            'account', 'product', 'customer', 'store', 'item', 'order',
            'status', 'type', 'code', 'id', 'name', 'date', 'amount'
        }
        
        if token_lower in common_column_patterns:
            return True
        
        # Termina en sufijos comunes
        if token_lower.endswith(('_id', '_code', '_status', '_type', '_date')):
            return True
        
        return False

    def _is_generic_code_value_english(self, token: str) -> bool:
        """Detecta c√≥digos gen√©ricos en ingl√©s"""
        if not re.match(r'^[A-Za-z0-9\-/\.]+$', token):
            return False
        
        if len(token) < 3:
            return False
        
        has_letter = any(c.isalpha() for c in token)
        has_number = any(c.isdigit() for c in token)
        
        if has_letter and has_number:
            return True
        
        if has_letter and not has_number and len(token) >= 2:
            return True
        
        if has_number and not has_letter and len(token) >= 4:
            return True
        
        return False

    def _calculate_generic_context_confidence_english(self, token: str, position: int, tokens: List[str]) -> float:
        """Calcula confianza basada en contexto para ingl√©s"""
        base_confidence = 0.75
        
        # Si est√° despu√©s de preposici√≥n, aumentar confianza
        if position > 0:
            prev_token = tokens[position - 1].lower()
            if prev_token in {'with', 'for', 'by', 'of'}:
                base_confidence += 0.1
        
        # Si es largo y alfanum√©rico, probablemente es c√≥digo
        if len(token) > 8 and any(c.isalpha() for c in token) and any(c.isdigit() for c in token):
            base_confidence += 0.05
        
        return min(0.95, base_confidence)

    def _is_potential_column_english(self, token: str) -> bool:
        """üá∫üá∏ VERIFICADOR DE COLUMNAS - NUNCA MAY√öSCULAS INDIVIDUALES"""
        
        # üîß REGLA ABSOLUTA: Letras may√∫sculas individuales NUNCA son columnas
        if len(token) == 1 and token.isupper() and token.isalpha():
            return False
        
        token_lower = token.lower()
        return (token_lower in self.dictionaries.dimensiones or 
                token_lower in self.dictionaries.metricas or
                '_' in token or
                token_lower in ['account', 'product', 'customer', 'partner', 'region', 'status', 'type', 'store', 'stock_out'])

            
    
    def classify_components_english(self, tokens: List[str], column_value_pairs: List[ColumnValuePair]) -> Dict[str, QueryComponent]:
        """üá∫üá∏ CLASIFICACI√ìN DE COMPONENTES EN INGL√âS"""
        
        print(f"üîç CLASSIFYING ENGLISH COMPONENTS:")
        
        classified = {}
        processed_tokens = set()
        
        # Obtener posiciones procesadas por patrones temporales
        temporal_positions = getattr(self, 'temporal_processed_positions', set())
        if temporal_positions:
            print(f"   ‚è∞ Temporal positions to skip: {sorted(temporal_positions)}")
        
        # üîß NUEVO: Obtener posiciones procesadas por valores impl√≠citos
        implicit_positions = set()
        
        # Mark tokens used in column-value pairs (incluye impl√≠citos)
        for cvp in column_value_pairs:
            pair_tokens = cvp.raw_text.split()
            processed_tokens.update(pair_tokens)
            
            # üîß NUEVO: Si es un valor multi-palabra, marcar las posiciones
            # de todos los tokens individuales
            for i, token in enumerate(tokens):
                # Buscar si este token es parte del raw_text del filtro
                if token.lower() in cvp.raw_text.lower():
                    # Verificar si es parte del valor compuesto
                    raw_lower = cvp.raw_text.lower()
                    token_lower = token.lower()
                    
                    # Si el raw_text contiene m√∫ltiples palabras y este token es una de ellas
                    if ' ' in raw_lower and token_lower in raw_lower.split():
                        implicit_positions.add(i)
                        print(f"   üîí Token '{token}' at position {i} is part of implicit value '{cvp.raw_text}'")
            
            print(f"üîó English filter detected: {cvp.column_name} = '{cvp.value}'")
        
        # Classify individual tokens
        for i, token in enumerate(tokens):
            # Saltar tokens en posiciones temporales
            if i in temporal_positions:
                print(f"   ‚è≠Ô∏è Skipping position {i} ('{token}') - used in temporal pattern")
                continue
            
            # üîß NUEVO: Saltar tokens que son parte de valores impl√≠citos multi-palabra
            if i in implicit_positions:
                print(f"   ‚è≠Ô∏è Skipping position {i} ('{token}') - part of implicit multi-word value")
                continue
            
            classified[token] = self.classify_single_component_english(token)
            
            if token in processed_tokens:
                classified[token].linguistic_info['used_in_filter'] = True
                print(f"üéØ English token '{token}' classified as {classified[token].type.value} (used in filter)")
            else:
                print(f"üîç English token '{token}' classified as {classified[token].type.value}")
        
        return classified

                     
    def classify_single_component_english(self, token: str) -> QueryComponent:
        # PRIORIDAD 0: Buscar PRIMERO en diccionario temporal
        if hasattr(self.dictionaries, 'temporal_dictionary'):
            # Buscar el token en el diccionario temporal
            temp_result = self.dictionaries.search_in_temporal_dictionary(token.lower())
            if not temp_result:
                temp_result = self.dictionaries.search_in_temporal_dictionary(token.upper())
            
            if temp_result:
                return QueryComponent(
                    text=token,
                    type=ComponentType.VALUE,
                    confidence=0.95,  # Alta confianza porque est√° en el diccionario
                    subtype='dictionary_value',
                    value=temp_result.get('original_value'),
                    column_name=temp_result.get('column_name'),
                    linguistic_info={
                        'source': 'temporal_dictionary',
                        'column': temp_result.get('column_name'),
                        'original_value': temp_result.get('original_value')
                    }
                )
    
        # üÜï REGLA ESPECIAL: N√∫meros en contexto (para SHOW_ROWS y otros)
        if token.isdigit():
            return QueryComponent(
                text=token,
                type=ComponentType.VALUE,
                confidence=0.95,
                subtype='numeric_value',
                value=int(token),
                linguistic_info={'source': 'numeric_literal'}
            )
        
        # üÜï REGLA PRIORITARIA: Dimensiones temporales (SINGULAR Y PLURAL)
        temporal_dimensions = {'week', 'weeks', 'month', 'months', 'day', 'days', 'year', 'years', 'quarter', 'quarters'}
        if token.lower() in temporal_dimensions:
            # Normalizar plural a singular para el valor
            temporal_singular_map = {
                'weeks': 'week',
                'months': 'month',
                'days': 'day', 
                'years': 'year',
                'quarters': 'quarter'
            }
            normalized_value = temporal_singular_map.get(token.lower(), token.lower())
            
            return QueryComponent(
                text=token,
                type=ComponentType.DIMENSION,
                confidence=0.98,
                subtype='temporal_dimension',
                value=normalized_value,  # Guardar la forma singular
                linguistic_info={
                    'source': 'temporal_dimension_priority', 
                    'is_plural': token.lower() != normalized_value,
                    'original_form': token
                }
            )
        
        # Para 'with' espec√≠ficamente:
        if token.lower() == 'with':
            print(f"üîç DEBUG CR√çTICO para 'with':")
            print(f"   ¬øEn self.dictionaries.conectores? {'with' in self.dictionaries.conectores}")
            print(f"   Conectores disponibles: {list(self.dictionaries.conectores)[:10]}...")  # Primeros 10
            print(f"   get_component_type('with') = {self.dictionaries.get_component_type('with')}")
        
        # üîß REGLA ABSOLUTA #1: Letras may√∫sculas individuales SIEMPRE son datos
        if len(token) == 1 and token.isupper() and token.isalpha():
            return QueryComponent(
                text=token,
                type=ComponentType.VALUE,
                confidence=1.0,  # Confianza m√°xima
                subtype='table_data_absolute',
                value=token,
                linguistic_info={
                    'source': 'uppercase_letter_absolute_rule',
                    'is_table_data': True,
                    'never_connector': True,
                    'absolute_rule_applied': True
                }
            )
        
        # üîß REGLA ABSOLUTA #2: C√≥digos cortos en may√∫sculas tambi√©n son datos
        if token.isupper() and 2 <= len(token) <= 4 and any(c.isalpha() for c in token):
            return QueryComponent(
                text=token,
                type=ComponentType.VALUE,
                confidence=0.98,
                subtype='code_data_absolute',
                value=token,
                linguistic_info={
                    'source': 'uppercase_code_absolute_rule',
                    'is_table_data': True
                }
            )
        
        # English ranking indicators
        ranking_indicators = {
            'top', 'best', 'highest', 'maximum', 'first', 'greatest', 'most',
            'worst', 'lowest', 'minimum', 'last', 'least', 'bottom'
        }
        
        if token.lower() in ranking_indicators:
            return QueryComponent(
                text=token,
                type=ComponentType.OPERATION,
                confidence=0.90,
                subtype='ranking_indicator',
                value=token.lower(),
                linguistic_info={'source': 'english_ranking_indicator'}
            )
        
        # Continuar con el resto de la clasificaci√≥n existente...
        corrected_token = self.dictionaries.correct_typo(token)
        if corrected_token != token:
            corrected_component = self.classify_single_component_english(corrected_token)
            if corrected_component.type != ComponentType.UNKNOWN:
                corrected_component.linguistic_info = {
                    'source': 'typo_correction',
                    'original': token,
                    'corrected': corrected_token
                }
                corrected_component.confidence *= 0.85
                return corrected_component
        
        component_type = self.dictionaries.get_component_type(token)
        
        if component_type == ComponentType.DIMENSION:
            return QueryComponent(
                text=token,
                type=ComponentType.DIMENSION,
                confidence=0.95,
                linguistic_info={'source': 'dimension_dictionary'}
            )
        elif component_type == ComponentType.OPERATION:
            english_operations = {
                'max': 'm√°ximo', 'maximum': 'm√°ximo', 'highest': 'm√°ximo', 
                'more': 'suma',
                'most': 'suma',
                'min': 'm√≠nimo', 'minimum': 'm√≠nimo', 'lowest': 'm√≠nimo', 'less': 'm√≠nimo',
                'sum': 'suma', 'total': 'suma',
                'avg': 'promedio', 'average': 'promedio',
                'count': 'conteo'
            }
            
            mapped_value = english_operations.get(token.lower(), token.lower())
            
            return QueryComponent(
                text=token,
                type=ComponentType.OPERATION,
                confidence=0.95,
                value=mapped_value,
                linguistic_info={'source': 'english_operation_dictionary'}
            )
        elif component_type == ComponentType.METRIC:
            return QueryComponent(
                text=token,
                type=ComponentType.METRIC,
                confidence=0.95,
                linguistic_info={'source': 'metric_dictionary'}
            )
        elif component_type == ComponentType.CONNECTOR:
            return QueryComponent(
                text=token,
                type=ComponentType.CONNECTOR,
                confidence=0.8,
                linguistic_info={'source': 'english_connector_dictionary'}
            )
        
        # Default: unknown
        return QueryComponent(
            text=token,
            type=ComponentType.UNKNOWN,
            confidence=0.3,
            linguistic_info={'source': 'unknown_english'}
        )

    

    def _is_potential_value_english(self, token: str) -> bool:
        """üá∫üá∏ VERIFICADOR DE VALORES - REGLA ABSOLUTA PARA MAY√öSCULAS"""
        
        # üîß REGLA ABSOLUTA #1: Letras may√∫sculas individuales SIEMPRE son valores
        if len(token) == 1 and token.isupper() and token.isalpha():
            return True
        
        # üîß REGLA ABSOLUTA #2: C√≥digos en may√∫sculas SIEMPRE son valores
        if token.isupper() and 2 <= len(token) <= 6 and any(c.isalpha() for c in token):
            return True
        
        # RESTO DE VALIDACIONES...
        if token.isdigit():
            return True
        
        if token.endswith('%'):
            return True
        
        # Estados comunes en may√∫sculas
        common_states = {'ACTIVE', 'INACTIVE', 'PENDING', 'COMPLETE', 'CANCELLED', 'YES', 'NO', 'TRUE', 'FALSE'}
        if token.upper() in common_states:
            return True
        
        return False
        
        
    def generate_english_sql(self, structure: QueryStructure) -> str:
        """üá∫üá∏ GENERACI√ìN SQL COMPLETA PARA INGL√âS - VERSI√ìN FINAL"""
        
        select_parts = []
        from_clause = "FROM datos"
        where_conditions = []
        group_by_parts = []
        order_by_parts = []
        
        # Identificar columnas temporales para evitar duplicaci√≥n
        temporal_columns = set()
        
        for tf in structure.temporal_filters:
            if tf.unit == TemporalUnit.WEEKS:
                temporal_columns.update(['week', 'weeks'])
            elif tf.unit == TemporalUnit.MONTHS:
                temporal_columns.update(['month', 'months'])
            elif tf.unit == TemporalUnit.DAYS:
                temporal_columns.update(['day', 'days'])
            elif tf.unit == TemporalUnit.YEARS:
                temporal_columns.update(['year', 'years'])
        
        print(f"üóÑÔ∏è Generating COMPLETE English SQL:")
        print(f"   ‚è∞ Temporal columns detected: {temporal_columns}")
        print(f"   üéØ Query pattern: {structure.query_pattern.value}")
        print(f"   üîó Is compound: {structure.is_compound_query}")
        print(f"   üèÜ Is ranking: {structure.is_ranking_query}")
        print(f"   üîó Is multi-dimensional: {structure.is_multi_dimension_query}")
        
        # üîß NUEVA L√ìGICA: Manejar rankings multi-dimensionales
        if (structure.is_ranking_query and 
            structure.is_multi_dimension_query and 
            len(structure.main_dimensions) >= 2):
            print(f"üèÜüîó DETECTED: English multi-dimensional ranking ‚Üí using specialized generator")
            return self.generate_multi_dimension_english_sql(structure, temporal_columns)
        
        # L√ìGICA: Manejar consultas multi-dimensionales sin ranking
        if (structure.is_multi_dimension_query and 
            structure.query_pattern == QueryPattern.MULTI_DIMENSION):
            print(f"üîó DETECTED: English multi-dimensional without ranking ‚Üí using specialized generator")
            return self.generate_multi_dimension_english_sql(structure, temporal_columns)
        
        # L√ìGICA: Manejar rankings simples
        if (structure.is_ranking_query and 
            structure.ranking_criteria and 
            not structure.is_multi_dimension_query):
            print(f"üèÜ DETECTED: English simple ranking ‚Üí using ranking generator")
            return self.generate_ranking_sql_english(structure, temporal_columns)
        
        # Verificar si es agregaci√≥n global
        is_global_aggregation = not structure.main_dimension and structure.operations and structure.metrics
        
        if is_global_aggregation:
            print(f"üåê Generating English SQL for global aggregation")
            
            if structure.operations and structure.metrics:
                operation = structure.operations[0]
                metric = structure.metrics[0]

                if operation.value == 'm√°ximo':
                    agg_function = self._get_contextual_aggregation_english(structure, metric.text, operation.value)
                else:
                    sql_operations = {
                        'm√≠nimo': f'MIN({metric.text})',
                        'suma': f'SUM({metric.text})',
                        'promedio': f'AVG({metric.text})',
                        'conteo': f'COUNT({metric.text})'
                    }
                    agg_function = sql_operations.get(operation.value, f'SUM({metric.text})')
                
                if agg_function:
                    select_parts.append(agg_function)
                            
        else:
            # L√≥gica para consultas con dimensi√≥n principal
            if structure.main_dimension:
                dim_name = structure.main_dimension.text
                formatted_dim = self.format_temporal_dimension(dim_name)
                select_parts.append(formatted_dim)
                group_by_parts.append(dim_name)  
            
            # CONSULTAS COMPUESTAS
            if structure.is_compound_query and structure.compound_criteria:
                print(f"üîó Processing English compound query with {len(structure.compound_criteria)} criteria:")
                
                for i, criteria in enumerate(structure.compound_criteria):
                    operation_value = criteria.operation.value
                    metric_text = criteria.metric.text
                    
                    if operation_value == 'm√°ximo':
                        agg_function = self._get_contextual_aggregation_english(structure, metric_text, operation_value)
                    else:
                        sql_operations = {
                            'm√≠nimo': f'MIN({metric_text})',
                            'suma': f'SUM({metric_text})',
                            'promedio': f'AVG({metric_text})',
                            'conteo': f'COUNT({metric_text})'
                        }
                        agg_function = sql_operations.get(operation_value, f'SUM({metric_text})')
                    
                    if agg_function:
                        select_parts.append(agg_function)
                        
                        if operation_value in ['m√°ximo', 'mayor']:
                            order_direction = "DESC"
                        elif operation_value in ['m√≠nimo', 'menor']:
                            order_direction = "ASC"
                        else:
                            order_direction = "DESC"
                        
                        order_by_parts.append(f"{agg_function} {order_direction}")
                        
                        print(f"   üîó English Criteria {i+1}: {operation_value} {metric_text} ‚Üí {agg_function} {order_direction}")
                    else:
                        select_parts.append(metric_text)
                        order_by_parts.append(f"{metric_text} DESC")
                        print(f"   üîó English Criteria {i+1}: {metric_text} ‚Üí {metric_text} DESC")
                
                
                
            # L√ìGICA TRADICIONAL
            elif structure.operations and structure.metrics:
                operation = structure.operations[0]
                metric = structure.metrics[0]
                
                if operation.value == 'm√°ximo':
                    agg_function = self._get_contextual_aggregation_english(structure, metric.text, operation.value)
                else:
                    sql_operations = {
                        'm√≠nimo': f'MIN({metric.text})',
                        'suma': f'SUM({metric.text})',
                        'promedio': f'AVG({metric.text})',
                        'conteo': f'COUNT({metric.text})'
                    }
                    agg_function = sql_operations.get(operation.value, f'SUM({metric.text})')
                
                if agg_function:
                    select_parts.append(agg_function)
                    
                    if structure.query_pattern == QueryPattern.REFERENCED:
                        if operation.value in ['m√°ximo', 'mayor']:
                            order_by_parts.append(f"{agg_function} DESC")
                        elif operation.value in ['m√≠nimo', 'menor']:
                            order_by_parts.append(f"{agg_function} ASC")
                        else:
                            order_by_parts.append(f"{agg_function} DESC")
                    else:
                        order_by_parts.append(f"{agg_function} DESC")
                else:
                    select_parts.append(metric.text)
                    if structure.query_pattern == QueryPattern.REFERENCED:
                        order_by_parts.append(f"{metric.text} DESC")
        
        # WHERE para condiciones de columna (excluyendo temporales duplicadas)
        for condition in structure.column_conditions:
            if condition.column_name not in temporal_columns:
                where_conditions.append(f"{condition.column_name} = '{condition.value}'")
                print(f"   ‚úÖ English WHERE condition: {condition.column_name} = '{condition.value}'")
            else:
                print(f"   ‚è∞ English excluding duplicate temporal condition: {condition.column_name} = '{condition.value}'")
        
        # FILTROS DE EXCLUSI√ìN
        if hasattr(structure, 'exclusion_filters'):
            for exclusion in structure.exclusion_filters:
                if exclusion.exclusion_type == ExclusionType.NOT_EQUALS:
                    where_conditions.append(f"{exclusion.column_name} != '{exclusion.value}'")
                    print(f"   üö´ English exclusion condition: {exclusion.column_name} != '{exclusion.value}'")
        
        # FILTROS TEMPORALES
        advanced_conditions = self.get_advanced_temporal_sql_conditions_english(structure)
        if advanced_conditions:
            where_conditions.extend(advanced_conditions)
            print(f"   ‚úÖ English using temporal filters: {advanced_conditions}")
        
        # CONSTRUCCI√ìN DEL SQL FINAL
        sql_parts = []
        
        if select_parts:
            sql_parts.append(f"SELECT {', '.join(select_parts)}")
        else:
            sql_parts.append("SELECT *")
        
        sql_parts.append(from_clause)
        
        if where_conditions:
            sql_parts.append(f"WHERE {' AND '.join(where_conditions)}")
        
        if group_by_parts:
            sql_parts.append(f"GROUP BY {', '.join(group_by_parts)}")
        
        if order_by_parts:
            sql_parts.append(f"ORDER BY {', '.join(order_by_parts)}")
        
        # LIMITAR LA DATA SEG√öN EL PATR√ìN
        if structure.query_pattern == QueryPattern.REFERENCED:
            sql_parts.append("LIMIT 1")
            print(f"   üéØ English adding LIMIT 1 for REFERENCED pattern")
            
        elif structure.query_pattern == QueryPattern.TOP_N and structure.limit_value:
            sql_parts.append(f"LIMIT {structure.limit_value}")
            print(f"   üèÜ English adding LIMIT {structure.limit_value} for TOP_N pattern")
        
        elif structure.is_ranking_query and structure.ranking_criteria and structure.ranking_criteria.value:
            limit_value = int(structure.ranking_criteria.value)
            sql_parts.append(f"LIMIT {limit_value}")
            print(f"   üèÜ English FORCING LIMIT {limit_value} for ranking (pattern: {structure.query_pattern.value})")
        
        final_sql = " ".join(sql_parts) + ";"
        print(f"   üéØ Final COMPLETE English SQL: {final_sql}")
        
        return final_sql


    def generate_ranking_sql_english(self, structure: QueryStructure, temporal_columns: set) -> str:
        """üá∫üá∏ GENERADOR DE SQL PARA RANKINGS EN INGL√âS - VERSI√ìN CORREGIDA CON MULTI-M√âTRICAS"""
        print(f"üèÜ GENERATING ENGLISH SQL FOR RANKING:")
        
        ranking = structure.ranking_criteria
        if not ranking:
            print(f"‚ùå Error: No English ranking criteria")
            return "SELECT * FROM datos;"
        
        # CONSTRUIR SELECT
        select_parts = []
        if structure.main_dimension:
            dim_name = structure.main_dimension.text
            formatted_dim = self.format_temporal_dimension(dim_name)
            select_parts.append(formatted_dim)
        
        order_by_parts = []
        
        # üÜï RECOPILAR TODAS LAS M√âTRICAS (principal + adicionales)
        all_metrics = []
        all_operations = []
        
        # M√©trica principal del ranking
        if ranking.metric:
            all_metrics.append(ranking.metric)
            if ranking.operation:
                all_operations.append(ranking.operation)
        
        # üÜï IMPORTANTE: Agregar TODAS las m√©tricas de la estructura
        print(f"   üìä Metrics in structure: {[m.text for m in structure.metrics]}")
        for metric in structure.metrics:
            # Evitar duplicados
            if not any(m.text == metric.text for m in all_metrics):
                all_metrics.append(metric)
                print(f"   üìä Adding additional metric: {metric.text}")
        
        # Agregar operaciones correspondientes
        print(f"   ‚ö° Operations in structure: {[op.text for op in structure.operations]}")
        for op in structure.operations:
            if op.text.lower() not in ['top', 'bottom', 'best', 'worst']:  # Filtrar indicadores de ranking
                all_operations.append(op)
        
        # Asegurar que tenemos operaciones para todas las m√©tricas
        while len(all_operations) < len(all_metrics):
            # Usar 'suma' como operaci√≥n por defecto
            default_op = QueryComponent(
                text='total',
                type=ComponentType.OPERATION,
                value='suma',
                confidence=0.85
            )
            all_operations.append(default_op)
            print(f"   ‚ö° Added default operation for metric")
        
        print(f"   üìä TOTAL METRICS TO PROCESS: {len(all_metrics)}")
        print(f"   üìä Metrics: {[m.text for m in all_metrics]}")
        print(f"   ‚ö° Operations: {[op.value if hasattr(op, 'value') else op.text for op in all_operations]}")
        
        # GENERAR FUNCIONES SQL PARA CADA M√âTRICA
        for i, metric in enumerate(all_metrics):
            if i < len(all_operations):
                op = all_operations[i]
                operation_value = op.value if hasattr(op, 'value') else 'suma'
            else:
                operation_value = 'suma'
            
            # Para la m√©trica principal del ranking
            if i == 0:
                # Si es "more" o "most", interpretar como SUM
                if ranking.operation and ranking.operation.text.lower() in ['more', 'most', 'highest']:
                    agg_function = f'SUM({metric.text})'
                    print(f"   üèÜ Primary ranking metric: 'more/most' ‚Üí SUM")
                elif operation_value == 'm√°ximo':
                    agg_function = self._get_contextual_aggregation_english(structure, metric.text, operation_value)
                else:
                    sql_operations = {
                        'm√≠nimo': f'MIN({metric.text})',
                        'suma': f'SUM({metric.text})',
                        'promedio': f'AVG({metric.text})',
                        'conteo': f'COUNT({metric.text})'
                    }
                    agg_function = sql_operations.get(operation_value, f'SUM({metric.text})')
            else:
                # M√©tricas adicionales - usar la operaci√≥n correspondiente
                sql_operations = {
                    'm√≠nimo': f'MIN({metric.text})',
                    'suma': f'SUM({metric.text})',
                    'promedio': f'AVG({metric.text})',
                    'conteo': f'COUNT({metric.text})',
                    'm√°ximo': f'MAX({metric.text})'
                }
                agg_function = sql_operations.get(operation_value, f'SUM({metric.text})')
            
            select_parts.append(agg_function)
            
            # Solo la primera m√©trica define el ORDER BY
            if i == 0:
                if ranking.direction == RankingDirection.TOP:
                    order_direction = "DESC"
                else:
                    order_direction = "ASC"
                
                order_by_parts.append(f"{agg_function} {order_direction}")
                print(f"   ‚úÖ Primary metric: {metric.text} ‚Üí {agg_function} (ORDER BY {order_direction})")
            else:
                print(f"   ‚úÖ Additional metric {i}: {metric.text} ‚Üí {agg_function}")
        
        # CONSTRUIR WHERE
        where_conditions = []
        
        # Condiciones regulares
        for condition in structure.column_conditions:
            if condition.column_name not in temporal_columns:
                where_conditions.append(f"{condition.column_name} = '{condition.value}'")
                print(f"   ‚úÖ WHERE: {condition.column_name} = '{condition.value}'")
        
        # Exclusiones
        if hasattr(structure, 'exclusion_filters'):
            for exclusion in structure.exclusion_filters:
                if exclusion.exclusion_type == ExclusionType.NOT_EQUALS:
                    where_conditions.append(f"{exclusion.column_name} != '{exclusion.value}'")
                    print(f"   üö´ Exclusion: {exclusion.column_name} != '{exclusion.value}'")
        
        # Filtros temporales
        print(f"üîß DEBUG: Checking temporal filters...")
        print(f"üîß DEBUG: structure.temporal_filters = {len(structure.temporal_filters)}")
        
        advanced_conditions = self.get_advanced_temporal_sql_conditions_english(structure)
        print(f"üîß DEBUG: advanced_conditions = {advanced_conditions}")
        
        if advanced_conditions:
            where_conditions.extend(advanced_conditions)
            print(f"   üìÖ English temporal conditions added: {advanced_conditions}")
        else:
            print(f"   ‚è∞ No temporal conditions found")
        
        # CONSTRUIR SQL FINAL
        sql_parts = [
            f"SELECT {', '.join(select_parts)}",
            "FROM datos"
        ]
        
        if where_conditions:
            sql_parts.append(f"WHERE {' AND '.join(where_conditions)}")
        
        if structure.main_dimension:
            sql_parts.append(f"GROUP BY {structure.main_dimension.text}")
        
        if order_by_parts:
            sql_parts.append(f"ORDER BY {', '.join(order_by_parts)}")
        
        # AGREGAR LIMIT BASADO EN EL VALOR DEL RANKING
        if ranking and hasattr(ranking, 'value') and ranking.value:
            if ranking.unit == RankingUnit.COUNT:
                limit_value = int(ranking.value)
                sql_parts.append(f"LIMIT {limit_value}")
                print(f"   üèÜ Adding LIMIT {limit_value} for TOP {limit_value} ranking")
            elif ranking.unit == RankingUnit.PERCENTAGE:
                print(f"   üèÜ Percentage ranking detected: {ranking.value}% - using default LIMIT 100")
                sql_parts.append("LIMIT 100")
        else:
            print(f"   ‚ö†Ô∏è No ranking value found, using default LIMIT 10")
            sql_parts.append("LIMIT 10")
        
        final_sql = " ".join(sql_parts) + ";"
        
        print(f"   üéØ English ranking SQL: {final_sql}")
        
        return final_sql


    def generate_temporal_conditional_sql_english(self, pattern_data: Dict) -> str:
        """üïê GENERADOR SQL MEJORADO PARA PATRONES TEMPORALES CONDICIONALES"""
        print(f"üïê GENERATING TEMPORAL CONDITIONAL SQL:")
        
        temporal_dim = pattern_data['temporal_dimension']
        entity_col = pattern_data.get('entity_column')
        entity_val = pattern_data.get('entity_value')
        target_metric = pattern_data.get('target_metric', 'Sell_Out')  # Default a Sell_Out
        order_direction = pattern_data.get('order_direction', 'DESC')
        
        print(f"   ‚è∞ Temporal: {temporal_dim}")
        if entity_col and entity_val:
            print(f"   üéØ Filter: {entity_col} = '{entity_val}'")
        print(f"   üìä Metric: {target_metric}")
        print(f"   üîÑ Order: {order_direction}")
        
        # Mapear dimensi√≥n temporal a nombre de columna real
        temporal_column_map = {
            'week': 'Week',
            'month': 'Month',
            'day': 'Day',
            'year': 'Year',
            'quarter': 'Quarter'
        }
        
        temporal_column = temporal_column_map.get(temporal_dim, temporal_dim)
        
        # Mapear m√©tricas comunes a nombres reales de columnas
        metric_column_map = {
            'sales': 'Sell_Out',
            'revenue': 'Sell_Out',
            'inventory': 'Inventory',
            'profit': 'profit',
            'margin': 'margin',
            'sell_out': 'Sell_Out',
            'stock': 'Inventory'
        }
        
        metric_column = metric_column_map.get(target_metric.lower(), target_metric)
        
        # Construir SQL con formato temporal si es necesario
        formatted_dim = self.format_temporal_dimension(temporal_column)
        
        # Si no hay entity_value espec√≠fico, listar todas las semanas con el total
        if not entity_val:
            sql = f"""SELECT {formatted_dim}, SUM({metric_column}) as total_{target_metric}
    FROM datos
    GROUP BY {temporal_column}
    ORDER BY total_{target_metric} {order_direction}
    LIMIT 10;"""
        else:
            # Con filtro espec√≠fico
            sql = f"""SELECT {formatted_dim}, SUM({metric_column}) as total_{target_metric}
    FROM datos
    WHERE {entity_col} = '{entity_val}'
    GROUP BY {temporal_column}
    ORDER BY total_{target_metric} {order_direction}
    LIMIT 10;"""
        
        # Limpiar el SQL (quitar saltos de l√≠nea extras)
        sql = ' '.join(sql.split())
        
        print(f"   üéØ Generated SQL: {sql}")
        return sql


    def validate_english_structure(self, structure: QueryStructure) -> Dict:
        """üá∫üá∏ VALIDACI√ìN COMPLETA DE ESTRUCTURA INGL√âS - MEJORADA PARA LIST_ALL"""
        
        print(f"üîç VALIDATING ENGLISH STRUCTURE:")
        print(f"   üìã Has list_all_pattern: {hasattr(structure, 'list_all_pattern')}")
        
        errors = []
        suggestions = []
        
        # üÜï VALIDACI√ìN ESPECIAL PARA LIST_ALL
        if hasattr(structure, 'list_all_pattern') and structure.list_all_pattern:
            print(f"   üìã LIST_ALL pattern detected - using special validation")
            
            target_dimension = structure.list_all_pattern.get('target_dimension')
            has_aggregation = structure.list_all_pattern.get('has_aggregation', False)
            
            if not target_dimension:
                errors.append("LIST_ALL pattern missing target dimension")
                suggestions.append("Specify what to list (e.g., 'list all accounts')")
            else:
                # Si tiene agregaci√≥n, verificar que haya m√©tricas
                if has_aggregation and not structure.metrics:
                    print(f"   ‚ö†Ô∏è LIST_ALL has aggregation indicator but no metrics found")
                    suggestions.append("Metrics detected in query but not properly identified")
                else:
                    print(f"   ‚úÖ LIST_ALL validation passed - target: {target_dimension}")
                    return {
                        'valid': True,
                        'error': None,
                        'suggestions': []
                    }
                
#  VALIDACI√ìN ESPECIAL PARA SHOW_ROWS
        if hasattr(structure, 'show_rows_pattern') and structure.show_rows_pattern:
            print(f"   üìä SHOW_ROWS pattern detected - using special validation")
            
            # Para SHOW_ROWS solo verificamos que tenga row_count v√°lido
            row_count = structure.show_rows_pattern.get('row_count')
            
            if not row_count or row_count <= 0:
                errors.append("SHOW_ROWS pattern missing valid row count")
                suggestions.append("Specify number of rows (e.g., 'show first 100 rows')")
            elif row_count > 10000:  # L√≠mite de seguridad
                errors.append("SHOW_ROWS pattern: row count too large (max 10000)")
                suggestions.append("Use a smaller number of rows")
            else:
                print(f"   ‚úÖ SHOW_ROWS validation passed - count: {row_count}")
                return {
                    'valid': True,
                    'error': None,
                    'suggestions': []
                }                
                
#  VALIDACI√ìN ESPECIAL PARA TEMPORAL CONDITIONAL
        if hasattr(structure, 'temporal_conditional_pattern') and structure.temporal_conditional_pattern:
            print(f"   üïê TEMPORAL_CONDITIONAL pattern detected - using special validation")
            
            pattern = structure.temporal_conditional_pattern
            if (pattern.get('entity_column') and 
                pattern.get('entity_value') and 
                pattern.get('target_metric')):
                print(f"   ‚úÖ TEMPORAL_CONDITIONAL validation passed")
                return {
                    'valid': True,
                    'error': None,
                    'suggestions': []
                }
            else:
                errors.append("TEMPORAL_CONDITIONAL pattern incomplete")
                suggestions.append("Include entity and metric (e.g., 'week where store X had most sales')")
                
        # VALIDACI√ìN TRADICIONAL PARA OTROS PATRONES
        
        # NUEVA VALIDACI√ìN: Permitir agregaciones globales
        if not structure.main_dimension:
            # Verificar si es una agregaci√≥n global v√°lida
            has_operations_and_metrics = structure.operations and structure.metrics
            
            if has_operations_and_metrics:
                print(f"   ‚úÖ English global aggregation valid - no main dimension required")
            else:
                # Solo es error si NO es agregaci√≥n global
                if structure.column_conditions:
                    available_columns = [cvp.column_name for cvp in structure.column_conditions]
                    suggestions.append(f"English columns detected: {', '.join(available_columns)}")
                errors.append("Missing main dimension")
                suggestions.append("Add an entity like: store, account, product, customer")
        
        # Validaci√≥n para contenido significativo
        has_meaningful_content = (
            structure.metrics or 
            structure.operations or 
            structure.column_conditions or
            structure.temporal_filters
        )
        
        if not has_meaningful_content:
            errors.append("Missing metric, operation or condition")
            suggestions.append("Add a metric like: sales, revenue, inventory")
        
        # Advertencias para tokens desconocidos (pero NO como errores para LIST_ALL)
        if structure.unknown_tokens:
            unknown_words = [token.text for token in structure.unknown_tokens]
            # Para LIST_ALL, solo es sugerencia, no error
            if hasattr(structure, 'list_all_pattern') and structure.list_all_pattern:
                suggestions.append(f"English unrecognized words (non-critical): {', '.join(unknown_words)}")
            else:
                suggestions.append(f"English unrecognized words: {', '.join(unknown_words)}")
        
        final_result = {
            'valid': len(errors) == 0,
            'error': '; '.join(errors) if errors else None,
            'suggestions': suggestions
        }
        
        print(f"   üéØ Validation result: {final_result}")
        return final_result


    # ========================================================================
    # M√âTODOS DE SOPORTE ADICIONALES
    # ========================================================================

    def generate_natural_interpretation_english(self, structure: QueryStructure) -> str:
        """üá∫üá∏ GENERADOR DE INTERPRETACI√ìN NATURAL EN INGL√âS - CON SOPORTE SHOW_ROWS"""
        
        # üÜï CASO ESPECIAL: SHOW_ROWS
        if hasattr(structure, 'show_rows_pattern') and structure.show_rows_pattern:
            pattern = structure.show_rows_pattern
            position = pattern.get('position_type', '')
            count = pattern.get('row_count', 0)
            object_type = pattern.get('object_type', 'rows')
            
            if position:
                return f"Show the {position} {count} {object_type} from the table"
            else:
                return f"Show {count} {object_type} from the table"
        
        # CASO ESPECIAL: Rankings
        if structure.is_ranking_query and structure.ranking_criteria:
            ranking = structure.ranking_criteria
            parts = []
            
            direction_text = "the best" if ranking.direction == RankingDirection.TOP else "the worst"
            
            # Verificar si hay dimensi√≥n principal
            dimension_text = structure.main_dimension.text if structure.main_dimension else "records"
            
            if ranking.unit == RankingUnit.COUNT:
                parts.append(f"Find {direction_text} {int(ranking.value)} {dimension_text}")
            else:  # PERCENTAGE
                parts.append(f"Find {direction_text} {ranking.value}% of {dimension_text}")
            
            if ranking.metric:
                if ranking.operation and ranking.operation.text.lower() in ['more', 'most', 'highest']:
                    parts.append(f"with highest total {ranking.metric.text}")
                elif ranking.operation and ranking.operation.text.lower() in ['less', 'least', 'lowest']:
                    parts.append(f"with lowest total {ranking.metric.text}")
                else:
                    parts.append(f"based on {ranking.metric.text}")
            
            # üîß FIX: Agregar filtros temporales con validaci√≥n
            if structure.temporal_filters:
                for tf in structure.temporal_filters:
                    if tf.filter_type == "range_between":
                        # Usar start_value y end_value en lugar de quantity
                        start_val = getattr(tf, 'start_value', None)
                        end_val = getattr(tf, 'end_value', None)
                        
                        if start_val is not None and end_val is not None:
                            if tf.unit == TemporalUnit.WEEKS:
                                # Extraer solo el n√∫mero de semana si es formato YYYYWW
                                if start_val > 1000:  # Es formato YYYYWW
                                    start_week = start_val % 100
                                    end_week = end_val % 100
                                else:
                                    start_week = start_val
                                    end_week = end_val
                                parts.append(f"between weeks {start_week} and {end_week}")
                            elif tf.unit == TemporalUnit.MONTHS:
                                parts.append(f"between months {start_val} and {end_val}")
                            elif tf.unit == TemporalUnit.DAYS:
                                parts.append(f"between days {start_val} and {end_val}")
                    elif tf.filter_type == "specific":
                        if tf.quantity is not None:  # Validar que quantity no sea None
                            if tf.unit == TemporalUnit.WEEKS:
                                # Extraer solo el n√∫mero de semana si es formato YYYYWW
                                if tf.quantity > 1000:  # Es formato YYYYWW
                                    week_num = tf.quantity % 100
                                else:
                                    week_num = tf.quantity
                                parts.append(f"in week number {week_num}")
                            elif tf.unit == TemporalUnit.MONTHS:
                                parts.append(f"in month number {tf.quantity}")
                            elif tf.unit == TemporalUnit.DAYS:
                                parts.append(f"in day number {tf.quantity}")
                    elif tf.filter_type == "range":
                        if tf.quantity is not None:  # Validar que quantity no sea None
                            parts.append(f"in the {tf.indicator} {tf.quantity} {tf.unit.value}")
                    elif tf.filter_type == "current_week":
                        parts.append("in this week")
            
            # Agregar otros filtros
            if structure.column_conditions:
                conditions = []
                for condition in structure.column_conditions:
                    conditions.append(f"where {condition.column_name} = '{condition.value}'")
                parts.extend(conditions)
            
            interpretation = ", ".join(parts)
            return interpretation.capitalize() if interpretation else "English ranking query without clear interpretation"
        
        # L√ìGICA PARA CONSULTAS NO-RANKING
        parts = []
        
        # Parte principal
        if structure.main_dimension:
            parts.append(f"Find {structure.main_dimension.text}")
        
        # Condiciones de columna
        if structure.column_conditions:
            conditions = []
            for condition in structure.column_conditions:
                conditions.append(f"where {condition.column_name} = '{condition.value}'")
            parts.append(", ".join(conditions))
        
        # Operaci√≥n y m√©trica
        if structure.operations and structure.metrics:
            operation = structure.operations[0]
            metric = structure.metrics[0]
            
            if operation.value == 'm√°ximo':
                parts.append(f"with the highest value in {metric.text}")
            elif operation.value == 'm√≠nimo':
                parts.append(f"with the lowest value in {metric.text}")
            else:
                parts.append(f"calculating {operation.value} of {metric.text}")
        elif structure.operations:
            operation = structure.operations[0]
            parts.append(f"with {operation.value}")
        elif structure.metrics:
            metric = structure.metrics[0]
            parts.append(f"related to {metric.text}")
        
        # üîß FIX: Filtros temporales con validaci√≥n
        if structure.temporal_filters:
            for tf in structure.temporal_filters:
                if tf.filter_type == "specific":
                    if tf.quantity is not None:  # Validar quantity
                        if tf.unit == TemporalUnit.WEEKS:
                            week_num = tf.quantity % 100 if tf.quantity > 1000 else tf.quantity
                            parts.append(f"in week number {week_num}")
                        elif tf.unit == TemporalUnit.MONTHS:
                            parts.append(f"in month number {tf.quantity}")
                elif tf.filter_type == "range":
                    if tf.quantity is not None:  # Validar quantity
                        parts.append(f"in the {tf.indicator} {tf.quantity} {tf.unit.value}")
                elif tf.filter_type == "range_between":
                    # Usar start_value y end_value
                    start_val = getattr(tf, 'start_value', None)
                    end_val = getattr(tf, 'end_value', None)
                    if start_val is not None and end_val is not None:
                        if tf.unit == TemporalUnit.WEEKS:
                            start_week = start_val % 100 if start_val > 1000 else start_val
                            end_week = end_val % 100 if end_val > 1000 else end_val
                            parts.append(f"between weeks {start_week} and {end_week}")
        
        interpretation = ", ".join(parts)
        return interpretation.capitalize() if interpretation else "English query without clear interpretation"

    # ========================================
    # M√âTODOS DE SOPORTE - VERSIONES LIMPIAS
    # ========================================

    def get_advanced_temporal_sql_conditions_english(self, structure: QueryStructure) -> List[str]:
        """üîß VERSI√ìN COMPLETA CON TODOS LOS CASOS TEMPORALES"""
        
        print(f"üîß DEBUG EXTREMO: M√©todo get_advanced_temporal_sql_conditions_english INICIADO")
        
        try:
            sql_conditions = []
            
            print(f"üîß DEBUG EXTREMO: Inicializando sql_conditions = {sql_conditions}")
            
            print(f"‚è∞ GENERATING ADVANCED TEMPORAL CONDITIONS (With Special Patterns):")
            
            print(f"üîß DEBUG EXTREMO: Verificando structure.temporal_filters...")
            print(f"üîß DEBUG EXTREMO: hasattr(structure, 'temporal_filters') = {hasattr(structure, 'temporal_filters')}")
            
            if not hasattr(structure, 'temporal_filters'):
                print(f"‚ùå ERROR: structure no tiene temporal_filters")
                return []
            
            print(f"üîß DEBUG EXTREMO: structure.temporal_filters = {structure.temporal_filters}")
            print(f"üîß DEBUG EXTREMO: len(structure.temporal_filters) = {len(structure.temporal_filters)}")
            
            # üîß DEBUG: Mostrar todos los filtros temporales
            print(f"   üìã Total temporal filters in structure: {len(structure.temporal_filters)}")
            
            for i, tf in enumerate(structure.temporal_filters):
                print(f"üîß DEBUG EXTREMO: Procesando filtro {i}: {tf}")
                print(f"üîß DEBUG EXTREMO: tf.indicator = {tf.indicator}")
                print(f"üîß DEBUG EXTREMO: tf.filter_type = {tf.filter_type}")
                print(f"üîß DEBUG EXTREMO: tf.confidence = {tf.confidence}")
                print(f"   {i+1}. {tf.indicator} | {tf.filter_type} | {tf.confidence}")
            
    # STEP 1: PROCESAR FILTROS ESPECIALES EN structure.temporal_filters
            for i, tf in enumerate(structure.temporal_filters):
                print(f"üîß DEBUG EXTREMO: Entrando al loop para filtro {i}")
                print(f"   üìÖ Procesando filtro: {tf.indicator} {tf.quantity} {tf.unit.value} (type: {tf.filter_type})")
                
                
        # CASO 1: THIS WEEK
                if tf.filter_type == "current_week":
                    print(f"üîß DEBUG EXTREMO: Detectado current_week")
                    
                    if hasattr(self, 'generate_this_week_sql_condition'):
                        condition = self.generate_this_week_sql_condition()
                        sql_conditions.append(condition)
                        print(f"   üìÖ THIS WEEK condition: {condition}")
                    else:
                        # Crear condici√≥n manualmente
                        condition = "Week = (SELECT MAX(Week) FROM datos)"
                        sql_conditions.append(condition)
                        print(f"   üìÖ THIS WEEK condition (manual): {condition}")
               
                
        # CASO 2: SPECIFIC (week 5, month 3, etc.)
                elif tf.filter_type == "specific":
                    print(f"üîß DEBUG EXTREMO: Procesando filtro specific")
                    if tf.unit == TemporalUnit.WEEKS:
                        condition = f"Week = {tf.quantity}"
                        sql_conditions.append(condition)
                        print(f"      ‚úÖ SQL: {condition}")
                    elif tf.unit == TemporalUnit.MONTHS:
                        condition = f"Month = {tf.quantity}"
                        sql_conditions.append(condition)
                        print(f"      ‚úÖ SQL: {condition}")
                    elif tf.unit == TemporalUnit.DAYS:
                        condition = f"Day = {tf.quantity}"
                        sql_conditions.append(condition)
                        print(f"      ‚úÖ SQL: {condition}")
                
                
        # CASO 3: RANGE (last X weeks/months/days)
                elif tf.filter_type == "range":
                    print(f"üîß DEBUG EXTREMO: Procesando filtro range")
                    if tf.unit == TemporalUnit.WEEKS and tf.quantity:
                        # Calcular semana actual y restar
                        current_week = 202531  # O usar self.get_current_week()
                        start_week = current_week - tf.quantity + 1
                        condition = f"Week >= {start_week} AND Week <= {current_week}"
                        sql_conditions.append(condition)
                        print(f"      ‚úÖ SQL: {condition} (last {tf.quantity} weeks)")
                    elif tf.unit == TemporalUnit.MONTHS and tf.quantity:
                        condition = f"fecha >= DATE('now', '-{tf.quantity} months')"
                        sql_conditions.append(condition)
                        print(f"      ‚úÖ SQL: {condition}")
                    elif tf.unit == TemporalUnit.DAYS and tf.quantity:
                        condition = f"fecha >= DATE('now', '-{tf.quantity} days')"
                        sql_conditions.append(condition)
                        print(f"      ‚úÖ SQL: {condition}")
                
                
        # CASO 4: RANGE BETWEEN (between weeks X and Y)
                elif tf.filter_type == "range_between":
                    print(f"üîß DEBUG: Procesando filtro range_between")
                    print(f"üîß DEBUG: Todos los atributos de tf: {vars(tf)}")
                    
                    # Verificar que los valores existen Y no son None
                    start_val = getattr(tf, 'start_value', None)
                    end_val = getattr(tf, 'end_value', None)
                    
                    print(f"üîß DEBUG: start_value = {start_val}, end_value = {end_val}")
                    
                    if start_val is not None and end_val is not None:
                        if tf.unit == TemporalUnit.WEEKS:
                            # üîß CAMBIO: Usar >= y <= en lugar de BETWEEN
                            condition = f"Week >= {start_val} AND Week <= {end_val}"
                            sql_conditions.append(condition)
                            print(f"      ‚úÖ SQL: {condition}")
                        elif tf.unit == TemporalUnit.MONTHS:
                            # üîß CAMBIO: Usar >= y <= en lugar de BETWEEN
                            condition = f"Month >= {start_val} AND Month <= {end_val}"
                            sql_conditions.append(condition)
                            print(f"      ‚úÖ SQL: {condition}")
                        elif tf.unit == TemporalUnit.DAYS:
                            # üîß CAMBIO: Usar >= y <= en lugar de BETWEEN
                            condition = f"Day >= {start_val} AND Day <= {end_val}"
                            sql_conditions.append(condition)
                            print(f"      ‚úÖ SQL: {condition}")
                    else:
                        print(f"      ‚ùå ERROR: start_value o end_value son None")
                                            
        # üîß CASO 5: SINCE (since week X)
                elif tf.filter_type == "since":
                    print(f"üîß DEBUG EXTREMO: Procesando filtro since")
                    
                    if hasattr(tf, 'start_value') and tf.unit == TemporalUnit.WEEKS:
                        condition = f"Week >= {tf.start_value}"
                        sql_conditions.append(condition)
                        print(f"      ‚úÖ SQL: {condition}")
                    elif hasattr(tf, 'start_value') and tf.unit == TemporalUnit.MONTHS:
                        condition = f"Month >= {tf.start_value}"
                        sql_conditions.append(condition)
                        print(f"      ‚úÖ SQL: {condition}")
                    else:
                        print(f"      ‚ùå ERROR: since filter missing start_value")
                
        # üîß CASO 6: SINCE AGO (since X weeks ago)
                elif tf.filter_type == "since_ago":
                    print(f"üîß DEBUG EXTREMO: Procesando filtro since_ago")
                    
                    if tf.unit == TemporalUnit.WEEKS and tf.quantity:
                        current_week = 202510  # usar self.get_current_week()
                        since_week = current_week - tf.quantity
                        condition = f"Week >= {since_week}"
                        sql_conditions.append(condition)
                        print(f"      ‚úÖ SQL: {condition} (since {tf.quantity} weeks ago)")
                    elif tf.unit == TemporalUnit.MONTHS and tf.quantity:
                        condition = f"fecha >= DATE('now', '-{tf.quantity} months')"
                        sql_conditions.append(condition)
                        print(f"      ‚úÖ SQL: {condition}")
                    else:
                        print(f"      ‚ùå ERROR: since_ago filter missing quantity")
                
        # üîß CASO 7: WEEK REFERENCE (para "week 5" = 202505)
                elif tf.filter_type == "week_reference":
                    print(f"üîß DEBUG EXTREMO: Procesando filtro week_reference")
                    
                    if tf.unit == TemporalUnit.WEEKS and hasattr(tf, 'week_number'):
                        current_year = 2025
                        week_value = int(f"{current_year}{str(tf.week_number).zfill(2)}")
                        condition = f"Week = {week_value}"
                        sql_conditions.append(condition)
                        print(f"      ‚úÖ SQL: Week = {week_value} (from 'week {tf.week_number}')")
                
        # üîß CASO 8: FROM TO (from week X to Y)
                elif tf.filter_type == "from_to":
                    print(f"üîß DEBUG EXTREMO: Procesando filtro from_to")
                    
                    # Similar a range_between
                    if hasattr(tf, 'start_value') and hasattr(tf, 'end_value'):
                        if tf.unit == TemporalUnit.WEEKS:
                            # üîß CAMBIO: Usar >= y <= en lugar de BETWEEN
                            condition = f"Week >= {tf.start_value} AND Week <= {tf.end_value}"
                            sql_conditions.append(condition)
                            print(f"      ‚úÖ SQL: {condition}")

        # STEP 2: PROCESAR advanced_temporal_info (RESPALDO)
        #     if hasattr(self, 'advanced_temporal_info') and self.advanced_temporal_info:
        #         print(f"   üîç Processing advanced_temporal_info: {len(self.advanced_temporal_info)} items")
                
        #         for ati in self.advanced_temporal_info:
        #             if ati.is_range_between and hasattr(ati, 'start_value') and hasattr(ati, 'end_value'):
        #                 if ati.original_filter.unit == TemporalUnit.WEEKS:
        #                     condition = f"Week BETWEEN {ati.start_value} AND {ati.end_value}"
        #                     if condition not in sql_conditions:
        #                         sql_conditions.append(condition)
        #                         print(f"      ‚úÖ SQL from advanced_temporal_info: {condition}")
            else:
                print(f"   ‚ö†Ô∏è  No advanced_temporal_info available")
            
            print(f"üîß DEBUG EXTREMO: sql_conditions final = {sql_conditions}")
            print(f"‚è∞ TOTAL TEMPORAL CONDITIONS: {len(sql_conditions)}")
            
            if sql_conditions:
                print(f"   üìù Conditions generated: {sql_conditions}")
            else:
                print(f"   ‚ùå NO CONDITIONS GENERATED!")
            
            print(f"üîß DEBUG EXTREMO: Retornando sql_conditions = {sql_conditions}")
            return sql_conditions
            
        except Exception as e:
            print(f"‚ùå ERROR EXTREMO en get_advanced_temporal_sql_conditions_english: {e}")
            import traceback
            print(f"‚ùå TRACEBACK: {traceback.format_exc()}")
            return []

    def _get_contextual_aggregation_english(self, structure: QueryStructure, metric_text: str, operation: str) -> str:
        """üá∫üá∏ Determina agregaci√≥n contextual usando intent sem√°ntico para ingl√©s"""
        
        if operation == 'm√°ximo':
            original_intent = getattr(structure, 'original_semantic_intent', 'DEFAULT')
            
            print(f"   üéØ ENGLISH CONTEXTUAL AGGREGATION:")
            print(f"      üìä Metric: {metric_text}")
            print(f"      ‚ö° Operation: {operation}")
            print(f"      üß† Original Intent: {original_intent}")
            
            if original_intent == 'MAX':
                print(f"      ‚úÖ INTENT ‚Üí MAX({metric_text}) [singular context]")
                return f'MAX({metric_text})'
            elif original_intent == 'SUM':
                print(f"      ‚úÖ INTENT ‚Üí SUM({metric_text}) [plural context]")
                return f'SUM({metric_text})'
            else:
                print(f"      ‚úÖ INTENT ‚Üí SUM({metric_text}) [default for English]")
                return f'SUM({metric_text})'
        
        # Para otras operaciones
        sql_operations = {
            'm√≠nimo': f'MIN({metric_text})',
            'suma': f'SUM({metric_text})',
            'promedio': f'AVG({metric_text})',
            'conteo': f'COUNT({metric_text})'
        }
        
        result = sql_operations.get(operation, f'SUM({metric_text})')
        print(f"   üéØ ENGLISH DIRECT MAPPING: {operation} ‚Üí {result}")
        return result


    def build_english_structure(self, classified_components: Dict, column_value_pairs: List[ColumnValuePair], 
                                temporal_filters: List[TemporalFilter], tokens: List[str], original_intent: str) -> QueryStructure:
        """üá∫üá∏ CONSTRUCCI√ìN DE ESTRUCTURA COMPLETA PARA INGL√âS - VERSI√ìN CORREGIDA PARA PATRONES ESPECIALES"""
        
        print(f"üèóÔ∏è BUILDING COMPLETE ENGLISH QUERY STRUCTURE")
                
        # üÜï PASO 0: DETECTAR PATR√ìN GROUP BY PRIMERO
        groupby_dimension = self.detect_groupby_pattern_english(tokens)

        if groupby_dimension:
            print(f"   üìç GROUP BY dimension detected: {groupby_dimension.text}")
            # Remover filtros que coincidan con la dimensi√≥n de agrupaci√≥n
            filtered_column_value_pairs = []
            for cvp in column_value_pairs:
                # Si el filtro es sobre la misma dimensi√≥n que el GROUP BY, no agregarlo
                if cvp.column_name.lower() != groupby_dimension.text.lower():
                    filtered_column_value_pairs.append(cvp)
                else:
                    print(f"   üîÑ Removing conflicting filter: {cvp.column_name} = {cvp.value} (conflicts with GROUP BY {groupby_dimension.text})")
            
            column_value_pairs = filtered_column_value_pairs
            
        # 0.1: Detectar SHOW ROWS pattern
        show_rows_pattern = self.detect_show_rows_pattern_english(tokens)
        has_show_rows = show_rows_pattern is not None
        print(f"   üìä Show rows pattern detected: {has_show_rows}")
        
        # 0.2: Detectar LIST ALL pattern  
        list_all_pattern = self.detect_list_all_pattern_english(tokens)
        has_list_all = list_all_pattern is not None
        print(f"   üìã List all pattern detected: {has_list_all}")
        
        # 0.3: Detectar TEMPORAL CONDITIONAL pattern
        temporal_conditional_pattern = self.detect_temporal_conditional_pattern_english(tokens)
        has_temporal_conditional = temporal_conditional_pattern is not None
        print(f"   üïê Temporal conditional pattern detected: {has_temporal_conditional}")
        
        # üÜï PASO 1: Solo detectar ranking si NO hay patrones especiales prioritarios
        ranking_criteria = None
        exclusion_filters = []
        is_ranking = False
        
        if not (has_show_rows or has_list_all or has_temporal_conditional):
            # Solo procesar ranking si no hay patrones especiales
            ranking_criteria = self.detect_ranking_criteria_english(tokens, classified_components)
            exclusion_filters = self.detect_exclusion_filters_english(tokens, classified_components)
            is_ranking = self.is_ranking_query_english(ranking_criteria, exclusion_filters)
            print(f"   üèÜ Ranking detected (no special patterns): {is_ranking}")
        else:
            print(f"   üèÜ Skipping ranking detection due to special patterns")
            exclusion_filters = self.detect_exclusion_filters_english(tokens, classified_components)
        
        # PASO 1.5: Si hay ranking y no hay m√©trica, buscar o asignar m√©trica impl√≠cita
        if is_ranking and ranking_criteria:
            if not ranking_criteria.metric:
                # Buscar m√©tricas en los componentes clasificados
                metric_found = False
                for token, component in classified_components.items():
                    if component.type == ComponentType.METRIC:
                        ranking_criteria.metric = component
                        print(f"   üìä M√©trica encontrada para ranking: {component.text}")
                        metric_found = True
                        break
                
                # Si no hay m√©trica, usar default basado en contexto
                if not metric_found:
                    # Determinar m√©trica default basada en palabras clave
                    default_metric_name = 'Sell_Out'  # Default m√°s com√∫n
                    
                    # Buscar pistas en los tokens
                    tokens_lower = [t.lower() for t in tokens]
                    if 'inventory' in tokens_lower:
                        default_metric_name = 'Inventory'
                    elif 'profit' in tokens_lower:
                        default_metric_name = 'profit'
                    elif 'margin' in tokens_lower:
                        default_metric_name = 'margin'
                    elif 'sales' in tokens_lower or 'sell' in tokens_lower:
                        default_metric_name = 'Sell_Out'
                    
                    default_metric = QueryComponent(
                        text=default_metric_name,
                        type=ComponentType.METRIC,
                        confidence=0.7,
                        subtype='default_ranking_metric',
                        linguistic_info={'source': 'default_for_ranking', 'reason': 'no_metric_specified'}
                    )
                    ranking_criteria.metric = default_metric
                    print(f"   üìä Usando m√©trica default para ranking: {default_metric_name}")
        
        # PASO 2: Detectar m√∫ltiples dimensiones
        multi_dimensions = self.detect_multi_dimensions_english(tokens, classified_components)
        is_multi_dimension = len(multi_dimensions) >= 2
        
        # PASO 3: Solo SI NO es ranking, procesar otros patrones
        if not is_ranking:
            compound_criteria = self.detect_compound_criteria_english(tokens, classified_components)
            is_compound = self.is_compound_query_english(compound_criteria)
        else:
            compound_criteria = []
            is_compound = False
        
        # PASO 4: Construir componentes b√°sicos
        main_dimension = None
        operations = []
        metrics = []
        values = []
        connectors = []
        unknown_tokens = []
        
        # PASO 4.1: Si es ranking, buscar dimensi√≥n objetivo primero
        if is_ranking and ranking_criteria:
            # Buscar dimensiones t√≠picas de ranking
            ranking_dimensions = ['account', 'accounts', 'store', 'stores', 'item', 'items', 
                                'product', 'products', 'customer', 'customers', 'brand', 'brands']
            
            for token, component in classified_components.items():
                token_lower = token.lower()
                if token_lower in ranking_dimensions or component.type == ComponentType.DIMENSION:
                    # Normalizar plural a singular
                    dimension_map = {
                        'accounts': 'account', 'stores': 'store', 'items': 'item',
                        'products': 'product', 'customers': 'customer', 'brands': 'brand'
                    }
                    
                    normalized_dim = dimension_map.get(token_lower, token_lower)
                    
                    main_dimension = QueryComponent(
                        text=normalized_dim,
                        type=ComponentType.DIMENSION,
                        confidence=0.95,
                        subtype='ranking_target',
                        linguistic_info={'source': 'ranking_dimension', 'original': token}
                    )
                    print(f"   üìç English ranking dimension: {normalized_dim} (from {token})")
                    break
        
        # PASO 4.1.5: Si es ranking, buscar m√©tricas adicionales despu√©s de "and"
        if is_ranking and ranking_criteria:
            # Buscar si hay "and" en los tokens
            and_positions = [i for i, t in enumerate(tokens) if t.lower() == 'and']
            
            for and_pos in and_positions:
                # Buscar m√©tricas despu√©s del "and"
                for i in range(and_pos + 1, len(tokens)):
                    token = tokens[i]
                    if token.lower() in ['inventory', 'profit', 'cost', 'margin', 'stock']:
                        # Crear componente m√©trica adicional
                        additional_metric = QueryComponent(
                            text=token.lower(),
                            type=ComponentType.METRIC,
                            confidence=0.9,
                            subtype='additional_ranking_metric',
                            linguistic_info={'source': 'ranking_additional_metric'}
                        )
                        metrics.append(additional_metric)
                        print(f"   üìä Additional ranking metric found: {token}")
                        
                        # Si hay "total" antes, agregar operaci√≥n
                        if i > 0 and tokens[i-1].lower() == 'total':
                            total_op = QueryComponent(
                                text='total',
                                type=ComponentType.OPERATION,
                                confidence=0.9,
                                value='suma',
                                subtype='additional_operation',
                                linguistic_info={'source': 'ranking_additional_operation'}
                            )
                            operations.append(total_op)
                            print(f"   ‚ö° Additional operation found: total")
        
        # PASO 4.2: Procesar componentes normalmente
        for token, component in classified_components.items():
            # Si ya tenemos main_dimension del ranking, skip dimensiones
            if is_ranking and main_dimension and component.type == ComponentType.DIMENSION:
                continue
                
            if component.type == ComponentType.DIMENSION and not main_dimension:
                main_dimension = component
                print(f"   üìç English main dimension: {component.text}")

            # üÜï Si tenemos GROUP BY dimension, usarla como main_dimension
            if groupby_dimension and not main_dimension:
                main_dimension = groupby_dimension
                print(f"   üìç Using GROUP BY as main dimension: {main_dimension.text}")
            elif component.type == ComponentType.OPERATION:
                # No agregar "top" como operaci√≥n si es ranking
                if not (is_ranking and component.text.lower() in ['top', 'bottom', 'best', 'worst', 'first', 'last']):
                    operations.append(component)
                    print(f"   ‚ö° English operation: {component.text}")
            elif component.type == ComponentType.METRIC:
                metrics.append(component)
                print(f"   üìä English metric: {component.text}")
            elif component.type == ComponentType.VALUE:
                values.append(component)
            elif component.type == ComponentType.CONNECTOR:
                connectors.append(component)
            elif component.type == ComponentType.UNKNOWN:
                # No marcar n√∫meros como unknown si son parte del ranking o show_rows
                if not ((is_ranking or has_show_rows) and token.isdigit()):
                    unknown_tokens.append(component)
        
        # PASO 5: Construir estructura completa
        # Preparar m√©tricas finales
        final_metrics = []
        if is_ranking:
            # Si es ranking, incluir la m√©trica principal del ranking
            if ranking_criteria and ranking_criteria.metric:
                final_metrics.append(ranking_criteria.metric)
            # Y tambi√©n incluir TODAS las m√©tricas adicionales detectadas
            final_metrics.extend(metrics)
        else:
            # Si no es ranking, usar las m√©tricas normales
            final_metrics = metrics

        structure = QueryStructure(
            main_dimension=main_dimension,
            main_dimensions=multi_dimensions if is_multi_dimension else ([main_dimension] if main_dimension else []),
            is_multi_dimension_query=is_multi_dimension,
            operations=operations,
            metrics=final_metrics,
            column_conditions=column_value_pairs,
            temporal_filters=temporal_filters,
            values=values,
            connectors=connectors,
            unknown_tokens=unknown_tokens,
            compound_criteria=compound_criteria,
            is_compound_query=is_compound,
            ranking_criteria=ranking_criteria,
            exclusion_filters=exclusion_filters,
            is_ranking_query=is_ranking,
            original_semantic_intent=original_intent
        )
        
        # üÜï PASO 5.5: AGREGAR PATRONES ESPECIALES SI EXISTEN
        if show_rows_pattern:
            structure.show_rows_pattern = show_rows_pattern
            structure.is_ranking_query = False  # Forzar que NO sea ranking
            print(f"   üìä Structure marked as SHOW_ROWS (overriding ranking)")
        
        if list_all_pattern:
            structure.list_all_pattern = list_all_pattern
            if not has_show_rows:  # Solo si no hay show_rows
                structure.is_ranking_query = False
            print(f"   üìã Structure marked as LIST_ALL")
        
        if temporal_conditional_pattern:
            structure.temporal_conditional_pattern = temporal_conditional_pattern
            if not (has_show_rows or has_list_all):  # Solo si no hay otros
                structure.is_ranking_query = False
            print(f"   üïê Structure marked as TEMPORAL_CONDITIONAL")
        
        # PASO 6: Detectar patr√≥n de consulta
        query_pattern = self.detect_query_pattern_english(structure)
        structure.query_pattern = query_pattern
        
        # PASO 7: Configurar l√≠mites seg√∫n el tipo de consulta
        if query_pattern == QueryPattern.TOP_N and structure.ranking_criteria:
            if structure.ranking_criteria.unit == RankingUnit.COUNT:
                structure.limit_value = int(structure.ranking_criteria.value)
            elif structure.ranking_criteria.unit == RankingUnit.PERCENTAGE:
                # Para porcentajes, necesitaremos calcularlo despu√©s
                structure.limit_value = None
            structure.is_single_result = False
            
            print(f"üèÜ ENGLISH RANKING CONFIGURATION:")
            print(f"   üìç Target dimension: {structure.main_dimension.text if structure.main_dimension else 'N/A'}")
            print(f"   üìä Ranking metric: {structure.ranking_criteria.metric.text if structure.ranking_criteria.metric else 'N/A'}")
            print(f"   üéØ Direction: {structure.ranking_criteria.direction.value}")
            print(f"   üìà Unit: {structure.ranking_criteria.unit.value}")
            print(f"   üî¢ Value: {structure.ranking_criteria.value}")
            
        elif query_pattern == QueryPattern.REFERENCED:
            structure.reference_metric = metrics[0] if metrics else None
            structure.is_single_result = True
            structure.limit_value = 1
            
            print(f"üéØ ENGLISH REFERENCED CONFIGURATION:")
            print(f"   üìç Target dimension: {structure.main_dimension.text if structure.main_dimension else 'N/A'}")
            print(f"   üìä Reference metric: {structure.reference_metric.text if structure.reference_metric else 'N/A'}")
        
        elif query_pattern == QueryPattern.TEMPORAL_CONDITIONAL:
            # Para temporal conditional, no necesitamos configuraci√≥n especial aqu√≠
            print(f"üïê ENGLISH TEMPORAL CONDITIONAL CONFIGURATION:")
            print(f"   ‚è∞ Pattern will be handled by specialized generator")
        
        # PASO 8: Detectar patr√≥n superlativo
        superlative_pattern = None
        # Solo si no hay otros patrones especiales
        if not (is_ranking or has_temporal_conditional or has_show_rows or has_list_all):
            superlative_pattern = self.detect_superlative_pattern_english(tokens)
            if superlative_pattern:
                # Configurar estructura para superlativo
                if not structure.main_dimension:
                    target_dim_component = QueryComponent(
                        text=superlative_pattern.target_dimension,
                        type=ComponentType.DIMENSION,
                        confidence=0.95,
                        subtype='superlative_target'
                    )
                    structure.main_dimension = target_dim_component
                
                # Agregar m√©trica impl√≠cita
                if superlative_pattern.implied_metric and not structure.metrics:
                    implied_metric_component = QueryComponent(
                        text=superlative_pattern.implied_metric,
                        type=ComponentType.METRIC,
                        confidence=0.85,
                        subtype='implied_from_verb'
                    )
                    structure.metrics.append(implied_metric_component)
                
                # Marcar como superlativo
                structure.superlative_pattern = superlative_pattern
                structure.is_superlative_query = True
                structure.query_pattern = QueryPattern.REFERENCED
                structure.is_single_result = True
                structure.limit_value = 1
        
        # PASO 9: Detectar y aplicar patr√≥n COUNT
        structure = self.detect_and_apply_count_pattern(structure, tokens)
        
        print(f"üèóÔ∏è English structure built successfully:")
        print(f"   üìä Operations: {len(operations)}")
        print(f"   üìà Metrics: {len(structure.metrics)}")
        print(f"   üéØ Query pattern: {query_pattern.value}")
        print(f"   üèÜ Is ranking: {is_ranking}")
        print(f"   üîó Is compound: {is_compound}")
        print(f"   üîó Is multi-dimensional: {is_multi_dimension}")
        print(f"   üïê Has temporal conditional: {has_temporal_conditional}")
        print(f"   üìã Has list all: {has_list_all}")
        print(f"   üìä Has show rows: {has_show_rows}")
        
        return structure



    def detect_ranking_criteria_english(self, tokens: List[str], classified_components: Dict) -> Optional[RankingCriteria]:
        """üá∫üá∏ DETECTOR DE CRITERIOS DE RANKING EN INGL√âS - VERSI√ìN SIMPLIFICADA QUE FUNCIONA"""
        print(f"üèÜ DETECTING ENGLISH RANKING CRITERIA:")
        print(f"   üî§ Tokens: {tokens}")
        
        # Validaci√≥n contextual: Verificar si es parte de SHOW_ROWS primero
        show_rows_indicators = {'rows', 'row', 'records', 'record', 'entries', 'lines'}
        
        for i in range(len(tokens) - 1):
            if tokens[i].lower() in ['first', 'last', 'top', 'bottom']:
                if i + 2 < len(tokens):
                    next_token = tokens[i + 1]
                    after_next = tokens[i + 2].lower() if i + 2 < len(tokens) else None
                    
                    if (next_token.isdigit() or next_token.lower() in self.dictionaries.numeros_palabras_en) and \
                    after_next in show_rows_indicators:
                        
                        # Verificar contexto adicional
                        has_metric_context = False
                        for j in range(i + 3, len(tokens)):
                            if tokens[j].lower() in ['with', 'having', 'by', 'sales', 'revenue', 'more', 'most', 'best']:
                                has_metric_context = True
                                break
                        
                        if not has_metric_context:
                            print(f"   ‚ùå Detected SHOW_ROWS pattern, not ranking: {tokens[i]} {next_token} {after_next}")
                            return None
        
        # English ranking indicators
        top_indicators = {'top', 'best', 'highest', 'maximum', 'first', 'greatest', 'most'}
        bottom_indicators = {'worst', 'lowest', 'minimum', 'last', 'least', 'bottom'}
        
        ranking_direction = None
        ranking_start_idx = -1
        
        for i, token in enumerate(tokens):
            token_lower = token.lower()
            
            if token_lower in top_indicators:
                ranking_direction = RankingDirection.TOP
                ranking_start_idx = i
                print(f"   üîù TOP indicator found: '{token}' at position {i}")
                break
            elif token_lower in bottom_indicators:
                ranking_direction = RankingDirection.BOTTOM
                ranking_start_idx = i
                print(f"   üìâ BOTTOM indicator found: '{token}' at position {i}")
                break
        
        if not ranking_direction:
            print(f"   ‚ùå No ranking indicators found")
            return None
        
        # Find ranking value
        ranking_value = None
        ranking_unit = None
        value_tokens = []
        
        search_end = min(ranking_start_idx + 4, len(tokens))
        
        for i in range(ranking_start_idx + 1, search_end):
            if i >= len(tokens):
                break
            
            token = tokens[i]
            
            # Percentage: "25%", "10.5%"
            if token.endswith('%'):
                try:
                    percent_value = float(token[:-1])
                    ranking_value = percent_value
                    ranking_unit = RankingUnit.PERCENTAGE
                    value_tokens.append(token)
                    print(f"   üìä Percentage detected: {percent_value}%")
                    break
                except ValueError:
                    continue
            
            # Number: "5", "10"
            elif token.isdigit():
                ranking_value = int(token)
                ranking_unit = RankingUnit.COUNT
                value_tokens.append(token)
                print(f"   üî¢ Number detected: {ranking_value}")
                break
            
            # English number words: "five", "ten"
            elif token.lower() in self.dictionaries.numeros_palabras_en:
                ranking_value = self.dictionaries.numeros_palabras_en[token.lower()]
                ranking_unit = RankingUnit.COUNT
                value_tokens.append(token)
                print(f"   üî§ English number word detected: {token} = {ranking_value}")
                break
        
        # üîß FIX: ELIMINAR EL DEFAULT INTELIGENTE - SI NO HAY N√öMERO, NO ES RANKING V√ÅLIDO
        if ranking_value is None:
            print(f"   ‚ùå No numeric value found after indicator - not a valid ranking")
            return None
        
        # Find ranking metric and operation
        ranking_metric = None
        ranking_operation = None
        
        for token, component in classified_components.items():
            if component.type == ComponentType.METRIC and not ranking_metric:
                ranking_metric = component
                print(f"   üìä Ranking metric: {component.text}")
            elif component.type == ComponentType.OPERATION and not ranking_operation:
                # Filtrar indicadores de ranking que no son operaciones reales
                if component.text.lower() not in ['top', 'bottom', 'best', 'worst', 'first', 'last']:
                    ranking_operation = component
                    print(f"   ‚ö° Ranking operation: {component.text}")
        
        # Si no hay m√©trica expl√≠cita, buscar en tokens restantes
        if not ranking_metric:
            metric_keywords = ['sales', 'revenue', 'profit', 'inventory', 'margin', 'cost']
            for i, token in enumerate(tokens):
                if token.lower() in metric_keywords:
                    implied_metric = QueryComponent(
                        text=token.lower(),
                        type=ComponentType.METRIC,
                        confidence=0.85,
                        subtype='implied_ranking_metric',
                        linguistic_info={'source': 'ranking_context_detection'}
                    )
                    ranking_metric = implied_metric
                    print(f"   üìä Implied ranking metric: {token}")
                    break
        
        # Calculate confidence
        confidence_factors = []
        base_confidence = 0.5
        
        base_confidence += 0.3  # Has ranking indicator
        confidence_factors.append("ranking_indicator")
        
        base_confidence += 0.2  # Has numeric value
        confidence_factors.append("numeric_value")
        
        if ranking_metric:
            base_confidence += 0.1
            confidence_factors.append("metric_found")
        
        if ranking_operation:
            base_confidence += 0.1
            confidence_factors.append("operation_found")
        
        final_confidence = min(1.0, base_confidence)
        
        raw_tokens = tokens[ranking_start_idx:ranking_start_idx + len(value_tokens) + 1] if value_tokens else [tokens[ranking_start_idx]]
        
        ranking_criteria = RankingCriteria(
            direction=ranking_direction,
            unit=ranking_unit,
            value=ranking_value,
            metric=ranking_metric,
            operation=ranking_operation,
            confidence=final_confidence,
            raw_tokens=raw_tokens
        )
        
        print(f"üèÜ ENGLISH RANKING CRITERIA DETECTED:")
        print(f"   üéØ Direction: {ranking_direction.value}")
        print(f"   üìä Unit: {ranking_unit.value}")
        print(f"   üî¢ Value: {ranking_value}")
        print(f"   üìà Metric: {ranking_metric.text if ranking_metric else 'N/A'}")
        print(f"   ‚ö° Operation: {ranking_operation.text if ranking_operation else 'N/A'}")
        print(f"   ‚≠ê Confidence: {final_confidence:.2f}")
        
        return ranking_criteria  # üîß FIX: RETURN QUE FALTABA


    def detect_compound_criteria_english(self, tokens: List[str], classified_components: Dict) -> List[CompoundCriteria]:
        """üá∫üá∏ DETECTOR DE CONSULTAS COMPUESTAS EN INGL√âS"""
        print(f"üîó DETECTING ENGLISH COMPOUND CRITERIA:")
        print(f"   üî§ Tokens: {tokens}")
        
        compound_criteria = []
        
        # Split by English connectors
        segments = self.split_by_connector_english(tokens, 'and')
        
        print(f"   üìä Segments detected: {segments}")
        
        for i, segment in enumerate(segments):
            print(f"\n   üéØ Processing English segment {i+1}: {segment}")
            
            criteria = self.extract_criteria_from_segment_english(segment, classified_components)
            if criteria:
                compound_criteria.append(criteria)
                print(f"      ‚úÖ English criteria extracted: {criteria.operation.text} {criteria.metric.text}")
            else:
                print(f"      ‚ùå Could not extract criteria from segment")
        
        print(f"\nüîó TOTAL ENGLISH COMPOUND CRITERIA: {len(compound_criteria)}")
        for i, criteria in enumerate(compound_criteria):
            print(f"   {i+1}. {criteria.operation.text} {criteria.metric.text} (confidence: {criteria.confidence:.2f})")
        
        return compound_criteria


    def split_by_connector_english(self, tokens: List[str], connector: str) -> List[List[str]]:
        """üá∫üá∏ DIVISOR POR CONECTORES EN INGL√âS"""
        segments = []
        current_segment = []
        
        for token in tokens:
            if token.lower() == connector.lower():
                if current_segment:
                    segments.append(current_segment)
                    current_segment = []
            else:
                current_segment.append(token)
        
        if current_segment:
            segments.append(current_segment)
        
        return segments


    def extract_criteria_from_segment_english(self, segment: List[str], classified_components: Dict) -> Optional[CompoundCriteria]:
        """üá∫üá∏ EXTRACTOR DE CRITERIOS DE SEGMENTOS EN INGL√âS"""
        operation_found = None
        metric_found = None
        dimension_candidate = None
        confidence_sum = 0.0
        count = 0
        
        print(f"      üîç Analyzing English segment: {segment}")
        
        # PRIMERA PASADA: Buscar operaciones y m√©tricas REALES
        for token in segment:
            if token in classified_components:
                component = classified_components[token]
                
                # Buscar operaci√≥n
                if component.type == ComponentType.OPERATION and not operation_found:
                    operation_found = component
                    confidence_sum += component.confidence
                    count += 1
                    print(f"         ‚ö° English operation found: {token}")
                
                # Priorizar m√©tricas reales
                elif component.type == ComponentType.METRIC and not metric_found:
                    metric_found = component
                    confidence_sum += component.confidence
                    count += 1
                    print(f"         üìä English REAL metric found: {token}")
                
                # Guardar dimensi√≥n como candidato
                elif component.type == ComponentType.DIMENSION and not dimension_candidate:
                    dimension_candidate = component
                    print(f"         üìç English dimension candidate: {token}")
        
        # SEGUNDA PASADA: Solo si NO hay m√©trica real, usar dimensi√≥n
        if not metric_found and dimension_candidate:
            metric_component = QueryComponent(
                text=dimension_candidate.text,
                type=ComponentType.METRIC,
                confidence=dimension_candidate.confidence * 0.85,
                subtype='converted_from_dimension',
                value=dimension_candidate.value,
                column_name=dimension_candidate.column_name,
                linguistic_info={'converted_from': 'dimension'}
            )
            metric_found = metric_component
            confidence_sum += metric_component.confidence
            count += 1
            print(f"         üîÑ English dimension converted to metric: {dimension_candidate.text}")
        
        # VALIDACI√ìN FINAL
        if operation_found and metric_found:
            avg_confidence = confidence_sum / count if count > 0 else 0.0
            
            print(f"         ‚úÖ English criteria complete: {operation_found.text} + {metric_found.text}")
            
            return CompoundCriteria(
                operation=operation_found,
                metric=metric_found,
                confidence=avg_confidence,
                raw_tokens=segment
            )
        
        print(f"         ‚ùå English criteria incomplete:")
        print(f"             Operation: {operation_found.text if operation_found else 'NOT FOUND'}")
        print(f"             Metric: {metric_found.text if metric_found else 'NOT FOUND'}")
        
        return None


    def detect_multi_dimensions_english(self, tokens: List[str], classified_components: Dict) -> List[QueryComponent]:
        """üá∫üá∏ DETECTOR DE M√öLTIPLES DIMENSIONES EN INGL√âS"""
        
        print(f"üîó DETECTING ENGLISH MULTIPLE DIMENSIONS:")
        
        # PASO 1: Identificar dimensiones y conectores
        dimension_candidates = []
        connector_positions = []
        
        for i, token in enumerate(tokens):
            if token in classified_components:
                component = classified_components[token]
                if component.type == ComponentType.DIMENSION:
                    dimension_candidates.append((i, component))
                elif (component.type == ComponentType.CONNECTOR and 
                    token.lower() in ['and', 'with', ',']):
                    connector_positions.append(i)
        
        print(f"   üìç English dimensions found: {[(i, comp.text) for i, comp in dimension_candidates]}")
        print(f"   üîó English connectors at positions: {connector_positions}")
        
        # PASO 2: Validar patr√≥n secuencial
        if len(dimension_candidates) >= 2 and len(connector_positions) >= 1:
            valid_dimensions = self._validate_dimension_sequence_english(
                dimension_candidates, connector_positions, tokens
            )
            
            if len(valid_dimensions) >= 2:
                print(f"   ‚úÖ ENGLISH MULTIPLE DIMENSIONS valid: {[d.text for d in valid_dimensions]}")
                return valid_dimensions
        
        print(f"   ‚ùå No valid English multi-dimensional pattern detected")
        return []


    def _validate_dimension_sequence_english(self, dimension_candidates: List, connector_positions: List, tokens: List[str]) -> List[QueryComponent]:
        """üá∫üá∏ VALIDADOR DE SECUENCIA DIMENSIONAL EN INGL√âS"""
        valid_dimensions = []
        
        for i, (pos, component) in enumerate(dimension_candidates):
            if i == 0:
                # Primera dimensi√≥n siempre v√°lida
                valid_dimensions.append(component)
            else:
                # Verificar que hay conector antes de esta dimensi√≥n
                prev_dim_pos = dimension_candidates[i-1][0]
                has_connector_between = any(
                    prev_dim_pos < conn_pos < pos 
                    for conn_pos in connector_positions
                )
                
                if has_connector_between:
                    valid_dimensions.append(component)
                    print(f"      ‚úÖ English '{component.text}' valid (connector found)")
                else:
                    print(f"      ‚ùå English '{component.text}' invalid (no connector)")
                    break
        
        return valid_dimensions


    def detect_exclusion_filters_english(self, tokens: List[str], classified_components: Dict) -> List[ExclusionFilter]:
        """üá∫üá∏ DETECTOR DE FILTROS DE EXCLUSI√ìN EN INGL√âS"""
        print(f"üö´ DETECTING ENGLISH EXCLUSION FILTERS:")
        
        exclusion_filters = []
        
        # English exclusion indicators
        exclusion_indicators = {
            'excluding', 'except', 'without', 'not', 'minus', 'omitting', 'excluding'
        }
        
        # Buscar indicadores de exclusi√≥n
        for i, token in enumerate(tokens):
            token_lower = token.lower()
            
            if token_lower in exclusion_indicators:
                print(f"   üö´ English exclusion indicator found: '{token}' at position {i}")
                
                # Buscar patr√≥n [COLUMNA] [VALOR] despu√©s del indicador
                exclusion_filter = self.extract_exclusion_from_position_english(tokens, i + 1, classified_components)
                
                if exclusion_filter:
                    exclusion_filters.append(exclusion_filter)
                    print(f"   ‚úÖ English exclusion filter extracted: {exclusion_filter.column_name} != '{exclusion_filter.value}'")
        
        print(f"üö´ TOTAL ENGLISH EXCLUSION FILTERS: {len(exclusion_filters)}")
        return exclusion_filters


    def extract_exclusion_from_position_english(self, tokens: List[str], start_pos: int, classified_components: Dict) -> Optional[ExclusionFilter]:
        """üá∫üá∏ EXTRACTOR DE EXCLUSIONES POSICIONALES EN INGL√âS"""
        if start_pos >= len(tokens) - 1:
            return None
        
        # Buscar patr√≥n [COLUMNA] [VALOR] en las siguientes posiciones
        search_end = min(start_pos + 3, len(tokens))
        
        for i in range(start_pos, search_end - 1):
            if i + 1 >= len(tokens):
                break
            
            current_token = tokens[i]
            next_token = tokens[i + 1]
            
            print(f"      üîç Analyzing English exclusion: '{current_token}' + '{next_token}'")
            
            # Verificar si current_token es una columna potencial
            if self._is_potential_column_english(current_token):
                # Verificar si next_token es un valor
                if self._is_potential_value_english(next_token):
                    # Construir filtro de exclusi√≥n
                    confidence = 0.8  # Base confidence for exclusions
                    
                    return ExclusionFilter(
                        exclusion_type=ExclusionType.NOT_EQUALS,
                        column_name=current_token.lower(),
                        value=next_token.upper(),
                        confidence=confidence,
                        raw_tokens=tokens[start_pos-1:i+2]
                    )
        
        return None


    def detect_query_pattern_english(self, structure: QueryStructure) -> QueryPattern:
        """üá∫üá∏ DETECTOR DE PATR√ìN DE CONSULTA EN INGL√âS - PRIORIDAD CORREGIDA"""
        print(f"üîç DETECTING ENGLISH QUERY PATTERN:")
        print(f"   üìç Dimension: {structure.main_dimension.text if structure.main_dimension else 'N/A'}")
        print(f"   üîó Multiple dimensions: {len(structure.main_dimensions) if structure.main_dimensions else 0}")
        print(f"   ‚ö° Operations: {[op.text for op in structure.operations]}")
        print(f"   üìä Metrics: {[m.text for m in structure.metrics]}")
        print(f"   üèÜ Is ranking: {structure.is_ranking_query}")
        print(f"   üîó Is compound: {structure.is_compound_query}")
        print(f"   üìê Is multi-dimensional: {structure.is_multi_dimension_query}")

        # üÜï PRIORIDAD 0: PATRONES ESPECIALES (m√°xima prioridad)
        
        # SHOW ROWS tiene la m√°xima prioridad
        if hasattr(structure, 'show_rows_pattern') and structure.show_rows_pattern:
            print(f"   üìä ENGLISH PATTERN: SHOW_ROWS (special pattern priority)")
            structure.confidence_score = structure.show_rows_pattern.get('confidence', 0.9)
            return QueryPattern.SHOW_ROWS
        
        # LIST ALL tiene segunda prioridad
        if hasattr(structure, 'list_all_pattern') and structure.list_all_pattern:
            print(f"   üìã ENGLISH PATTERN: LIST_ALL (special pattern priority)")
            structure.confidence_score = structure.list_all_pattern.get('confidence', 0.8)
            return QueryPattern.LIST_ALL
        
        # TEMPORAL CONDITIONAL tiene tercera prioridad
        if hasattr(structure, 'temporal_conditional_pattern') and structure.temporal_conditional_pattern:
            print(f"   üïí ENGLISH PATTERN: TEMPORAL_CONDITIONAL (special pattern priority)")
            structure.confidence_score = structure.temporal_conditional_pattern.get('confidence', 0.8)
            return QueryPattern.TEMPORAL_CONDITIONAL
        
        # PATR√ìN 1: RANKING (incluyendo multi-dimensionales)
        if structure.is_ranking_query and structure.ranking_criteria:
            confidence = self.calculate_ranking_confidence_english(structure)
            if confidence >= 0.7:
                print(f"   üèÜ ENGLISH PATTERN: TOP_N (ranking, confidence: {confidence:.2f})")
                structure.confidence_score = confidence
                return QueryPattern.TOP_N
        
        # PATR√ìN 2: M√öLTIPLES DIMENSIONES SIN RANKING
        if (structure.is_multi_dimension_query and 
            len(structure.main_dimensions) >= 2 and 
            not structure.is_ranking_query):
            confidence = self.calculate_multi_dimension_confidence_english(structure)
            if confidence >= 0.7:
                print(f"   üîó ENGLISH PATTERN: MULTI_DIMENSION (confidence: {confidence:.2f})")
                structure.confidence_score = confidence
                return QueryPattern.MULTI_DIMENSION
        
        # PATR√ìN 3: CONSULTAS COMPUESTAS REFERENCIADAS
        if (structure.is_compound_query and 
            structure.main_dimension and 
            len(structure.compound_criteria) >= 2):
            
            all_reference_operations = True
            reference_operations = ['m√°ximo', 'm√≠nimo', 'mayor', 'menor']
            
            for criteria in structure.compound_criteria:
                if criteria.operation.value not in reference_operations:
                    all_reference_operations = False
                    break
            
            if all_reference_operations:
                confidence = self.calculate_compound_reference_confidence_english(structure)
                if confidence >= 0.7:
                    print(f"   üéØ ENGLISH PATTERN: REFERENCED (compound, confidence: {confidence:.2f})")
                    structure.confidence_score = confidence
                    return QueryPattern.REFERENCED
        
        # PATR√ìN 4: DATOS REFERENCIADOS SIMPLES
        if (structure.main_dimension and 
            len(structure.operations) >= 1 and 
            len(structure.metrics) >= 1 and 
            len(structure.column_conditions) == 0 and
            not structure.is_ranking_query):
            
            operation = structure.operations[0]
            reference_operations = ['m√°ximo', 'm√≠nimo', 'mayor', 'menor']
            
            if operation.value in reference_operations:
                confidence = self.calculate_reference_confidence_english(structure)
                if confidence >= 0.7:
                    print(f"   üéØ ENGLISH PATTERN: REFERENCED (simple, confidence: {confidence:.2f})")
                    structure.confidence_score = confidence
                    return QueryPattern.REFERENCED
        
        # PATR√ìN 5: AGREGACI√ìN COMPLETA
        if (len(structure.operations) >= 1 and 
            len(structure.metrics) >= 1 and 
            not structure.main_dimension):
            
            print(f"   üìä ENGLISH PATTERN: AGGREGATION (global)")
            structure.confidence_score = 0.90
            return QueryPattern.AGGREGATION
        
        # PATR√ìN 6: AGREGACI√ìN CON DIMENSI√ìN
        if (structure.main_dimension and 
            len(structure.operations) >= 1 and 
            len(structure.metrics) >= 1):
            
            print(f"   üìä ENGLISH PATTERN: AGGREGATION (with grouping)")
            structure.confidence_score = 0.85
            return QueryPattern.AGGREGATION
        
        # PATR√ìN 7: LISTAR TODOS
        if (structure.main_dimension and 
            len(structure.operations) == 0):
            
            print(f"   üìã ENGLISH PATTERN: LIST_ALL")
            structure.confidence_score = 0.80
            return QueryPattern.LIST_ALL
        
        # PATR√ìN: Metrica de Valor ‚Üí Dimensi√≥n impl√≠cita
        if (not structure.main_dimension and 
            structure.metrics and 
            len(structure.column_conditions) > 0):
            
            # Si tenemos m√©trica + filtros pero no dimensi√≥n
            # La dimensi√≥n impl√≠cita es la columna del primer filtro
            first_filter = structure.column_conditions[0]
            
            # Crear dimensi√≥n impl√≠cita basada en el filtro
            if first_filter.column_name.lower() in ['item', 'store', 'account', 'brand']:
                implicit_dimension = QueryComponent(
                    text=first_filter.column_name.lower(),
                    type=ComponentType.DIMENSION,
                    confidence=0.85,
                    subtype='implicit_from_filter',
                    linguistic_info={'source': 'implicit_dimension_from_filter'}
                )
                
                structure.main_dimension = implicit_dimension
                
                print(f"   üéØ IMPLICIT DIMENSION from filter: {first_filter.column_name}")
                print(f"   üìä ENGLISH PATTERN: AGGREGATION (metric of value)")
                structure.confidence_score = 0.8
                return QueryPattern.AGGREGATION
        
        print(f"   ‚ùì ENGLISH PATTERN: UNKNOWN")
        structure.confidence_score = 0.4
        return QueryPattern.UNKNOWN


    def is_ranking_query_english(self, ranking_criteria: Optional[RankingCriteria], exclusion_filters: List[ExclusionFilter]) -> bool:
        """üá∫üá∏ VERIFICADOR DE CONSULTA DE RANKING EN INGL√âS"""
        has_valid_ranking = ranking_criteria and ranking_criteria.confidence >= 0.6
        is_ranking = bool(has_valid_ranking)
        
        print(f"üèÜ EVALUATING ENGLISH RANKING QUERY:")
        print(f"   üìä Has valid criteria: {has_valid_ranking}")
        print(f"   üö´ Exclusion filters: {len(exclusion_filters)}")
        print(f"   üéØ Is ranking: {is_ranking}")
        
        return is_ranking


    def is_compound_query_english(self, compound_criteria: List[CompoundCriteria]) -> bool:
        """üá∫üá∏ VERIFICADOR DE CONSULTA COMPUESTA EN INGL√âS"""
        valid_criteria = [c for c in compound_criteria if c.confidence >= 0.6]
        is_compound = len(valid_criteria) >= 2
        
        print(f"üîó EVALUATING ENGLISH COMPOUND QUERY:")
        print(f"   üìä Valid criteria: {len(valid_criteria)}")
        print(f"   üéØ Is compound: {is_compound}")
        
        return is_compound


    def calculate_ranking_confidence_english(self, structure: QueryStructure) -> float:
        """üá∫üá∏ CALCULADOR DE CONFIANZA DE RANKING EN INGL√âS"""
        print(f"   üîç CALCULATING ENGLISH RANKING CONFIDENCE:")
        
        if not structure.ranking_criteria:
            return 0.0
        
        base_confidence = structure.ranking_criteria.confidence
        factors = ['base_criteria']
        
        # Factor 1: Tiene dimensi√≥n principal
        if structure.main_dimension:
            base_confidence += 0.1
            factors.append("has_dimension")
        
        # Factor 2: Tipo de unidad
        if structure.ranking_criteria.unit == RankingUnit.PERCENTAGE:
            base_confidence += 0.05
            factors.append("uses_percentage")
        elif structure.ranking_criteria.unit == RankingUnit.COUNT:
            base_confidence += 0.03
            factors.append("uses_count")
        
        # Factor 3: Tiene m√©trica espec√≠fica
        if structure.ranking_criteria.metric:
            base_confidence += 0.05
            factors.append("specific_metric")
        
        # Factor 4: Valor razonable
        if structure.ranking_criteria.unit == RankingUnit.COUNT and 1 <= structure.ranking_criteria.value <= 50:
            base_confidence += 0.03
            factors.append("reasonable_count_value")
        elif structure.ranking_criteria.unit == RankingUnit.PERCENTAGE and 1 <= structure.ranking_criteria.value <= 100:
            base_confidence += 0.03
            factors.append("reasonable_percentage_value")
        
        final_confidence = max(0.0, min(1.0, base_confidence))
        
        print(f"      üìä English factors applied: {factors}")
        print(f"      ‚≠ê English confidence: {final_confidence:.2f}")
        
        return final_confidence


    def calculate_multi_dimension_confidence_english(self, structure: QueryStructure) -> float:
        """üá∫üá∏ CALCULADOR DE CONFIANZA MULTI-DIMENSIONAL EN INGL√âS"""
        print(f"   üîç CALCULATING ENGLISH MULTI-DIMENSION CONFIDENCE:")
        
        base_confidence = 0.6
        factors = ['base_multi_dimension']
        
        # Factor 1: N√∫mero de dimensiones
        extra_dims = len(structure.main_dimensions) - 2
        if extra_dims > 0:
            bonus = min(extra_dims * 0.05, 0.15)
            base_confidence += bonus
            factors.append(f"extra_dimensions_{extra_dims}")
        
        # Factor 2: Tiene operaci√≥n y m√©trica
        if structure.operations and structure.metrics:
            base_confidence += 0.2
            factors.append("operation_metric")
        
        # Factor 3: Sin filtros complejos
        if len(structure.column_conditions) == 0:
            base_confidence += 0.1
            factors.append("no_complex_filters")
        
        final_confidence = max(0.0, min(1.0, base_confidence))
        
        print(f"      üìä English factors applied: {factors}")
        print(f"      ‚≠ê English confidence: {final_confidence:.2f}")
        
        return final_confidence


    def calculate_reference_confidence_english(self, structure: QueryStructure) -> float:
        """üá∫üá∏ CALCULADOR DE CONFIANZA REFERENCIAL EN INGL√âS"""
        print(f"   üîç CALCULATING ENGLISH REFERENCE CONFIDENCE:")
        
        base_confidence = 0.5
        factors = []
        
        # Factor 1: Tiene dimensi√≥n
        if structure.main_dimension:
            base_confidence += 0.15
            factors.append("has_dimension")
        
        # Factor 2: Operaci√≥n √∫nica
        if len(structure.operations) == 1:
            base_confidence += 0.1
            factors.append("single_operation")
        
        # Factor 3: M√©trica √∫nica
        if len(structure.metrics) == 1:
            base_confidence += 0.1
            factors.append("single_metric")
        
        # Factor 4: Sin filtros de columna
        if len(structure.column_conditions) == 0:
            base_confidence += 0.1
            factors.append("no_column_filters")
        
        # Factor 5: Operaci√≥n de comparaci√≥n
        if structure.operations and structure.operations[0].value in ['m√°ximo', 'm√≠nimo']:
            base_confidence += 0.2
            factors.append("comparison_operation")
        
        final_confidence = max(0.0, min(1.0, base_confidence))
        
        print(f"      üìä English factors applied: {factors}")
        print(f"      ‚≠ê English confidence: {final_confidence:.2f}")
        
        return final_confidence


    def calculate_compound_reference_confidence_english(self, structure: QueryStructure) -> float:
        """üá∫üá∏ CALCULADOR DE CONFIANZA COMPUESTA EN INGL√âS"""
        print(f"   üîç CALCULATING ENGLISH COMPOUND REFERENCE CONFIDENCE:")
        
        base_confidence = 0.6
        factors = []
        
        # Factor 1: Tiene dimensi√≥n
        if structure.main_dimension:
            base_confidence += 0.1
            factors.append("has_dimension")
        
        # Factor 2: N√∫mero de criterios v√°lidos
        valid_criteria = len([c for c in structure.compound_criteria if c.confidence >= 0.7])
        criteria_bonus = min(valid_criteria * 0.05, 0.15)
        base_confidence += criteria_bonus
        factors.append(f"valid_criteria_{valid_criteria}")
        
        # Factor 3: Sin filtros de columna
        if len(structure.column_conditions) == 0:
            base_confidence += 0.1
            factors.append("no_column_filters")
        
        # Factor 4: Todas las operaciones son de comparaci√≥n
        reference_operations = ['m√°ximo', 'm√≠nimo', 'mayor', 'menor']
        all_reference = all(
            criteria.operation.value in reference_operations 
            for criteria in structure.compound_criteria
        )
        if all_reference:
            base_confidence += 0.1
            factors.append("all_comparison_operations")
        
        final_confidence = max(0.0, min(1.0, base_confidence))
        
        print(f"      üìä English factors applied: {factors}")
        print(f"      ‚≠ê English confidence: {final_confidence:.2f}")
        
        return final_confidence


    def generate_multi_dimension_english_sql(self, structure: QueryStructure, temporal_columns: set) -> str:
        """üîß GENERADOR SQL PARA M√öLTIPLES DIMENSIONES - VERSI√ìN CORREGIDA"""
        print(f"üîó GENERANDO SQL PARA M√öLTIPLES DIMENSIONES:")
        
        select_parts = []
        group_by_parts = []
        order_by_parts = []
        where_conditions = []
        
        # PASO 1: Agregar todas las dimensiones principales
        for dimension in structure.main_dimensions:
            dim_name = dimension.text
            formatted_dim = self.format_temporal_dimension(dim_name)
            select_parts.append(formatted_dim)
            group_by_parts.append(dim_name)  
                
        # PASO 2: üîß BUSCAR LA M√âTRICA CORRECTA PARA EL RANKING
        ranking_metric = None
        operation_value = None
        
        # Prioridad 1: M√©trica especificada en ranking_criteria
        if structure.ranking_criteria and structure.ranking_criteria.metric:
            ranking_metric = structure.ranking_criteria.metric
            print(f"   üìä M√©trica del ranking: {ranking_metric.text}")
        
        # Prioridad 2: Buscar m√©tricas reales (NO convertidas de dimensiones)
        else:
            real_metrics = [
                m for m in structure.metrics 
                if not m.linguistic_info.get('converted_from') == 'dimension'
            ]
            
            if real_metrics:
                ranking_metric = real_metrics[0]
                print(f"   üìä M√©trica real encontrada: {ranking_metric.text}")
            else:
                # Fallback: usar la primera m√©trica disponible
                if structure.metrics:
                    ranking_metric = structure.metrics[0]
                    print(f"   üìä M√©trica fallback: {ranking_metric.text}")
        
        # PASO 3: Determinar operaci√≥n
        if structure.operations:
            # Buscar operaci√≥n relevante (no ranking indicators)
            relevant_operations = [
                op for op in structure.operations 
                if op.value not in ['top', 'bottom'] and op.subtype != 'ranking_indicator'
            ]
            
            if relevant_operations:
                operation = relevant_operations[0]
                operation_value = operation.value
                print(f"   ‚ö° Operaci√≥n relevante: {operation.text} ‚Üí {operation_value}")
            else:
                # Si solo hay indicadores de ranking, usar operaci√≥n por defecto
                operation_value = 'suma'  # Por defecto para rankings
                print(f"   ‚ö° Usando operaci√≥n por defecto: suma")
        else:
            operation_value = 'suma'
            print(f"   ‚ö° Sin operaciones, usando por defecto: suma")
        
        # PASO 4: Construir funci√≥n de agregaci√≥n
        if ranking_metric:
            if operation_value == 'm√°ximo':
                agg_function = self._get_contextual_aggregation_english(structure, ranking_metric.text, operation_value)
            else:
                sql_operations = {
                    'm√≠nimo': f'MIN({ranking_metric.text})',
                    'suma': f'SUM({ranking_metric.text})',
                    'promedio': f'AVG({ranking_metric.text})',
                    'conteo': f'COUNT({ranking_metric.text})'
                }
                agg_function = sql_operations.get(operation_value, f'SUM({ranking_metric.text})')
            
            select_parts.append(agg_function)
            
            # Determinar orden basado en ranking
            if structure.ranking_criteria:
                if structure.ranking_criteria.direction == RankingDirection.TOP:
                    order_direction = "DESC"
                else:
                    order_direction = "ASC"
            else:
                # Determinar orden basado en operaci√≥n
                if operation_value in ['m√°ximo', 'mayor']:
                    order_direction = "DESC"
                elif operation_value in ['m√≠nimo', 'menor']:
                    order_direction = "ASC"
                else:
                    order_direction = "DESC"
            
            order_by_parts.append(f"{agg_function} {order_direction}")
            print(f"   üìä Agregaci√≥n: {agg_function} {order_direction}")
        else:
            print(f"   ‚ùå No se encontr√≥ m√©trica v√°lida para el ranking")
            return "SELECT * FROM datos;"
        
        # PASO 5: WHERE conditions
        for condition in structure.column_conditions:
            if condition.column_name not in temporal_columns:
                where_conditions.append(f"{condition.column_name} = '{condition.value}'")
        
        # PASO 6: Filtros temporales
        advanced_conditions = self.get_advanced_temporal_sql_conditions_english(structure)
        where_conditions.extend(advanced_conditions)
        
        # PASO 7: Construir SQL final
        sql_parts = [f"SELECT {', '.join(select_parts)}", "FROM datos"]
        
        if where_conditions:
            sql_parts.append(f"WHERE {' AND '.join(where_conditions)}")
        
        if group_by_parts:
            sql_parts.append(f"GROUP BY {', '.join(group_by_parts)}")
        
        if order_by_parts:
            sql_parts.append(f"ORDER BY {', '.join(order_by_parts)}")
        
        # PASO 8: Aplicar l√≠mite
        if structure.is_ranking_query and structure.ranking_criteria:
            ranking_value = int(structure.ranking_criteria.value)
            sql_parts.append(f"LIMIT {ranking_value}")
            print(f"   üèÜ APLICANDO LIMIT de ranking: {ranking_value}")
        else:
            sql_parts.append("LIMIT 10")  # L√≠mite por defecto m√°s razonable
            print(f"   üìç APLICANDO LIMIT por defecto: 10")
        
        final_sql = " ".join(sql_parts) + ";"
        print(f"   üéØ SQL multi-dimensional: {final_sql}")
        
        return final_sql


    def _get_contextual_aggregation_english(self, structure: QueryStructure, metric_text: str, operation: str) -> str:
        """Usar intent sem√°ntico original (pre-mapeo) para decidir SUM vs MAX"""
        
        if operation == 'm√°ximo':
            # üéØ USAR INTENT ORIGINAL (analizado ANTES del mapeo)
            original_intent = getattr(structure, 'original_semantic_intent', 'DEFAULT')
            
            if original_intent == 'MAX':
                print(f"   üéØ INTENT ORIGINAL: MAX ‚Üí MAX({metric_text}) [palabras originales singulares]")
                return f'MAX({metric_text})'
            elif original_intent == 'SUM':
                print(f"   üéØ INTENT ORIGINAL: SUM ‚Üí SUM({metric_text}) [palabras originales plurales]")
                return f'SUM({metric_text})'
            else:
                print(f"   üéØ INTENT ORIGINAL: DEFAULT ‚Üí SUM({metric_text}) [configuraci√≥n por defecto]")
                return f'SUM({metric_text})'  # Tu configuraci√≥n por defecto
        
        return f'SUM({metric_text})'  

                
    def generate_optimized_sql_english(self, structure: QueryStructure) -> str:
        """
        üá∫üá∏ GENERADOR SQL OPTIMIZADO CON LIST ALL MEJORADO Y MULTI-M√âTRICAS
        
        Versi√≥n mejorada del m√©todo existente - mantiene el mismo nombre
        """
        
        print(f"üîß GENERATING OPTIMIZED SQL (Enhanced with LIST ALL support):")
        
        # üÜï FAST PATH PARA MULTI-M√âTRICAS COMPUESTAS - CORREGIDO
        if (structure.is_compound_query and 
            len(structure.compound_criteria) > 1 and 
            not structure.main_dimension):
            
            print(f"üìä MULTI-METRIC COMPOUND detected - using fast path")
            
            select_parts = []
            for criteria in structure.compound_criteria:
                if criteria.operation.value == 'm√°ximo':
                    agg_function = self._get_contextual_aggregation_english(structure, criteria.metric.text, criteria.operation.value)
                else:
                    sql_operations = {
                        'm√≠nimo': f'MIN({criteria.metric.text})',
                        'suma': f'SUM({criteria.metric.text})',
                        'promedio': f'AVG({criteria.metric.text})',
                        'conteo': f'COUNT({criteria.metric.text})',
                        'total': f'SUM({criteria.metric.text})'  # üÜï Mapeo para 'total'
                    }
                    agg_function = sql_operations.get(criteria.operation.value, f'SUM({criteria.metric.text})')
                
                select_parts.append(agg_function)
                print(f"   ‚úÖ Added: {agg_function}")
            
            # Construir SQL
            sql_parts = [f"SELECT {', '.join(select_parts)}", "FROM datos"]
            
            # Agregar WHERE con TODOS los filtros
            where_conditions = []
            
            # 1. Filtros de columna
            for condition in structure.column_conditions:
                where_conditions.append(f"{condition.column_name} = '{condition.value}'")
                print(f"   ‚úÖ Filter: {condition.column_name} = '{condition.value}'")
            
            # 2. üîß CORRECCI√ìN: AGREGAR FILTROS TEMPORALES
            temporal_conditions = self.get_advanced_temporal_sql_conditions_english(structure)
            if temporal_conditions:
                where_conditions.extend(temporal_conditions)
                print(f"   üìÖ Temporal filters added: {temporal_conditions}")
            
            # 3. üÜï OPCIONAL: Agregar filtros de exclusi√≥n si existen
            if hasattr(structure, 'exclusion_filters'):
                for exclusion in structure.exclusion_filters:
                    if exclusion.exclusion_type == ExclusionType.NOT_EQUALS:
                        where_conditions.append(f"{exclusion.column_name} != '{exclusion.value}'")
                        print(f"   üö´ Exclusion filter: {exclusion.column_name} != '{exclusion.value}'")
            
            if where_conditions:
                sql_parts.append(f"WHERE {' AND '.join(where_conditions)}")
            
            final_sql = " ".join(sql_parts) + ";"
            print(f"   üéØ Multi-metric SQL: {final_sql}")
            return final_sql
        
        # Verificar si es consulta superlativa PRIMERO
        if hasattr(structure, 'superlative_pattern') and structure.superlative_pattern:
            print(f"üèÜ DETECTED: Superlative pattern ‚Üí using superlative generator")
            return self.generate_superlative_sql_english(structure.superlative_pattern, structure)
        
        # Verificar si es consulta COUNT
        is_count_query = getattr(structure, 'is_count_query', False)
        if is_count_query:
            print(f"üî¢ COUNT query detected - using COUNT SQL generator")
            return self._generate_count_sql_simple(structure)
        
        # ‚úÖ CASOS ESPECIALES CON LIST ALL MEJORADO
        if (hasattr(structure, 'list_all_pattern') and structure.list_all_pattern):
            print(f"üìã DETECTED: Enhanced English list all ‚Üí using ENHANCED specialized generator")
            return self.generate_enhanced_list_all_sql_english(structure.list_all_pattern, structure)
        
        if (hasattr(structure, 'show_rows_pattern') and structure.show_rows_pattern):
            print(f"üìä DETECTED: English show rows ‚Üí using specialized generator")
            return self.generate_show_rows_sql_english(structure.show_rows_pattern) 
        
        if (hasattr(structure, 'temporal_conditional_pattern') and structure.temporal_conditional_pattern):
            print(f"üïí DETECTED: English temporal conditional ‚Üí using specialized generator")
            return self.generate_temporal_conditional_sql_english(structure.temporal_conditional_pattern)
        
        # ‚úÖ RESTO DEL C√ìDIGO ORIGINAL
        select_parts = []
        from_clause = "FROM datos"
        where_conditions = []
        group_by_parts = []
        order_by_parts = []
        
        # Identificar columnas temporales para evitar duplicaci√≥n
        temporal_columns = set()
        
        for tf in structure.temporal_filters:
            if tf.unit == TemporalUnit.WEEKS:
                temporal_columns.update(['week', 'weeks', 'semana', 'semanas'])
            elif tf.unit == TemporalUnit.MONTHS:
                temporal_columns.update(['month', 'months', 'mes', 'meses'])
            elif tf.unit == TemporalUnit.DAYS:
                temporal_columns.update(['day', 'days', 'dia', 'dias'])
            elif tf.unit == TemporalUnit.YEARS:
                temporal_columns.update(['year', 'years', 'a√±o', 'a√±os'])
        
        print(f"üóÑÔ∏è Generating OPTIMIZED English SQL:")
        print(f"   ‚è∞ Temporal columns detected: {temporal_columns}")
        print(f"   üéØ Query pattern: {structure.query_pattern.value}")
        print(f"   üîó Is compound: {structure.is_compound_query}")
        print(f"   üèÜ Is ranking: {structure.is_ranking_query}")
        print(f"   üîó Is multi-dimensional: {structure.is_multi_dimension_query}")
        
        # üîß MANEJAR RANKINGS MULTI-DIMENSIONALES
        if (structure.is_ranking_query and 
            structure.is_multi_dimension_query and 
            len(structure.main_dimensions) >= 2):
            print(f"üèÜüîó DETECTED: English ranking multi-dimensional ‚Üí using specialized generator")
            return self.generate_multi_dimension_english_sql(structure, temporal_columns)
        
        # MANEJAR CONSULTAS MULTI-DIMENSIONALES SIN RANKING
        if (structure.is_multi_dimension_query and 
            structure.query_pattern == QueryPattern.MULTI_DIMENSION):
            print(f"üîó DETECTED: English multi-dimensional without ranking ‚Üí using specialized generator")
            return self.generate_multi_dimension_english_sql(structure, temporal_columns)
        
        # MANEJAR RANKINGS SIMPLES
        if (structure.is_ranking_query and 
            structure.ranking_criteria and 
            not structure.is_multi_dimension_query):
            print(f"üèÜ DETECTED: English simple ranking ‚Üí using ranking generator")
            return self.generate_ranking_sql_english(structure, temporal_columns)
        
        # VERIFICAR SI ES AGREGACI√ìN GLOBAL
        is_global_aggregation = not structure.main_dimension and structure.operations and structure.metrics
        
        if is_global_aggregation:
            print(f"üåê Generating English SQL for global aggregation")
            
            # üÜï VERIFICAR SI HAY M√öLTIPLES M√âTRICAS EN AGREGACI√ìN GLOBAL
            if structure.is_compound_query and len(structure.compound_criteria) > 1:
                print(f"üìä Detected MULTIPLE metrics in global compound aggregation")
                
                # Procesar cada criterio compuesto
                for i, criteria in enumerate(structure.compound_criteria):
                    operation_value = criteria.operation.value
                    metric_text = criteria.metric.text
                    
                    if operation_value == 'm√°ximo':
                        agg_function = self._get_contextual_aggregation_english(structure, metric_text, operation_value)
                    else:
                        sql_operations = {
                            'm√≠nimo': f'MIN({metric_text})',
                            'suma': f'SUM({metric_text})',
                            'promedio': f'AVG({metric_text})',
                            'conteo': f'COUNT({metric_text})',
                            'total': f'SUM({metric_text})'  # üÜï Mapeo para 'total'
                        }
                        agg_function = sql_operations.get(operation_value, f'SUM({metric_text})')
                    
                    select_parts.append(agg_function)
                    print(f"   ‚úÖ Global metric {i+1}: {agg_function}")
            
            # CASO NORMAL: Una sola m√©trica
            elif structure.operations and structure.metrics:
                operation = structure.operations[0]
                metric = structure.metrics[0]

                if operation.value == 'm√°ximo':
                    agg_function = self._get_contextual_aggregation_english(structure, metric.text, operation.value)
                else:
                    sql_operations = {
                        'm√≠nimo': f'MIN({metric.text})',
                        'suma': f'SUM({metric.text})',
                        'promedio': f'AVG({metric.text})',
                        'conteo': f'COUNT({metric.text})'
                    }
                    agg_function = sql_operations.get(operation.value, f'SUM({metric.text})')
                
                if agg_function:
                    select_parts.append(agg_function)
                                
        else:
            # üÜï L√ìGICA MEJORADA PARA CONSULTAS CON DIMENSI√ìN PRINCIPAL
            if structure.main_dimension:
                dim_name = structure.main_dimension.text
                formatted_dim = self.format_temporal_dimension(dim_name)
                select_parts.append(formatted_dim)
                group_by_parts.append(dim_name)
                
                # üÜï CR√çTICO: Si hay m√©tricas con GROUP BY, agregarlas al SELECT
                if structure.metrics:
                    print(f"üîß GROUP BY dimension with metrics detected - adding aggregations")
                    
                    for metric in structure.metrics:
                        # Determinar funci√≥n de agregaci√≥n basada en operaciones
                        if structure.operations:
                            operation = structure.operations[0]
                            operation_text = operation.text.lower()
                            operation_value = getattr(operation, 'value', operation_text)
                            
                            print(f"   üìä Processing metric '{metric.text}' with operation '{operation_text}' (value: {operation_value})")
                            
                            if operation_value in ['suma', 'total'] or operation_text in ['total', 'sum']:
                                agg_function = f'SUM({metric.text})'
                            elif operation_value == 'promedio' or operation_text in ['average', 'avg']:
                                agg_function = f'AVG({metric.text})'
                            elif operation_value == 'm√°ximo' or operation_text in ['max', 'maximum']:
                                agg_function = f'MAX({metric.text})'
                            elif operation_value == 'm√≠nimo' or operation_text in ['min', 'minimum']:
                                agg_function = f'MIN({metric.text})'
                            else:
                                agg_function = f'SUM({metric.text})'  # Default para 'total'
                        else:
                            # Sin operaci√≥n expl√≠cita, usar SUM por defecto
                            agg_function = f'SUM({metric.text})'
                            print(f"   üìä No operation found, using default SUM for metric '{metric.text}'")
                        
                        select_parts.append(agg_function)
                        
                        # Agregar ORDER BY para ordenar por la m√©trica
                        order_by_parts.append(f"{agg_function} DESC")
                        
                        print(f"   ‚úÖ Added to GROUP BY query: {agg_function}")
            
            # CONSULTAS COMPUESTAS CON DIMENSI√ìN
            if structure.is_compound_query and structure.compound_criteria:
                print(f"üîó Processing English compound query with {len(structure.compound_criteria)} criteria:")
                
                for i, criteria in enumerate(structure.compound_criteria):
                    operation_value = criteria.operation.value
                    metric_text = criteria.metric.text
                    
                    if operation_value == 'm√°ximo':
                        agg_function = self._get_contextual_aggregation_english(structure, metric_text, operation_value)
                    else:
                        sql_operations = {
                            'm√≠nimo': f'MIN({metric_text})',
                            'suma': f'SUM({metric_text})',
                            'promedio': f'AVG({metric_text})',
                            'conteo': f'COUNT({metric_text})',
                            'total': f'SUM({metric_text})'  # üÜï Agregar mapeo para 'total'
                        }
                        agg_function = sql_operations.get(operation_value, f'SUM({metric_text})')
                    
                    if agg_function:
                        select_parts.append(agg_function)
                        
                        if operation_value in ['m√°ximo', 'mayor']:
                            order_direction = "DESC"
                        elif operation_value in ['m√≠nimo', 'menor']:
                            order_direction = "ASC"
                        else:
                            order_direction = "DESC"
                        
                        order_by_parts.append(f"{agg_function} {order_direction}")
                        
                        print(f"   üîó English Criteria {i+1}: {operation_value} {metric_text} ‚Üí {agg_function} {order_direction}")
                    else:
                        select_parts.append(metric_text)
                        order_by_parts.append(f"{metric_text} DESC")
                        print(f"   üîó English Criteria {i+1}: {metric_text} ‚Üí {metric_text} DESC")
            
            # L√ìGICA TRADICIONAL (una m√©trica) - SOLO SI NO HAY MAIN_DIMENSION CON M√âTRICAS
            elif structure.operations and structure.metrics and not (structure.main_dimension and structure.metrics):
                operation = structure.operations[0]
                metric = structure.metrics[0]
                
                if operation.value == 'm√°ximo':
                    agg_function = self._get_contextual_aggregation_english(structure, metric.text, operation.value)
                else:
                    sql_operations = {
                        'm√≠nimo': f'MIN({metric.text})',
                        'suma': f'SUM({metric.text})',
                        'promedio': f'AVG({metric.text})',
                        'conteo': f'COUNT({metric.text})'
                    }
                    agg_function = sql_operations.get(operation.value, f'SUM({metric.text})')
                
                if agg_function:
                    select_parts.append(agg_function)
                    
                    if structure.query_pattern == QueryPattern.REFERENCED:
                        if operation.value in ['m√°ximo', 'mayor']:
                            order_by_parts.append(f"{agg_function} DESC")
                        elif operation.value in ['m√≠nimo', 'menor']:
                            order_by_parts.append(f"{agg_function} ASC")
                        else:
                            order_by_parts.append(f"{agg_function} DESC")
                    else:
                        order_by_parts.append(f"{agg_function} DESC")
                else:
                    select_parts.append(metric.text)
                    if structure.query_pattern == QueryPattern.REFERENCED:
                        order_by_parts.append(f"{metric.text} DESC")
        
        # WHERE para condiciones de columna (excluyendo temporales duplicadas)
        for condition in structure.column_conditions:
            if condition.column_name not in temporal_columns:
                where_conditions.append(f"{condition.column_name} = '{condition.value}'")
                print(f"   ‚úÖ English WHERE condition: {condition.column_name} = '{condition.value}'")
            else:
                print(f"   ‚è∞ English excluding duplicate temporal condition: {condition.column_name} = '{condition.value}'")
        
        # FILTROS DE EXCLUSI√ìN
        if hasattr(structure, 'exclusion_filters'):
            for exclusion in structure.exclusion_filters:
                if exclusion.exclusion_type == ExclusionType.NOT_EQUALS:
                    where_conditions.append(f"{exclusion.column_name} != '{exclusion.value}'")
                    print(f"   üö´ English exclusion condition: {exclusion.column_name} != '{exclusion.value}'")
        
        # FILTROS TEMPORALES
        advanced_conditions = self.get_advanced_temporal_sql_conditions_english(structure)
        if advanced_conditions:
            where_conditions.extend(advanced_conditions)
            print(f"   ‚úÖ English using temporal filters: {advanced_conditions}")
        
        # CONSTRUCCI√ìN DEL SQL FINAL
        sql_parts = []
        
        if select_parts:
            sql_parts.append(f"SELECT {', '.join(select_parts)}")
        else:
            sql_parts.append("SELECT *")
        
        sql_parts.append(from_clause)
        
        if where_conditions:
            sql_parts.append(f"WHERE {' AND '.join(where_conditions)}")
        
        if group_by_parts:
            sql_parts.append(f"GROUP BY {', '.join(group_by_parts)}")
        
        if order_by_parts:
            sql_parts.append(f"ORDER BY {', '.join(order_by_parts)}")
        
        # LIMITAR LA DATA SEG√öN EL PATR√ìN
        if structure.query_pattern == QueryPattern.REFERENCED:
            sql_parts.append("LIMIT 1")
            print(f"   üéØ English adding LIMIT 1 for REFERENCED pattern")
            
        elif structure.query_pattern == QueryPattern.TOP_N and structure.limit_value:
            sql_parts.append(f"LIMIT {structure.limit_value}")
            print(f"   üèÜ English adding LIMIT {structure.limit_value} for TOP_N pattern")
        
        elif structure.is_ranking_query and structure.ranking_criteria and structure.ranking_criteria.value:
            limit_value = int(structure.ranking_criteria.value)
            sql_parts.append(f"LIMIT {limit_value}")
            print(f"   üèÜ English FORCING LIMIT {limit_value} for ranking (pattern: {structure.query_pattern.value})")
        
        final_sql = " ".join(sql_parts) + ";"
        print(f"   üéØ Final OPTIMIZED English SQL: {final_sql}")
        
        return final_sql

        
    def build_unified_structure_english(self, classified_components: Dict, column_value_pairs: List[ColumnValuePair], 
                            temporal_filters: List[TemporalFilter], tokens: List[str], original_intent: str) -> QueryStructure:
        """üá∫üá∏ CONSTRUCTOR DE ESTRUCTURA UNIFICADA PARA INGL√âS"""
        
        print(f"üèóÔ∏è BUILDING UNIFIED ENGLISH QUERY STRUCTURE")
        
# PASO 1: Detectar patrones complejos PRIMERO
        ranking_criteria = self.detect_ranking_criteria_english(tokens, classified_components)
        exclusion_filters = self.detect_exclusion_filters_english(tokens, classified_components)
        is_ranking = self.is_ranking_query_english(ranking_criteria, exclusion_filters)
        
# PASO 2: Detectar m√∫ltiples dimensiones
        multi_dimensions = self.detect_multi_dimensions_english(tokens, classified_components)
        is_multi_dimension = len(multi_dimensions) >= 2
        
# PASO 3: Solo SI NO es ranking, procesar otros patrones
        if not is_ranking:
            compound_criteria = self.detect_compound_criteria_english(tokens, classified_components)
            is_compound = self.is_compound_query_english(compound_criteria)
        else:
            compound_criteria = []
            is_compound = False
        
# PASO 4: Construir componentes b√°sicos
        main_dimension = None
        operations = []
        metrics = []
        values = []
        connectors = []
        unknown_tokens = []
        
        for token, component in classified_components.items():
            if component.type == ComponentType.DIMENSION and not main_dimension:
                main_dimension = component
                print(f"   üìç English main dimension: {component.text}")
            elif component.type == ComponentType.OPERATION:
                operations.append(component)
                print(f"   ‚ö° English operation: {component.text}")
            elif component.type == ComponentType.METRIC:
                metrics.append(component)
                print(f"   üìä English metric: {component.text}")
            elif component.type == ComponentType.VALUE:
                values.append(component)
            elif component.type == ComponentType.CONNECTOR:
                connectors.append(component)
            elif component.type == ComponentType.UNKNOWN:
                unknown_tokens.append(component)
        
# PASO 5: Construir estructura completa
        structure = QueryStructure(
            main_dimension=main_dimension,
            main_dimensions=multi_dimensions if is_multi_dimension else ([main_dimension] if main_dimension else []),
            is_multi_dimension_query=is_multi_dimension,
            operations=operations,
            metrics=metrics,
            column_conditions=column_value_pairs,
            temporal_filters=temporal_filters,
            values=values,
            connectors=connectors,
            unknown_tokens=unknown_tokens,
            compound_criteria=compound_criteria,
            is_compound_query=is_compound,
            ranking_criteria=ranking_criteria,
            exclusion_filters=exclusion_filters,
            is_ranking_query=is_ranking,
            original_semantic_intent=original_intent
        )
        
# PASO 6: Detectar patr√≥n de consulta
        query_pattern = self.detect_query_pattern_english(structure)
        structure.query_pattern = query_pattern
        
# PASO 7: Configurar l√≠mites seg√∫n el tipo de consulta
        if query_pattern == QueryPattern.TOP_N and structure.ranking_criteria:
            if structure.ranking_criteria.unit == RankingUnit.COUNT:
                structure.limit_value = int(structure.ranking_criteria.value)
            elif structure.ranking_criteria.unit == RankingUnit.PERCENTAGE:
                structure.limit_value = None
            structure.is_single_result = False
            
            print(f"üèÜ ENGLISH RANKING CONFIGURATION:")
            print(f"   üìç Target dimension: {structure.main_dimension.text if structure.main_dimension else 'N/A'}")
            print(f"   üìä Ranking metric: {structure.ranking_criteria.metric.text if structure.ranking_criteria.metric else 'N/A'}")
            print(f"   üéØ Direction: {structure.ranking_criteria.direction.value}")
            print(f"   üìà Unit: {structure.ranking_criteria.unit.value}")
            print(f"   üî¢ Value: {structure.ranking_criteria.value}")
            
        elif query_pattern == QueryPattern.REFERENCED:
            structure.reference_metric = metrics[0] if metrics else None
            structure.is_single_result = True
            structure.limit_value = 1
            
            print(f"üéØ ENGLISH REFERENCED CONFIGURATION:")
            print(f"   üìç Target dimension: {structure.main_dimension.text if structure.main_dimension else 'N/A'}")
            print(f"   üìä Reference metric: {structure.reference_metric.text if structure.reference_metric else 'N/A'}")
        
        print(f"üèóÔ∏è English structure built with {len(operations)} operations, {len(metrics)} metrics")
        print(f"   üéØ Query pattern: {query_pattern.value}")
        print(f"   üèÜ Is ranking: {is_ranking}")
        print(f"   üîó Is compound: {is_compound}")
        print(f"   üîó Is multi-dimensional: {is_multi_dimension}")
        
        return structure
        
    

    def structure_to_dict_english(self, structure: QueryStructure) -> Dict:
        """Convertidor de Estructura a Diccionario - CON SOPORTE SHOW_ROWS"""
        
        # Convertir main_dimension de forma segura
        main_dimension_dict = None
        if structure.main_dimension:
            main_dimension_dict = self.component_to_dict(structure.main_dimension)
        
        result = {
            'main_dimension': main_dimension_dict,
            'operations': [self.component_to_dict(op) for op in structure.operations],
            'metrics': [self.component_to_dict(m) for m in structure.metrics],
            'column_conditions': [self.cvp_to_dict(cvp) for cvp in structure.column_conditions],
            'temporal_filters': [self.temporal_to_dict(tf) for tf in structure.temporal_filters],
            'values': [self.component_to_dict(v) for v in structure.values],
            'connectors': [self.component_to_dict(c) for c in structure.connectors],
            'unknown_tokens': [self.component_to_dict(u) for u in structure.unknown_tokens],
            'complexity_level': structure.get_complexity_level()
        }
        
        # Agregar patrones especiales si existen
        if hasattr(structure, 'show_rows_pattern') and structure.show_rows_pattern:
            result['show_rows_pattern'] = structure.show_rows_pattern
        
        if hasattr(structure, 'list_all_pattern') and structure.list_all_pattern:
            result['list_all_pattern'] = structure.list_all_pattern
            
        return result
                
        
    def generate_hierarchical_structure_english(self, structure: QueryStructure) -> str:
        """üîß Generador de Estructura Jer√°rquica - VERSI√ìN CON SOPORTE SHOW_ROWS"""
        
        # üÜï CASO ESPECIAL: SHOW_ROWS
        if hasattr(structure, 'show_rows_pattern') and structure.show_rows_pattern:
            pattern = structure.show_rows_pattern
            position = pattern.get('position_type', '')
            count = pattern.get('row_count', 0)
            object_type = pattern.get('object_type', 'rows')
            
            if position:
                return f"show {position} {count} {object_type}"
            else:
                return f"show {count} {object_type}"
        
        # CASO ESPECIAL: Rankings - VERSI√ìN MULTI-CRITERIO
        if structure.is_ranking_query and structure.ranking_criteria:
            ranking = structure.ranking_criteria
            direction_text = "top" if ranking.direction == RankingDirection.TOP else "worst"
            
            # Verificar si hay dimensi√≥n principal
            main_dim_text = structure.main_dimension.text if structure.main_dimension else "records"
            
            if ranking.unit == RankingUnit.COUNT:
                result = f"{direction_text} {int(ranking.value)} ({main_dim_text})"
            else:  # PERCENTAGE
                result = f"{direction_text} {ranking.value}% ({main_dim_text})"
            
            # üîß NUEVA L√ìGICA: Incluir m√∫ltiples criterios
            if len(structure.metrics) > 1:
                operations_available = [op.text.lower() for op in structure.operations if op.text.lower() in ['mas', 'm√°s', 'mayor', 'menor']]
                metrics_available = [m.text for m in structure.metrics]
                
                criteria_parts = []
                for i, metric in enumerate(metrics_available):
                    if i < len(operations_available):
                        op = operations_available[i]
                    else:
                        op = operations_available[0] if operations_available else 'mas'
                    
                    criteria_parts.append(f"({op} {metric})")
                
                # Combinar con " y "
                combined_criteria = " y ".join(criteria_parts)
                result += f" por {combined_criteria}"
                
            else:
                # L√ìGICA ORIGINAL: Un criterio
                if ranking.metric:
                    result += f" por ({ranking.metric.text})"
            
            # NUEVA L√ìGICA: Agregar filtros temporales avanzados
            temporal_description = self.generate_hierarchical_structure_temporal_description(structure)
            if temporal_description:
                result += f" {temporal_description}"
            
            # NUEVA L√ìGICA: Agregar filtros de columna si existen
            if structure.column_conditions:
                filter_parts = []
                for condition in structure.column_conditions:
                    filter_parts.append(f"con {condition.column_name} = '{condition.value}'")
                
                if filter_parts:
                    result += f" {' y '.join(filter_parts)}"
            
            # NUEVA L√ìGICA: Agregar exclusiones si existen
            if structure.exclusion_filters:
                exclusion_parts = []
                for exclusion in structure.exclusion_filters:
                    exclusion_parts.append(f"excluyendo {exclusion.column_name} = '{exclusion.value}'")
                
                if exclusion_parts:
                    result += f" {' y '.join(exclusion_parts)}"
            
            print(f"   üèÜ Resultado ranking completo: {result}")
            return result
        
        # RESTO DE LA L√ìGICA ORIGINAL PARA CONSULTAS NO-RANKING
        parts = []
        
        # PASO 1: Identificar columnas temporales
        temporal_columns = set()
        for tf in structure.temporal_filters:
            if tf.unit == TemporalUnit.WEEKS:
                temporal_columns.add('semana')
                temporal_columns.add('week')
            elif tf.unit == TemporalUnit.MONTHS:
                temporal_columns.add('mes')
                temporal_columns.add('month')
            elif tf.unit == TemporalUnit.DAYS:
                temporal_columns.add('dia')
                temporal_columns.add('day')
            elif tf.unit == TemporalUnit.YEARS:
                temporal_columns.add('a√±o')
                temporal_columns.add('year')
        
        print(f"üîç Generando estructura jer√°rquica para consulta compuesta:")
        print(f"   üìç Dimensi√≥n: {structure.main_dimension.text if structure.main_dimension else 'N/A'}")
        print(f"   üîó Es compuesta: {structure.is_compound_query}")
        print(f"   üîó Criterios compuestos: {len(structure.compound_criteria)}")
        print(f"   ‚è∞ Columnas temporales: {temporal_columns}")
        
        # PASO 2: Verificar si dimensi√≥n est√° en filtros
        dimension_in_filter = False
        if structure.main_dimension and structure.column_conditions:
            main_dim_name = structure.main_dimension.text
            for condition in structure.column_conditions:
                if condition.column_name == main_dim_name:
                    dimension_in_filter = True
                    break
        
        print(f"   üîÑ ¬øDimensi√≥n en filtros? {dimension_in_filter}")
        
        # PASO 3: FILTRAR condiciones temporales duplicadas
        non_temporal_conditions = []
        for condition in structure.column_conditions:
            if condition.column_name not in temporal_columns:
                non_temporal_conditions.append(condition)
                print(f"   ‚úÖ Conservando filtro: {condition.column_name} = {condition.value}")
            else:
                print(f"   ‚è∞ EXCLUYENDO filtro temporal duplicado: {condition.column_name} = {condition.value}")
        
        # PASO 4: Construir dimensi√≥n principal
        if structure.main_dimension and not dimension_in_filter:
            main_part = f"({structure.main_dimension.text})"
            
            # CR√çTICO: Solo agregar filtros NO temporales
            if non_temporal_conditions:
                conditions = []
                for condition in non_temporal_conditions:
                    conditions.append(f"({condition.column_name} = '{condition.value}')")
                main_part += f" con {' y '.join(conditions)}"
            
            parts.append(main_part)
            print(f"   ‚úÖ Parte principal: {main_part}")
        
        # PASO 5: Filtros directos (solo NO temporales)
        elif non_temporal_conditions:
            filter_parts = []
            for condition in non_temporal_conditions:
                filter_parts.append(f"({condition.column_name} = '{condition.value}')")
            
            if len(filter_parts) == 1:
                parts.append(filter_parts[0])
            else:
                parts.append(f"({' Y '.join(filter_parts)})")
            
            print(f"   ‚úÖ Filtros directos (no temporales): {filter_parts}")
        
        # PASO 6 NUEVA L√ìGICA: Operaci√≥n y m√©trica COMPUESTA
        if structure.is_compound_query and structure.compound_criteria:
            print(f"üîó PROCESANDO ESTRUCTURA JER√ÅRQUICA COMPUESTA:")
            
            # Construir cada criterio como ((operaci√≥n) (m√©trica))
            criteria_parts = []
            for i, criteria in enumerate(structure.compound_criteria):
                criteria_part = f"(({criteria.operation.text}) ({criteria.metric.text}))"
                criteria_parts.append(criteria_part)
                print(f"   {i+1}. Criterio: {criteria_part}")
            
            # Unir criterios con " y "
            if len(criteria_parts) == 1:
                operation_part = criteria_parts[0]
            else:
                operation_part = " y ".join(criteria_parts)
            
            # NUEVA L√ìGICA: Agregar informaci√≥n temporal avanzada para compuestas
            temporal_description = self.generate_hierarchical_structure_temporal_description(structure)
            if temporal_description:
                operation_part += f" {temporal_description}"
            
            parts.append(operation_part)
            print(f"   ‚úÖ Operaci√≥n compuesta: {operation_part}")
        
        # PASO 6 L√ìGICA TRADICIONAL: Para consultas NO compuestas
        elif structure.operations and structure.metrics:
            op = structure.operations[0]
            metric = structure.metrics[0]
            operation_part = f"(({op.text}) ({metric.text}))"
            
            # NUEVA L√ìGICA: Agregar informaci√≥n temporal avanzada
            temporal_description = self.generate_hierarchical_structure_temporal_description(structure)
            if temporal_description:
                operation_part += f" {temporal_description}"
            
            parts.append(operation_part)
            print(f"   ‚úÖ Operaci√≥n+M√©trica tradicional: {operation_part}")
        
        elif structure.operations:
            op = structure.operations[0]
            parts.append(f"({op.text})")
            print(f"   ‚úÖ Solo operaci√≥n: ({op.text})")
            
        elif structure.metrics:
            # üîß Solo agregar m√©tricas que NO est√°n en filtros
            metrics_not_in_filters = []
            for metric in structure.metrics:
                used_in_filter = any(
                    cvp.column_name == metric.text 
                    for cvp in structure.column_conditions
                )
                if not used_in_filter:
                    metrics_not_in_filters.append(metric)
            
            if metrics_not_in_filters:
                metric = metrics_not_in_filters[0]
                parts.append(f"({metric.text})")
        
        # PASO 7: Combinar partes con l√≥gica correcta
        if len(parts) == 1:
            result = parts[0]
        elif len(parts) == 2:
            # Verificar si TODAS las condiciones son temporales
            all_conditions_are_temporal = all(
                condition.column_name in temporal_columns 
                for condition in structure.column_conditions
            )
            
            if all_conditions_are_temporal and structure.main_dimension:
                # Caso: dimensi√≥n + operaci√≥n temporal (sin filtros adicionales)
                result = f"{parts[0]} con {parts[1]}"
                print(f"   üîß Combinaci√≥n especial (dimensi√≥n con operaci√≥n temporal): {result}")
            else:
                # Caso: m√∫ltiples condiciones independientes
                result = f"{' Y '.join(parts)}"
                print(f"   üîß Combinaci√≥n est√°ndar (m√∫ltiples condiciones): {result}")
        elif len(parts) > 2:
            result = f"{' Y '.join(parts)}"
        else:
            result = "estructura_incompleta"
        
        print(f"   üéØ Resultado final COMPUESTO: {result}")
        return result
        
    

    def calculate_overall_confidence_english(self, structure: QueryStructure) -> float:
        """Calculador de Confianza General"""
        all_components = []
        
        if structure.main_dimension:
            all_components.append(structure.main_dimension)
        
        all_components.extend(structure.operations)
        all_components.extend(structure.metrics)
        all_components.extend(structure.values)
        all_components.extend(structure.connectors)
        all_components.extend(structure.unknown_tokens)
        
        # Agregar confianza de condiciones de columna
        for condition in structure.column_conditions:
            all_components.append(QueryComponent("dummy", ComponentType.COLUMN_VALUE, condition.confidence))
        
        # Agregar confianza de filtros temporales
        for tf in structure.temporal_filters:
            all_components.append(QueryComponent("dummy", ComponentType.TEMPORAL, tf.confidence))
        
        if not all_components:
            return 0.0
        
        # Calcular promedio ponderado
        total_confidence = sum(comp.confidence for comp in all_components)
        return round(total_confidence / len(all_components), 2)



# ------  "Convertidor de componente a diccionario" -------

    def component_to_dict(self, component: QueryComponent) -> Dict:
        """Convertidor de Componente a Diccionario"""
        if not component:
            return None
        
        return {
            'text': component.text,
            'type': component.type.value,
            'confidence': component.confidence,
            'subtype': component.subtype,
            'value': component.value,
            'column_name': component.column_name,
            'linguistic_info': component.linguistic_info
        }



    def generate_hierarchical_structure_temporal_description(self, structure: QueryStructure) -> str:
        """Genera descripci√≥n temporal avanzada para estructura jer√°rquica"""
        temporal_parts = []
        
        # NUEVA L√ìGICA: Usar informaci√≥n temporal avanzada si est√° disponible
        if hasattr(self, 'advanced_temporal_info') and self.advanced_temporal_info:
            for advanced_info in self.advanced_temporal_info:
                # üîß FIX: Validar que los valores existen antes de usarlos
                try:
                    if hasattr(advanced_info, 'is_range_between') and advanced_info.is_range_between:
                        start_value = getattr(advanced_info, 'start_value', None)
                        end_value = getattr(advanced_info, 'end_value', None)
                        
                        if start_value is not None and end_value is not None:
                            if advanced_info.original_filter.unit == TemporalUnit.WEEKS:
                                temporal_parts.append(f"de semana {start_value} a {end_value}")
                            elif advanced_info.original_filter.unit == TemporalUnit.MONTHS:
                                temporal_parts.append(f"de mes {start_value} a {end_value}")
                            elif advanced_info.original_filter.unit == TemporalUnit.DAYS:
                                temporal_parts.append(f"de d√≠a {start_value} a {end_value}")
                except Exception as e:
                    print(f"‚ö†Ô∏è Error processing advanced_temporal_info: {e}")
                    continue
        
        # NUEVA L√ìGICA: Usar informaci√≥n temporal avanzada si est√° disponible
        if hasattr(self, 'advanced_temporal_info') and self.advanced_temporal_info:
            for advanced_info in self.advanced_temporal_info:
                if advanced_info.is_range_from:
                    if advanced_info.original_filter.unit == TemporalUnit.WEEKS:
                        temporal_parts.append(f"desde semana {advanced_info.start_value}")
                    elif advanced_info.original_filter.unit == TemporalUnit.MONTHS:
                        temporal_parts.append(f"desde mes {advanced_info.start_value}")
                    elif advanced_info.original_filter.unit == TemporalUnit.DAYS:
                        temporal_parts.append(f"desde d√≠a {advanced_info.start_value}")
                elif advanced_info.is_range_between:
                    if advanced_info.original_filter.unit == TemporalUnit.WEEKS:
                        temporal_parts.append(f"de semana {advanced_info.start_value} a {advanced_info.end_value}")
                    elif advanced_info.original_filter.unit == TemporalUnit.MONTHS:
                        temporal_parts.append(f"de mes {advanced_info.start_value} a {advanced_info.end_value}")
                    elif advanced_info.original_filter.unit == TemporalUnit.DAYS:
                        temporal_parts.append(f"de d√≠a {advanced_info.start_value} a {advanced_info.end_value}")
                elif advanced_info.is_range_to:
                    if advanced_info.original_filter.unit == TemporalUnit.WEEKS:
                        temporal_parts.append(f"hasta semana {advanced_info.end_value}")
                    elif advanced_info.original_filter.unit == TemporalUnit.MONTHS:
                        temporal_parts.append(f"hasta mes {advanced_info.end_value}")
                    elif advanced_info.original_filter.unit == TemporalUnit.DAYS:
                        temporal_parts.append(f"hasta d√≠a {advanced_info.end_value}")
                else:
                    # Filtros tradicionales existentes
                    tf = advanced_info.original_filter
                    if tf.filter_type == "specific":
                        if tf.unit == TemporalUnit.WEEKS:
                            temporal_parts.append(f"en semana {tf.quantity}")
                        elif tf.unit == TemporalUnit.MONTHS:
                            temporal_parts.append(f"en mes {tf.quantity}")
                        elif tf.unit == TemporalUnit.DAYS:
                            temporal_parts.append(f"en d√≠a {tf.quantity}")
                    else:
                        temporal_parts.append(f"en las {tf.indicator} {tf.quantity} {tf.unit.value}")
        else:
            # FALLBACK: Usar filtros temporales tradicionales (para compatibilidad)
            for tf in structure.temporal_filters:
                if tf.filter_type == "specific":
                    if tf.unit == TemporalUnit.WEEKS:
                        temporal_parts.append(f"en semana {tf.quantity}")
                    elif tf.unit == TemporalUnit.MONTHS:
                        temporal_parts.append(f"en mes {tf.quantity}")
                    elif tf.unit == TemporalUnit.DAYS:
                        temporal_parts.append(f"en d√≠a {tf.quantity}")
                else:
                    temporal_parts.append(f"en las {tf.indicator} {tf.quantity} {tf.unit.value}")
        
        return ' y '.join(temporal_parts) if temporal_parts else ""


    def cvp_to_dict(self, cvp: ColumnValuePair) -> Dict:
        """Convertidor de Par Columna-Valor"""
        return {
            'column_name': cvp.column_name,
            'value': cvp.value,
            'confidence': cvp.confidence,
            'raw_text': cvp.raw_text
        }



# EN temporal_to_dict (alrededor de l√≠nea 3290)
    def temporal_to_dict(self, tf: TemporalFilter) -> Dict:
        """Convertidor de Filtro Temporal"""
        return {
            'indicator': tf.indicator,
            'quantity': tf.quantity,
            'unit': tf.unit.value,
            'confidence': tf.confidence,
            'filter_type': tf.filter_type,
            # AGREGAR ESTOS CAMPOS:
            'start_value': getattr(tf, 'start_value', None),
            'end_value': getattr(tf, 'end_value', None)
        }


# ------  "Inferidor de dimension por defecto" -------

    def infer_default_dimension_for_ranking(self, ranking_criteria: RankingCriteria) -> Optional[QueryComponent]:
        """Inferidor de Dimensi√≥n por Defecto"""
        # Dimensiones comunes por m√©trica
        metric_to_dimension = {
            'ventas': 'account',
            'venta': 'account', 
            'inventario': 'product',
            'margen': 'product',
            'revenue': 'account',
            'sales': 'account'
        }
        
        if ranking_criteria and ranking_criteria.metric:
            metric_text = ranking_criteria.metric.text.lower()
            if metric_text in metric_to_dimension:
                inferred_dim = metric_to_dimension[metric_text]
                
                return QueryComponent(
                    text=inferred_dim,
                    type=ComponentType.DIMENSION,
                    confidence=0.75,  # Confianza media por ser inferida
                    subtype='inferred',
                    linguistic_info={'source': 'inferred_for_ranking'}
                )
        
        return None



    def _is_potential_metric_english(self, token: str) -> bool:
        """Detecta si un token es potencialmente una m√©trica"""
        token_lower = token.lower()
        
        # M√©tricas en diccionario
        if token_lower in self.dictionaries.metricas:
            return True
        
        # M√©tricas comunes en ingl√©s
        common_metrics = {
            'sales', 'revenue', 'profit', 'margin', 'cost', 'price', 
            'inventory', 'stock', 'amount', 'value', 'total', 'count',
            'volume', 'quantity', 'units', 'dollars', 'euros'
        }
        
        if token_lower in common_metrics:
            return True
        
        # Plurales de m√©tricas
        if token_lower.endswith('s') and token_lower[:-1] in common_metrics:
            return True
        
        return False


    def detect_list_all_pattern_english(self, tokens: List[str]) -> Optional[Dict]:
        """üìã DETECTOR DE PATR√ìN LIST ALL EN INGL√âS - MEJORADO PARA TEMPORALES"""
        print(f"üìã DETECTING LIST ALL PATTERN:")
        print(f"   üî§ Tokens: {tokens}")
        
        # üÜï CASO ESPECIAL: Dimensi√≥n temporal sola o casi sola
        temporal_plurals = {'weeks', 'months', 'days', 'years', 'quarters'}
        
        # Si el primer token es una dimensi√≥n temporal plural
        if len(tokens) > 0 and tokens[0].lower() in temporal_plurals:
            # Verificar si es una consulta simple (solo la dimensi√≥n o con filtros simples)
            is_simple_temporal_query = False
            
            # Caso 1: Solo la dimensi√≥n temporal ("weeks")
            if len(tokens) == 1:
                is_simple_temporal_query = True
            
            # Caso 2: Dimensi√≥n temporal con filtros simples ("weeks of liverpool")
            elif len(tokens) <= 4 and 'where' not in [t.lower() for t in tokens]:
                # No debe tener verbos de agregaci√≥n ni "where"
                aggregation_verbs = {'sum', 'total', 'average', 'count', 'max', 'min'}
                has_aggregation = any(t.lower() in aggregation_verbs for t in tokens)
                
                if not has_aggregation:
                    is_simple_temporal_query = True
            
            if is_simple_temporal_query:
                # Normalizar plural a singular para la dimensi√≥n objetivo
                temporal_singular_map = {
                    'weeks': 'week',
                    'months': 'month',
                    'days': 'day',
                    'years': 'year',
                    'quarters': 'quarter'
                }
                
                target_dimension = temporal_singular_map.get(tokens[0].lower(), tokens[0].lower())
                
                pattern_result = {
                    'pattern_type': 'LIST_ALL',
                    'list_indicator': 'implicit',  # No hay indicador expl√≠cito
                    'has_all_indicator': True,
                    'all_indicator': 'all',
                    'target_dimension': target_dimension,
                    'has_aggregation': False,
                    'confidence': 0.95,
                    'raw_tokens': tokens,
                    'is_temporal_list': True  # üÜï Marcador especial
                }
                
                print(f"üìã TEMPORAL LIST PATTERN DETECTED:")
                print(f"   üìç Target dimension: {target_dimension}")
                print(f"   ‚è∞ Is temporal list: True")
                print(f"   ‚≠ê Confidence: {pattern_result['confidence']:.2f}")
                
                return pattern_result
        
        # CONTINUAR CON LA DETECCI√ìN NORMAL...
        if len(tokens) < 2:  # M√≠nimo: list items
            return None
        
        # STEP 1: Buscar indicadores de "list"
        list_indicators = {'list', 'show', 'display', 'get', 'find', 'give', 'tell'}
        list_start_pos = -1
        
        for i, token in enumerate(tokens):
            if token.lower() in list_indicators:
                list_start_pos = i
                print(f"   ‚úÖ List indicator: '{token}' at position {i}")
                break
        
        if list_start_pos == -1:
            print(f"   ‚ùå No list indicator found")
            return None
        
        # STEP 2: Buscar "all" (opcional pero com√∫n)
        all_indicators = {'all', 'every', 'each'}
        has_all_indicator = False
        all_pos = -1
        
        for i in range(list_start_pos + 1, min(list_start_pos + 3, len(tokens))):
            if i < len(tokens) and tokens[i].lower() in all_indicators:
                has_all_indicator = True
                all_pos = i
                print(f"   ‚úÖ All indicator: '{tokens[i]}' at position {i}")
                break
        
        # STEP 3: Buscar dimensi√≥n objetivo
        target_dimension = None
        dimension_pos = -1
        search_start = all_pos + 1 if has_all_indicator else list_start_pos + 1
        
        for i in range(search_start, len(tokens)):
            if i >= len(tokens):
                break
            
            token = tokens[i]
            if self._is_potential_dimension_english(token):
                target_dimension = token.lower()
                dimension_pos = i
                print(f"   ‚úÖ Target dimension: '{target_dimension}'")
                break
        
        if not target_dimension:
            print(f"   ‚ùå No target dimension found")
            return None
        
        # STEP 4: DETECTAR SI HAY M√âTRICAS/AGREGACIONES DESPU√âS
        has_aggregation = False
        aggregation_keywords = {'and', 'with', 'their', 'including'}
        metric_keywords = {
            'total', 'sum', 'average', 'count', 'sales', 'revenue', 
            'inventory', 'profit', 'cost', 'amount', 'quantity'
        }
        
        # Buscar indicadores de agregaci√≥n despu√©s de la dimensi√≥n
        for i in range(dimension_pos + 1, len(tokens)):
            if tokens[i].lower() in aggregation_keywords:
                # Verificar si hay m√©tricas despu√©s
                for j in range(i + 1, len(tokens)):
                    if tokens[j].lower() in metric_keywords:
                        has_aggregation = True
                        print(f"   ‚úÖ Aggregation detected: '{tokens[i]}' ... '{tokens[j]}'")
                        break
                if has_aggregation:
                    break
        
        # STEP 5: Calcular confianza
        confidence = 0.7  # Base
        confidence += 0.2  # Has list indicator
        if has_all_indicator:
            confidence += 0.1  # Has "all"
        
        # STEP 6: Construir resultado
        pattern_result = {
            'pattern_type': 'LIST_ALL',
            'list_indicator': tokens[list_start_pos].lower(),
            'has_all_indicator': has_all_indicator,
            'all_indicator': tokens[all_pos].lower() if has_all_indicator else None,
            'target_dimension': target_dimension,
            'has_aggregation': has_aggregation,
            'confidence': min(1.0, confidence),
            'raw_tokens': tokens,
            'is_temporal_list': False  # Normal list, no temporal
        }
        
        print(f"üìã LIST ALL PATTERN DETECTED:")
        print(f"   üìã List indicator: {pattern_result['list_indicator']}")
        print(f"   üåê Has 'all': {pattern_result['has_all_indicator']}")
        print(f"   üìç Target dimension: {pattern_result['target_dimension']}")
        print(f"   üìä Has aggregation: {pattern_result['has_aggregation']}")
        print(f"   ‚≠ê Confidence: {pattern_result['confidence']:.2f}")
        
        return pattern_result


    def _is_potential_dimension_english(self, token: str) -> bool:
        """üìç VERIFICADOR DE DIMENSIONES PARA LIST ALL"""
        token_lower = token.lower()
        
        # PRIORIDAD 1: Dimensiones en diccionario
        if token_lower in self.dictionaries.dimensiones:
            return True
        
        # PRIORIDAD 2: Dimensiones comunes en ingl√©s (plurales)
        common_dimensions = {
            'items', 'products', 'customers', 'stores', 'accounts', 'partners',
            'orders', 'users', 'clients', 'vendors', 'suppliers', 'categories',
            'regions', 'countries', 'cities', 'brands', 'models', 'types'
        }
        
        if token_lower in common_dimensions:
            return True
        
        # PRIORIDAD 3: Versiones singulares
        singular_dimensions = {
            'item', 'product', 'customer', 'store', 'account', 'partner',
            'order', 'user', 'client', 'vendor', 'supplier',
            'region', 'country', 'city', 'brand', 'model', 'type'
        }
        
        if token_lower in singular_dimensions:
            return True
        
        # PRIORIDAD 4: Snake_case dimensions
        if '_' in token_lower and len(token_lower) > 3:
            return True
        
        return False


    def generate_list_all_sql_english(self, pattern_data: Dict) -> str:
        """üìã GENERADOR SQL PARA PATR√ìN LIST ALL"""
        print(f"üìã GENERATING LIST ALL SQL:")
        
        target_dimension = pattern_data['target_dimension']
        list_indicator = pattern_data['list_indicator']
        
        print(f"   üìã List type: {list_indicator}")
        print(f"   üìç Target dimension: {target_dimension}")
        
        # Construir SQL simple
        sql_parts = [
            f"SELECT DISTINCT {target_dimension}",
            "FROM datos",
            f"ORDER BY {target_dimension}"
        ]
        
        final_sql = " ".join(sql_parts) + ";"
        
        print(f"   üéØ List all SQL: {final_sql}")
        return final_sql


    def detect_show_rows_pattern_english(self, tokens: List[str]) -> Optional[Dict]:
        """üìä DETECTOR DE PATR√ìN SHOW ROWS EN INGL√âS"""
        print(f"üìä DETECTING SHOW ROWS PATTERN:")
        print(f"   üî§ Tokens: {tokens}")
        
        if len(tokens) < 2:  # M√≠nimo: show rows
            return None
        
# STEP 1: Buscar indicadores de "show/display"
        show_indicators = {'show', 'display', 'get', 'fetch', 'list', 'give', 'return'}
        show_start_pos = -1
        
        for i, token in enumerate(tokens):
            if token.lower() in show_indicators:
                show_start_pos = i
                print(f"   ‚úÖ Show indicator: '{token}' at position {i}")
                break
        
        if show_start_pos == -1:
            print(f"   ‚ùå No show indicator found")
            return None
        
# STEP 2: Buscar indicadores de posici√≥n (opcional)
        position_indicators = {'first', 'last', 'top', 'bottom', 'initial', 'final'}
        position_type = None
        position_pos = -1
        
        # Buscar posici√≥n despu√©s del indicador de show
        for i in range(show_start_pos + 1, min(show_start_pos + 3, len(tokens))):
            if i < len(tokens) and tokens[i].lower() in position_indicators:
                position_type = tokens[i].lower()
                position_pos = i
                print(f"   ‚úÖ Position indicator: '{tokens[i]}' at position {i}")
                break
        
# STEP 3: Buscar n√∫mero de filas
        row_count = None
        number_pos = -1
        search_start = position_pos + 1 if position_pos != -1 else show_start_pos + 1
        
        for i in range(search_start, min(search_start + 3, len(tokens))):
            if i >= len(tokens):
                break
            
            token = tokens[i]
            
            # N√∫mero directo
            if token.isdigit():
                row_count = int(token)
                number_pos = i
                print(f"   ‚úÖ Row count (number): {row_count}")
                break
            
            # N√∫meros en palabras en ingl√©s
            elif token.lower() in self.dictionaries.numeros_palabras_en:
                row_count = self.dictionaries.numeros_palabras_en[token.lower()]
                number_pos = i
                print(f"   ‚úÖ Row count (word): '{token}' = {row_count}")
                break
        
        if row_count is None:
            print(f"   ‚ùå No row count found")
            return None
        
# STEP 4: Buscar indicador de objeto (rows, records, entries)
        object_indicators = {'rows', 'row', 'records', 'record', 'entries', 'entry', 'lines', 'line', 'items', 'item'}
        object_type = None
        
        search_start = number_pos + 1
        for i in range(search_start, min(search_start + 2, len(tokens))):
            if i < len(tokens) and tokens[i].lower() in object_indicators:
                object_type = tokens[i].lower()
                print(f"   ‚úÖ Object type: '{object_type}'")
                break
        
        # Si no encuentra objeto espec√≠fico pero los otros componentes est√°n, asumir "rows"
        if object_type is None:
            object_type = 'rows'
            print(f"   ‚úÖ Object type (default): 'rows'")
        
# STEP 5: Calcular confianza
        confidence = 0.7  # Base
        confidence += 0.2  # Has show indicator
        if position_type:
            confidence += 0.1  # Has position
        if object_type in object_indicators:
            confidence += 0.1  # Has valid object type
        
# STEP 6: Construir resultado
        pattern_result = {
            'pattern_type': 'SHOW_ROWS',
            'show_indicator': tokens[show_start_pos].lower(),
            'position_type': position_type,
            'row_count': row_count,
            'object_type': object_type,
            'confidence': min(1.0, confidence),
            'raw_tokens': tokens
        }
        
        print(f"üìä SHOW ROWS PATTERN DETECTED:")
        print(f"   üìä Show indicator: {pattern_result['show_indicator']}")
        print(f"   üìç Position: {pattern_result['position_type']}")
        print(f"   üî¢ Row count: {pattern_result['row_count']}")
        print(f"   üìã Object type: {pattern_result['object_type']}")
        print(f"   ‚≠ê Confidence: {pattern_result['confidence']:.2f}")
        
        return pattern_result


    def generate_show_rows_sql_english(self, pattern_data: Dict) -> str:
        """üìä GENERADOR SQL PARA PATR√ìN SHOW ROWS"""
        print(f"üìä GENERATING SHOW ROWS SQL:")
        
        show_indicator = pattern_data['show_indicator']
        position_type = pattern_data.get('position_type')
        row_count = pattern_data['row_count']
        object_type = pattern_data['object_type']
        
        print(f"   üìä Show type: {show_indicator}")
        print(f"   üìç Position: {position_type}")
        print(f"   üî¢ Count: {row_count}")
        print(f"   üìã Object: {object_type}")
        
        # Construir SQL base
        select_part = "SELECT *"
        from_part = "FROM datos"
        
        # Determinar ORDER BY seg√∫n la posici√≥n
        if position_type in ['last', 'bottom', 'final']:
            # Para √∫ltimas filas, necesitamos ordenar descendente
            # Nota: esto depende de tener una columna de ID o timestamp
            # Por simplicidad, usamos ROWID (disponible en SQLite)
            order_part = "ORDER BY id DESC"
            print(f"   üîÑ Using descending order for '{position_type}' rows")
        else:
            # Para primeras filas o sin posici√≥n espec√≠fica
            order_part = "ORDER BY id ASC"
            print(f"   üîÑ Using ascending order for '{position_type or 'default'}' rows")
        
        limit_part = f"LIMIT {row_count}"
        
        # Construir SQL final
        sql_parts = [select_part, from_part, order_part, limit_part]
        final_sql = " ".join(sql_parts) + ";"
        
        print(f"   üéØ Show rows SQL: {final_sql}")
        return final_sql


    def generate_multi_metric_sql_direct(self, pattern: MultiMetricPattern, normalized_query: str, original_query: str) -> Dict:
        """
        üìä GENERADOR DIRECTO DE SQL PARA MULTI-M√âTRICA
        Genera el resultado completo sin pasar por toda la pipeline
        """
        
        print(f"üìä GENERATING DIRECT MULTI-METRIC SQL:")
        
        # Construir SELECT
        select_parts = []
        
        # Si hay dimensi√≥n, agregarla primero
        if pattern.dimension:
            select_parts.append(pattern.dimension)
        
        # Agregar cada m√©trica con su operaci√≥n
        for i, metric in enumerate(pattern.metrics):
            if i < len(pattern.operations):
                op = pattern.operations[i]
            else:
                op = pattern.operations[0] if pattern.operations else 'total'
            
            # Mapear operaci√≥n a SQL
            if op in ['total', 'sum']:
                sql_func = f"SUM({metric})"
            elif op in ['average', 'avg']:
                sql_func = f"AVG({metric})"
            elif op == 'max':
                sql_func = f"MAX({metric})"
            elif op == 'min':
                sql_func = f"MIN({metric})"
            elif op == 'count':
                sql_func = f"COUNT({metric})"
            else:
                sql_func = f"SUM({metric})"
            
            alias = f"{op}_{metric}"
            select_parts.append(f"{sql_func} as {alias}")
            print(f"   ‚úÖ Added: {sql_func} as {alias}")
        
        # Construir WHERE
        where_conditions = []
        for filter_item in pattern.filters:
            condition = f"{filter_item['column']} = '{filter_item['value']}'"
            where_conditions.append(condition)
            print(f"   üîç Filter: {condition}")
        
        # Construir SQL
        sql_parts = [f"SELECT {', '.join(select_parts)}", "FROM datos"]
        
        if where_conditions:
            sql_parts.append(f"WHERE {' AND '.join(where_conditions)}")
        
        if pattern.dimension:
            sql_parts.append(f"GROUP BY {pattern.dimension}")
        
        final_sql = " ".join(sql_parts) + ";"
        
        print(f"   üéØ Final SQL: {final_sql}")
        
        # Retornar resultado completo
        return {
            'success': True,
            'language': 'english',
            'original_input': original_query,
            'normalized_query': normalized_query,
            'tokens': pattern.raw_tokens,
            'conceptual_sql': final_sql,
            'sql_query': final_sql,
            'complexity_level': 'multi_metric',
            'processing_method': 'multi_metric_direct',
            'note': 'üìä Processed with Multi-Metric Pattern',
            'query_structure': {
                'pattern': 'MULTI_METRIC',
                'metrics': pattern.metrics,
                'operations': pattern.operations,
                'dimension': pattern.dimension,
                'filters': pattern.filters
            },
            'confidence': pattern.confidence
        }
        

    def detect_this_week_pattern_english(self, tokens: List[str]) -> Optional[ThisWeekPattern]:
        """
        üìÖ DETECTOR DE PATR√ìN 'THIS WEEK'
        Detecta: "store with more sales this week"
        """
        
        print(f"üìÖ DETECTING THIS WEEK PATTERN:")
        print(f"   üì§ Tokens: {tokens}")
        
        # Buscar "this week" en los tokens
        this_week_patterns = [
            ['this', 'week'],
            ['this_week'],
            ['thisweek']
        ]
        
        for pattern in this_week_patterns:
            pattern_length = len(pattern)
            
            for i in range(len(tokens) - pattern_length + 1):
                # Verificar si los tokens coinciden con el patr√≥n
                match = True
                for j, pattern_token in enumerate(pattern):
                    if tokens[i + j].lower() != pattern_token.lower():
                        match = False
                        break
                
                if match:
                    indicator_text = ' '.join(tokens[i:i + pattern_length])
                    
                    this_week_pattern = ThisWeekPattern(
                        indicator_text=indicator_text,
                        position_start=i,
                        position_end=i + pattern_length - 1,
                        confidence=0.95,
                        raw_tokens=tokens[i:i + pattern_length]
                    )
                    
                    print(f"üìÖ THIS WEEK PATTERN DETECTED:")
                    print(f"   üìÖ Text: '{indicator_text}'")
                    print(f"   üìç Positions: {i}-{i + pattern_length - 1}")
                    print(f"   ‚≠ê Confidence: {this_week_pattern.confidence:.2f}")
                    
                    return this_week_pattern
        
        print(f"   ‚ùå No 'this week' pattern found")
        return None


    def detect_multi_metric_pattern_english(self, tokens: List[str]) -> Optional[MultiMetricPattern]:
        """
        üìä DETECTOR DE PATR√ìN MULTI-M√âTRICA EN INGL√âS
        Detecta consultas con m√∫ltiples m√©tricas
        """
        
        print(f"üìä DETECTING MULTI-METRIC PATTERN:")
        print(f"   üî§ Tokens: {tokens}")
        
        # M√©tricas conocidas
        known_metrics = {
            'sales', 'inventory', 'profit', 'revenue', 'margin', 
            'cost', 'stock', 'units', 'amount', 'quantity', 'volume'
        }
        
        # Operaciones conocidas
        known_operations = {
            'total', 'sum', 'average', 'avg', 'max', 'min', 'count'
        }
        
        # STEP 1: Buscar m√©tricas en los tokens
        found_metrics = []
        metric_positions = []
        
        for i, token in enumerate(tokens):
            token_lower = token.lower()
            if token_lower in known_metrics:
                found_metrics.append(token_lower)
                metric_positions.append(i)
                print(f"   üìä Metric found: '{token_lower}' at position {i}")
        
        # Necesitamos al menos 2 m√©tricas
        if len(found_metrics) < 2:
            print(f"   ‚ùå Not enough metrics (found {len(found_metrics)})")
            return None
        
        # STEP 2: Buscar conectores
        has_and = 'and' in [t.lower() for t in tokens]
        has_comma = any(',' in t for t in tokens)
        
        if not (has_and or has_comma):
            print(f"   ‚ùå No connectors found between metrics")
            return None
        
        print(f"   ‚úÖ Found {len(found_metrics)} metrics with connectors")
        
        # STEP 3: Buscar operaciones
        found_operations = []
        for token in tokens:
            if token.lower() in known_operations:
                found_operations.append(token.lower())
                print(f"   ‚ö° Operation found: '{token.lower()}'")
        
        # Si no hay operaciones, usar 'total' por defecto
        if not found_operations:
            found_operations = ['total'] * len(found_metrics)
            print(f"   ‚ö° Using default operation: 'total'")
        
        # STEP 4: Buscar dimensi√≥n
        dimension = None
        dimension_keywords = ['store', 'account', 'item', 'product', 'customer', 'week', 'month']
        for token in tokens:
            if token.lower() in dimension_keywords:
                dimension = token.lower()
                print(f"   üìç Dimension found: '{dimension}'")
                break
        
        # STEP 5: Buscar filtros simples (of X, in X)
        filters = []
        for i in range(len(tokens) - 1):
            if tokens[i].lower() in ['of', 'in', 'for']:
                next_token = tokens[i + 1]
                # Verificar si el siguiente token es un valor (may√∫sculas o alfanum√©rico)
                if next_token.isupper() or (next_token.isalnum() and not next_token.islower()):
                    filters.append({
                        'type': 'simple',
                        'column': 'account',  # Por defecto
                        'value': next_token.upper()
                    })
                    print(f"   üîç Filter found: {next_token.upper()}")
        
        # STEP 6: Calcular confianza
        confidence = 0.7  # Base
        confidence += min(len(found_metrics) * 0.05, 0.15)
        if has_and or has_comma:
            confidence += 0.1
        if dimension:
            confidence += 0.05
        
        # Crear el patr√≥n
        pattern = MultiMetricPattern(
            metrics=found_metrics,
            operations=found_operations,
            has_dimension=dimension is not None,
            dimension=dimension,
            has_filters=len(filters) > 0,
            filters=filters,
            confidence=min(1.0, confidence),
            raw_tokens=tokens
        )
        
        print(f"üìä MULTI-METRIC PATTERN DETECTED:")
        print(f"   üìä Metrics: {found_metrics}")
        print(f"   ‚ö° Operations: {found_operations}")
        print(f"   üìç Dimension: {dimension}")
        print(f"   üîç Filters: {len(filters)}")
        print(f"   ‚≠ê Confidence: {pattern.confidence:.2f}")
        
        return pattern


    def detect_stock_out_pattern_english(self, tokens: List[str]) -> Optional[YNColumnPattern]:
        """
        üì¶ DETECTOR DE PATR√ìN 'IN STOCK OUT'
        Detecta: "which products are in stock out" vs "which products are not in stock out"
        """
        
        print(f"üì¶ DETECTING STOCK OUT PATTERN:")
        print(f"   üì§ Tokens: {tokens}")
        
        # Buscar patrones de stock out
        stock_out_patterns = [
            # Patrones afirmativos (Stock_Out = 'Y')
            (['in', 'stock', 'out'], True),
            (['in_stock_out'], True),
            (['instockout'], True),
            
            # Patrones negativos (Stock_Out = 'N')
            (['not', 'in', 'stock', 'out'], False),
            (['not_in_stock_out'], False),
            (['aren\'t', 'in', 'stock', 'out'], False),
            (['arent', 'in', 'stock', 'out'], False),
            (['are', 'not', 'in', 'stock', 'out'], False),
        ]
        
        for pattern_tokens, is_positive in stock_out_patterns:
            pattern_length = len(pattern_tokens)
            
            for i in range(len(tokens) - pattern_length + 1):
                # Verificar si los tokens coinciden con el patr√≥n
                match = True
                for j, pattern_token in enumerate(pattern_tokens):
                    if tokens[i + j].lower() != pattern_token.lower():
                        match = False
                        break
                
                if match:
                    indicator_text = ' '.join(tokens[i:i + pattern_length])
                    negation_detected = not is_positive
                    
                    stock_out_pattern = YNColumnPattern(
                        is_in_stock_out=is_positive,
                        negation_detected=negation_detected,
                        indicator_text=indicator_text,
                        position_start=i,
                        position_end=i + pattern_length - 1,
                        confidence=0.95,
                        raw_tokens=tokens[i:i + pattern_length]
                    )
                    
                    print(f"üì¶ STOCK OUT PATTERN DETECTED:")
                    print(f"   üì¶ Text: '{indicator_text}'")
                    print(f"   ‚úÖ Is in stock out: {is_positive}")
                    print(f"   üö´ Negation detected: {negation_detected}")
                    print(f"   üìç Positions: {i}-{i + pattern_length - 1}")
                    print(f"   ‚≠ê Confidence: {stock_out_pattern.confidence:.2f}")
                    print(f"   üéØ SQL Value: Stock_Out = {'Y' if is_positive else 'N'}")
                    
                    return stock_out_pattern
        
        print(f"   ‚ùå No stock out pattern found")
        return None


    def generate_this_week_sql_condition(self) -> str:
        """
        üìÖ GENERADOR DE CONDICI√ìN SQL PARA 'THIS WEEK'
        Genera: WHERE week = (SELECT MAX(week) FROM datos)
        """
        
        print(f"üìÖ GENERATING THIS WEEK SQL CONDITION:")
        
        # Usar subconsulta para obtener la semana m√°xima
        condition = "week = (SELECT MAX(week) FROM datos)"
        
        print(f"   üìÖ This week condition: {condition}")
        return condition


    def generate_stock_out_sql_condition(self, pattern: YNColumnPattern) -> str:
        """
        üì¶ GENERADOR DE CONDICI√ìN SQL PARA STOCK OUT
        """
        
        print(f"üì¶ GENERATING STOCK OUT SQL CONDITION:")
        print(f"   üì¶ Is in stock out: {pattern.is_in_stock_out}")
        
        if pattern.is_in_stock_out:
            # "in stock out" ‚Üí Stock_Out = 'Y'
            condition = "Stock_Out = 'Y'"
        else:
            # "not in stock out" ‚Üí Stock_Out = 'N'  
            condition = "Stock_Out = 'N'"
        
        print(f"   üì¶ Stock out condition: {condition}")
        return condition


    def _is_potential_row_object_english(self, token: str) -> bool:
        """üìã VERIFICADOR DE OBJETOS DE FILA PARA SHOW ROWS"""
        token_lower = token.lower()
        
        # Objetos que representan filas/registros
        row_objects = {
            'rows', 'row', 'records', 'record', 'entries', 'entry', 
            'lines', 'line', 'items', 'item', 'data', 'results'
        }
        
        return token_lower in row_objects


    def detect_and_apply_count_pattern(self, query_structure: QueryStructure, tokens: List[str]) -> QueryStructure:
        """
        üî¢ DETECTA Y APLICA PATR√ìN COUNT - VERSI√ìN CORREGIDA
        Evita falsos positivos con "count of" vs "account of"
        """
        
        # Crear texto completo para buscar patrones
        full_text = ' '.join(tokens).lower()
        
        print(f"üî¢ DETECTING COUNT PATTERNS IN: '{full_text}'")
        
        # üÜï PATRONES M√ÅS ESPEC√çFICOS Y CONTEXTUALES
        count_patterns = [
            'how many', 'how much', 'cu√°ntos', 'cu√°ntas', 'cuantos', 'cuantas',
            'total number', 'number of', 'cantidad de', 'n√∫mero de'
        ]
        # REMOVIDO 'count of' porque causa falsos positivos con "account of"
        
        # üÜï DETECTAR "COUNT OF" SOLO EN CONTEXTO CORRECTO
        count_of_pattern = False
        if 'count of' in full_text:
            # Verificar que NO sea parte de otra construcci√≥n como "account of"
            count_of_index = full_text.find('count of')
            if count_of_index != -1:
                # Verificar que "count" est√© al inicio de una palabra (no sea parte de "account")
                is_part_of_account = (count_of_index > 0 and 
                                    full_text[count_of_index - 1:count_of_index + 8] == 'account of')
                
                if not is_part_of_account:
                    # Verificar que despu√©s de "count of" hay una m√©trica, no una preposici√≥n
                    after_count_of = full_text[count_of_index + 8:].strip()  # 8 = len('count of')
                    
                    # Si despu√©s de "count of" hay construcciones como "item", "store", es falso positivo
                    if after_count_of:
                        first_words = after_count_of.split()[:2]  # Primeras 2 palabras
                        false_positive_constructions = [
                            'item', 'store', 'account', 'product', 'customer', 'brand',
                            'category', 'line', 'city', 'state', 'country'
                        ]
                        
                        is_false_positive = any(word in false_positive_constructions for word in first_words)
                        
                        if not is_false_positive:
                            count_of_pattern = True
                            print(f"   ‚úÖ Valid COUNT OF pattern found")
                        else:
                            print(f"   üö´ FALSE POSITIVE: 'count of {first_words[0] if first_words else ''}' - not a count pattern")
                    else:
                        print(f"   üö´ Invalid COUNT OF: no content after 'count of'")
                else:
                    print(f"   üö´ FALSE POSITIVE: 'account of' detected, not 'count of'")
        
        # Verificar otros patrones v√°lidos
        count_detected = count_of_pattern
        detected_pattern = 'count of' if count_of_pattern else None
        
        if not count_detected:
            for pattern in count_patterns:
                if pattern in full_text:
                    count_detected = True
                    detected_pattern = pattern
                    print(f"   ‚úÖ COUNT pattern detected: '{pattern}'")
                    break
        
        if not count_detected:
            print(f"   ‚ùå No COUNT pattern detected")
            return query_structure  # Devolver sin cambios
        
        # APLICAR TRANSFORMACI√ìN COUNT solo si es v√°lida
        print(f"   üîß Applying COUNT transformation...")
        
        # Agregar operaci√≥n COUNT si no existe
        count_operation = QueryComponent(
            text=detected_pattern,
            type=ComponentType.OPERATION,
            confidence=0.95,
            subtype='count_operation', 
            value='conteo',
            linguistic_info={
                'source': 'count_pattern_detector',
                'pattern': detected_pattern,
                'sql_function': 'COUNT'
            }
        )
        
        # Agregar a la lista de operaciones
        query_structure.operations.append(count_operation)
        
        # Marcar que es una consulta COUNT
        query_structure.is_count_query = True
        query_structure.count_pattern = detected_pattern
        
        print(f"   ‚úÖ COUNT operation added: {detected_pattern} ‚Üí conteo")
        print(f"   üî¢ Structure marked as COUNT query")
        
        return query_structure


    def _generate_count_sql_simple(self, structure: QueryStructure) -> str:
        """üî¢ GENERADOR SQL SIMPLE PARA COUNT"""
        
        print(f"   üî¢ Generating COUNT SQL:")
        print(f"      üìç Main dimension: {structure.main_dimension.text if structure.main_dimension else 'None'}")
        print(f"      üîó Filters: {len(structure.column_conditions)}")
        print(f"      ‚è∞ Temporal filters: {len(structure.temporal_filters)}")  # AGREGAR
        
        # Determinar qu√© contar
        if structure.main_dimension:
            count_target = f'COUNT(DISTINCT {structure.main_dimension.text})'
            print(f"      üéØ Counting distinct: {structure.main_dimension.text}")
        else:
            count_target = 'COUNT(*)'
            print(f"      üéØ Counting all records")
        
        # WHERE conditions
        where_conditions = []
        
        # Condiciones de columna
        for condition in structure.column_conditions:
            where_conditions.append(f"{condition.column_name} = '{condition.value}'")
            print(f"      ‚úÖ WHERE: {condition.column_name} = '{condition.value}'")
        
        # AGREGAR: Filtros temporales
        temporal_conditions = self.get_advanced_temporal_sql_conditions_english(structure)
        if temporal_conditions:
            where_conditions.extend(temporal_conditions)
            print(f"      üìÖ Temporal conditions: {temporal_conditions}")
        
        # Construir SQL
        sql_parts = [f"SELECT {count_target}", "FROM datos"]
        
        if where_conditions:
            sql_parts.append(f"WHERE {' AND '.join(where_conditions)}")
        
        final_sql = " ".join(sql_parts) + ";"
        print(f"      üéØ COUNT SQL: {final_sql}")
        return final_sql



# =====================================================================
# ========= M√âTODOS COMPLETOS PARA AGREGAR AL FINAL ==================
# =====================================================================

    def detect_superlative_pattern_english(self, tokens: List[str]) -> Optional[SuperlativePattern]:
        """
        üèÜ DETECTOR DE PATR√ìN SUPERLATIVO EN INGL√âS
        Detecta: "which account sold the most", "who had the least", etc.
        """
        
        print(f"üèÜ DETECTING SUPERLATIVE PATTERN:")
        print(f"   üî§ Tokens: {tokens}")
        
        if len(tokens) < 4:  # M√≠nimo: which store sold most
            return None
        
        # STEP 1: Buscar palabra interrogativa
        question_words = {'which', 'who', 'what', 'where'}
        question_word = None
        question_pos = -1
        
        for i, token in enumerate(tokens):
            if token.lower() in question_words:
                question_word = token.lower()
                question_pos = i
                break
        
        if not question_word:
            print(f"   ‚ùå No question word found")
            return None
        
        print(f"   ‚úÖ Question word: '{question_word}' at position {question_pos}")
        
        # STEP 2: Buscar dimensi√≥n objetivo (despu√©s de la palabra interrogativa)
        target_dimension = None
        dimension_pos = -1
        
        for i in range(question_pos + 1, min(question_pos + 3, len(tokens))):
            if i < len(tokens):
                token = tokens[i]
                if self._is_potential_dimension_english(token):
                    target_dimension = token.lower()
                    dimension_pos = i
                    print(f"   ‚úÖ Target dimension: '{target_dimension}' at position {i}")
                    break
        
        if not target_dimension:
            print(f"   ‚ùå No target dimension found")
            return None
        
        # STEP 3: Buscar verbo de acci√≥n
        action_verbs = {
            'sold', 'generated', 'produced', 'made', 'earned', 'achieved',
            'had', 'has', 'got', 'obtained', 'reached', 'recorded'
        }
        
        action_verb = None
        verb_pos = -1
        
        for i in range(dimension_pos + 1, len(tokens)):
            if tokens[i].lower() in action_verbs:
                action_verb = tokens[i].lower()
                verb_pos = i
                print(f"   ‚úÖ Action verb: '{action_verb}' at position {i}")
                break
        
        if not action_verb:
            print(f"   ‚ùå No action verb found")
            return None
        
        # STEP 4: Buscar superlativo
        superlative_patterns = {
            'the most': {'type': 'most', 'direction': 'DESC'},
            'the least': {'type': 'least', 'direction': 'ASC'},
            'the highest': {'type': 'highest', 'direction': 'DESC'},
            'the lowest': {'type': 'lowest', 'direction': 'ASC'},
            'most': {'type': 'most', 'direction': 'DESC'},
            'least': {'type': 'least', 'direction': 'ASC'},
            'highest': {'type': 'highest', 'direction': 'DESC'},
            'lowest': {'type': 'lowest', 'direction': 'ASC'}
        }
        
        superlative_info = None
        superlative_text = None
        
        # Buscar patrones de superlativo despu√©s del verbo
        remaining_text = ' '.join(tokens[verb_pos + 1:]).lower()
        
        for pattern, info in superlative_patterns.items():
            if pattern in remaining_text:
                superlative_info = info
                superlative_text = pattern
                print(f"   ‚úÖ Superlative: '{pattern}' ‚Üí {info['direction']}")
                break
        
        if not superlative_info:
            print(f"   ‚ùå No superlative pattern found")
            return None
        
        # STEP 5: Inferir m√©trica impl√≠cita basada en el verbo
        implied_metric = self._infer_metric_from_verb_english(action_verb)
        print(f"   üìä Implied metric from '{action_verb}': {implied_metric}")
        
        # STEP 6: Calcular confianza
        confidence = 0.6  # Base
        confidence += 0.2  # Tiene palabra interrogativa
        confidence += 0.1  # Tiene dimensi√≥n
        confidence += 0.1  # Tiene verbo de acci√≥n
        confidence += 0.1  # Tiene superlativo
        
        superlative_pattern = SuperlativePattern(
            question_word=question_word,
            target_dimension=target_dimension,
            action_verb=action_verb,
            superlative_type=superlative_info['type'],
            direction=superlative_info['direction'],
            implied_metric=implied_metric,
            confidence=min(1.0, confidence),
            raw_tokens=tokens
        )
        
        print(f"üèÜ SUPERLATIVE PATTERN DETECTED:")
        print(f"   ‚ùì Question: {question_word}")
        print(f"   üìç Target: {target_dimension}")
        print(f"   ‚ö° Action: {action_verb}")
        print(f"   üèÜ Superlative: {superlative_info['type']} ({superlative_info['direction']})")
        print(f"   üìä Implied metric: {implied_metric}")
        print(f"   ‚≠ê Confidence: {superlative_pattern.confidence:.2f}")
        
        return superlative_pattern


    def _infer_metric_from_verb_english(self, action_verb: str) -> Optional[str]:
        """
        üìä INFERIR M√âTRICA BASADA EN EL VERBO DE ACCI√ìN
        """
        
        verb_to_metric = {
            'sold': 'sales',
            'generated': 'revenue', 
            'produced': 'production',
            'made': 'revenue',
            'earned': 'revenue',
            'achieved': 'performance',
            'had': 'sales',  # Default para "had"
            'has': 'sales',
            'got': 'sales',
            'obtained': 'revenue',
            'reached': 'sales',
            'recorded': 'sales'
        }
        
        inferred = verb_to_metric.get(action_verb.lower())
        print(f"      üìä Verb '{action_verb}' ‚Üí metric '{inferred}'")
        
        return inferred


    def generate_superlative_sql_english(self, pattern: SuperlativePattern, structure: QueryStructure) -> str:
        """
        üèÜ GENERADOR SQL PARA PATRONES SUPERLATIVOS
        """
        
        print(f"üèÜ GENERATING SUPERLATIVE SQL:")
        print(f"   üìç Target dimension: {pattern.target_dimension}")
        print(f"   üìä Implied metric: {pattern.implied_metric}")
        print(f"   üéØ Direction: {pattern.direction}")
        
        # PASO 1: Construir SELECT con dimensi√≥n + m√©trica agregada
        select_parts = [pattern.target_dimension]
        
        # Usar m√©trica impl√≠cita o buscar en structure
        metric_to_use = pattern.implied_metric
        if not metric_to_use and structure.metrics:
            metric_to_use = structure.metrics[0].text
        
        if not metric_to_use:
            # Fallback: usar sales como default
            metric_to_use = 'sales'
            print(f"   üìä Using fallback metric: {metric_to_use}")
        
        # Construir funci√≥n de agregaci√≥n
        if pattern.superlative_type in ['most', 'highest']:
            agg_function = f'SUM({metric_to_use})'
        elif pattern.superlative_type in ['least', 'lowest']:
            agg_function = f'SUM({metric_to_use})'
        else:
            agg_function = f'SUM({metric_to_use})'
        
        select_parts.append(agg_function)
        
        # PASO 2: Construir WHERE conditions de structure
        where_conditions = []
        
        for condition in structure.column_conditions:
            where_conditions.append(f"{condition.column_name} = '{condition.value}'")
            print(f"   ‚úÖ WHERE condition: {condition.column_name} = '{condition.value}'")
        
        # Filtros temporales
        if structure.temporal_filters:
            temporal_conditions = self.get_advanced_temporal_sql_conditions_english(structure)
            where_conditions.extend(temporal_conditions)
        
        # PASO 3: Construir SQL completo
        sql_parts = [
            f"SELECT {', '.join(select_parts)}",
            "FROM datos"
        ]
        
        if where_conditions:
            sql_parts.append(f"WHERE {' AND '.join(where_conditions)}")
        
        sql_parts.extend([
            f"GROUP BY {pattern.target_dimension}",
            f"ORDER BY {agg_function} {pattern.direction}",
            "LIMIT 1"
        ])
        
        final_sql = " ".join(sql_parts) + ";"
        
        print(f"   üéØ Superlative SQL: {final_sql}")
        return final_sql


    def detect_enhanced_yn_column_pattern_english(self, tokens: List[str]):
        """üì¶ DETECTA STOCK OUT Y DEAD INVENTORY
        
        Detecta:
        - "stock out" ‚Üí Stock_Out = 'Y'
        - "not stock out" ‚Üí Stock_Out = 'N'
        - "dead inventory" ‚Üí Dead_Inventory = 'Y'
        - "without dead inventory" ‚Üí Dead_Inventory = 'N'
        """
        
        print(f"üì¶ DETECTING ENHANCED Y/N COLUMN PATTERN:")
        print(f"   üì§ Tokens: {tokens}")
        
        # üîç PASO 1: Buscar patrones de columnas Y/N
        yn_positions = []
        
        # Configuraci√≥n de patrones para cada columna
        column_configs = {
            'Stock_Out': {
                'two_words': [['stock', 'out']],
                'single_words': ['stock_out', 'stockout', 'stock-out'],
                'display_name': 'stock out'
            },
            'Dead_Inventory': {
                'two_words': [['dead', 'inventory']],
                'single_words': ['dead_inventory', 'deadinventory', 'dead-inventory'],
                'display_name': 'dead inventory'
            }
        }
        
        # Buscar patrones para cada columna
        for column_name, config in column_configs.items():
            # Buscar patrones de dos palabras separadas
            for word_pair in config['two_words']:
                for i in range(len(tokens) - 1):
                    if (tokens[i].lower() == word_pair[0] and 
                        i + 1 < len(tokens) and 
                        tokens[i + 1].lower() == word_pair[1]):
                        yn_positions.append((i, i + 1, f'{word_pair[0]} {word_pair[1]}', 'separated', column_name))
                        print(f"   ‚úÖ Found '{word_pair[0]} {word_pair[1]}' ({column_name}) at positions {i}-{i+1}")
            
            # Buscar patrones de una sola palabra
            for i in range(len(tokens)):
                token_lower = tokens[i].lower()
                if token_lower in config['single_words']:
                    yn_positions.append((i, i, token_lower, 'single', column_name))
                    print(f"   ‚úÖ Found '{token_lower}' ({column_name}) at position {i}")
            
            # üÜï NUEVO: Buscar patrones como "in stock_out" o "in dead_inventory"
            for i in range(len(tokens) - 1):
                if (tokens[i].lower() == 'in' and 
                    i + 1 < len(tokens) and 
                    tokens[i + 1].lower() in config['single_words']):
                    yn_positions.append((i, i + 1, f'in {tokens[i + 1]}', 'in_pattern', column_name))
                    print(f"   ‚úÖ Found 'in {tokens[i + 1]}' pattern ({column_name}) at positions {i}-{i+1}")
        
        if not yn_positions:
            print(f"   ‚ùå No Y/N column pattern found")
            return None
        
        # üîç PASO 2: Procesar el primer patr√≥n encontrado
        start_pos, end_pos, pattern_text, pattern_type, column_name = yn_positions[0]
        print(f"   üîç Analyzing pattern '{pattern_text}' (column: {column_name}, type: {pattern_type}) at {start_pos}-{end_pos}")
        
        # üÜï CASO ESPECIAL: "in [column]" = POSITIVO (no buscar negaciones)
        if pattern_type == 'in_pattern':
            yn_value = True   # = 'Y'
            negation_found = False
            negation_type = None
            negation_start = start_pos
            indicator_text = pattern_text
            print(f"   ‚úÖ POSITIVE 'in' pattern: '{indicator_text}' ‚Üí {column_name} = 'Y'")
        else:
            # L√ìGICA NORMAL: Buscar negaciones en las 3 posiciones anteriores
            negation_found = False
            negation_type = None
            negation_start = start_pos
            
            # Palabras de negaci√≥n en ingl√©s
            negation_words = {
                'not': 'not',
                'no': 'no', 
                'without': 'without',
                'aren\'t': 'aren\'t',
                'arent': 'wasn\'t',
                'wasnt': 'aren\'t',
                'isnt': 'isn\'t',
                'isn\'t': 'isn\'t',
                'dont': 'don\'t',
                'don\'t': 'don\'t',
                'never': 'never',
                'doesnt': 'doesn\'t',
                'doesn\'t': 'doesn\'t'
            }
            
            # Buscar negaci√≥n en las 3 posiciones anteriores
            search_start = max(0, start_pos - 3)
            for neg_pos in range(search_start, start_pos):
                if tokens[neg_pos].lower() in negation_words:
                    negation_found = True
                    negation_type = negation_words[tokens[neg_pos].lower()]
                    negation_start = neg_pos
                    print(f"   üö´ Negation found: '{tokens[neg_pos]}' ‚Üí '{negation_type}' at position {neg_pos}")
                    break
            
            # üîç PASO 3: Determinar valor Y/N
            if negation_found:
                yn_value = False  # = 'N'
                indicator_text = f"{negation_type} {pattern_text}"
                print(f"   ‚úÖ NEGATIVE pattern: '{indicator_text}' ‚Üí {column_name} = 'N'")
            else:
                yn_value = True   # = 'Y'
                indicator_text = pattern_text
                print(f"   ‚úÖ POSITIVE pattern: '{indicator_text}' ‚Üí {column_name} = 'Y'")
        
        # üîç PASO 4: Calcular confianza
        confidence = 0.90  # Alta confianza para patrones directos
        
        # Bonus por patrones espec√≠ficos
        if pattern_type == 'separated':  # Patr√≥n de dos palabras es m√°s natural
            confidence += 0.05
        
        if negation_found:
            confidence += 0.03  # Negaci√≥n clara
        
        confidence = min(1.0, confidence)
        
        # üîç PASO 5: Crear YNColumnPattern
        yn_pattern = YNColumnPattern(
            column_name=column_name,  # Ahora es din√°mico: Stock_Out o Dead_Inventory
            value='Y' if yn_value else 'N',
            negation_detected=negation_found,
            indicator_text=indicator_text,
            position_start=negation_start if negation_found else start_pos,
            position_end=end_pos,
            confidence=confidence,
            raw_tokens=tokens[negation_start if negation_found else start_pos:end_pos + 1]
        )
        
        print(f"üì¶ Y/N COLUMN PATTERN DETECTED:")
        print(f"   üìã Column: {column_name}")
        print(f"   üì¶ Text: '{indicator_text}'")
        print(f"   ‚úÖ Value: {'Y' if yn_value else 'N'}")
        print(f"   üö´ Negation detected: {negation_found}")
        print(f"   üìç Positions: {negation_start if negation_found else start_pos}-{end_pos}")
        print(f"   ‚≠ê Confidence: {confidence:.2f}")
        print(f"   üéØ SQL: {column_name} = '{yn_pattern.value}'")
        
        # Retornar el primer patr√≥n encontrado (m√°s espec√≠fico)
        return yn_pattern


    def detect_groupby_pattern_english(self, tokens: List[str]) -> Optional[QueryComponent]:
        """
        üîç DETECTOR DE PATR√ìN 'BY [DIMENSI√ìN]' usando diccionarios existentes
        """
        
        print(f"üîç DETECTING GROUP BY PATTERN:")
        
        for i, token in enumerate(tokens):
            if token.lower() == 'by' and i + 1 < len(tokens):
                next_token = tokens[i + 1]
                
                # Usar el m√©todo existente que ya consulta self.dictionaries.dimensiones
                column_info = self._identify_potential_column_english(next_token)
                
                if column_info['is_column'] and column_info['type'] == 'dimension':
                    # Es una dimensi√≥n v√°lida seg√∫n el diccionario
                    normalized_dimension = column_info['normalized_name']
                    
                    groupby_dimension = QueryComponent(
                        text=normalized_dimension,
                        type=ComponentType.DIMENSION,
                        confidence=0.95,
                        subtype='groupby_dimension',
                        linguistic_info={
                            'source': 'by_pattern',
                            'original_form': next_token,
                            'pattern': f'by {next_token}'
                        }
                    )
                    
                    print(f"   ‚úÖ GROUP BY pattern detected: 'by {next_token}' ‚Üí GROUP BY {normalized_dimension}")
                    return groupby_dimension
        
        return None


    def generate_enhanced_list_all_sql_english(self, pattern_data: Dict, structure: QueryStructure) -> str:
        """
        üìã GENERADOR SQL INTELIGENTE PARA LIST ALL - CORREGIDO PARA VALORES √öNICOS
        """
        
        print(f"üìã GENERATING ENHANCED LIST ALL SQL (WITH DISTINCT/GROUP BY LOGIC):")
        
        target_dimension = pattern_data['target_dimension']
        list_indicator = pattern_data['list_indicator']
        has_aggregation = pattern_data.get('has_aggregation', False)
        
        print(f"   üìã List type: {list_indicator}")
        print(f"   üìç Target dimension: {target_dimension}")
        print(f"   üìä Has aggregation: {has_aggregation}")
        
        # PASO 1: Construir SELECT
        formatted_dim = self.format_temporal_dimension(target_dimension)
        select_parts = []
        
        # üîß L√ìGICA CORREGIDA: Determinar si necesitamos DISTINCT o GROUP BY
        needs_group_by = False
        use_distinct = True  # Por defecto usar DISTINCT
        
        # PASO 1.5: SI HAY M√âTRICAS Y OPERACIONES, AGREGARLAS
        if has_aggregation or (structure.operations and structure.metrics):
            print(f"   üìä Detected metrics and operations - adding aggregations")
            needs_group_by = True
            use_distinct = False  # No usar DISTINCT cuando hay GROUP BY
            
            # Agregar dimensi√≥n SIN DISTINCT
            select_parts.append(formatted_dim)
            
            # Procesar operaciones y m√©tricas
            if structure.operations and structure.metrics:
                for i, metric in enumerate(structure.metrics):
                    # Determinar operaci√≥n para esta m√©trica
                    if i < len(structure.operations):
                        operation = structure.operations[i]
                    else:
                        operation = structure.operations[0] if structure.operations else None
                    
                    if operation:
                        operation_value = operation.value
                        
                        # Mapear operaci√≥n a funci√≥n SQL
                        if operation_value == 'm√°ximo':
                            agg_function = self._get_contextual_aggregation_english(
                                structure, metric.text, operation_value
                            )
                        else:
                            sql_operations = {
                                'm√≠nimo': f'MIN({metric.text})',
                                'suma': f'SUM({metric.text})',
                                'promedio': f'AVG({metric.text})',
                                'conteo': f'COUNT({metric.text})',
                                'total': f'SUM({metric.text})'
                            }
                            agg_function = sql_operations.get(operation_value, f'SUM({metric.text})')
                        
                        # Agregar alias descriptivo
                        alias = f"total_{metric.text}" if 'sum' in agg_function.lower() else agg_function
                        select_parts.append(f"{agg_function} as {alias}")
                        print(f"   ‚úÖ Added aggregation: {agg_function} as {alias}")
                    else:
                        # Si no hay operaci√≥n, asumir SUM por defecto
                        select_parts.append(f"SUM({metric.text}) as total_{metric.text}")
                        print(f"   ‚úÖ Added default aggregation: SUM({metric.text})")
            
            # Si solo hay m√©tricas sin operaciones
            elif structure.metrics and not structure.operations:
                for metric in structure.metrics:
                    select_parts.append(f"SUM({metric.text}) as total_{metric.text}")
                    print(f"   ‚úÖ Added metric aggregation: SUM({metric.text})")
        else:
            # üîß CASO SIMPLE: Solo listar valores √∫nicos
            print(f"   üìã Simple list - using DISTINCT")
            use_distinct = True
            needs_group_by = False
            select_parts.append(formatted_dim)
        
        # üîß CONSTRUIR SELECT CLAUSE CON O SIN DISTINCT
        if use_distinct:
            select_clause = f"SELECT DISTINCT {', '.join(select_parts)}"
            print(f"   ‚úÖ Using DISTINCT for unique values")
        else:
            select_clause = f"SELECT {', '.join(select_parts)}"
            print(f"   ‚úÖ Not using DISTINCT (GROUP BY will handle uniqueness)")
        
        # PASO 2: FROM clause
        from_part = "FROM datos"
        
        # PASO 3: WHERE conditions
        where_conditions = []
        
        # 3.1: Filtros de columna
        for condition in structure.column_conditions:
            # No agregar la dimensi√≥n objetivo como filtro si es la misma que estamos listando
            if condition.column_name.lower() != target_dimension.lower():
                sql_condition = f"{condition.column_name} = '{condition.value}'"
                where_conditions.append(sql_condition)
                print(f"   ‚úÖ Column filter: {sql_condition}")
        
        # 3.2: Filtros temporales
        temporal_conditions = self._get_temporal_conditions_for_list_all(structure)
        if temporal_conditions:
            where_conditions.extend(temporal_conditions)
        
        # 3.3: Filtros de exclusi√≥n
        if hasattr(structure, 'exclusion_filters'):
            for exclusion in structure.exclusion_filters:
                if exclusion.exclusion_type == ExclusionType.NOT_EQUALS:
                    sql_condition = f"{exclusion.column_name} != '{exclusion.value}'"
                    where_conditions.append(sql_condition)
                    print(f"   ‚úÖ Exclusion filter: {sql_condition}")
        
        # PASO 4: GROUP BY si es necesario
        group_by_part = None
        if needs_group_by:
            group_by_part = f"GROUP BY {target_dimension}"
            print(f"   ‚úÖ GROUP BY added: {target_dimension}")
        
        # PASO 5: ORDER BY
        order_by_parts = []
        
        # Si hay agregaciones, podemos ordenar por ellas tambi√©n
        if needs_group_by and len(select_parts) > 1:
            # Extraer la primera funci√≥n de agregaci√≥n para ordenar por ella
            for part in select_parts[1:]:  # Saltar la dimensi√≥n
                if 'SUM' in part or 'MAX' in part or 'MIN' in part or 'AVG' in part:
                    # Extraer el alias o la funci√≥n completa
                    if ' as ' in part:
                        alias = part.split(' as ')[1]
                        order_by_parts.append(f"{alias} DESC")
                    else:
                        order_by_parts.append(f"{part} DESC")
                    break
        
        # Siempre ordenar tambi√©n por la dimensi√≥n
        order_by_parts.append(target_dimension)
        
        order_part = f"ORDER BY {', '.join(order_by_parts)}" if order_by_parts else f"ORDER BY {target_dimension}"
        
        # PASO 6: Construir SQL final
        sql_parts = [select_clause, from_part]
        
        if where_conditions:
            sql_parts.append(f"WHERE {' AND '.join(where_conditions)}")
        
        if group_by_part:
            sql_parts.append(group_by_part)
        
        sql_parts.append(order_part)
        
        # Agregar LIMIT para consultas muy grandes
        # Puedes descomentar esto si quieres limitar resultados por defecto
        # if not needs_group_by:  # Solo para listados simples
        #     sql_parts.append("LIMIT 1000")
        
        final_sql = " ".join(sql_parts) + ";"
        
        print(f"   üéØ Enhanced LIST ALL SQL: {final_sql}")
        return final_sql


    def _get_temporal_conditions_for_list_all(self, structure: QueryStructure) -> List[str]:
        """
        ‚è∞ M√ìDULO DE FILTROS TEMPORALES PARA LIST ALL
        Reutiliza la l√≥gica existente de filtros temporales
        """
        
        print(f"   ‚è∞ Processing temporal conditions for LIST ALL...")
        
        # Reutilizar el m√©todo existente que ya funciona bien
        if hasattr(self, 'get_advanced_temporal_sql_conditions_english'):
            temporal_conditions = self.get_advanced_temporal_sql_conditions_english(structure)
            print(f"   ‚è∞ Found {len(temporal_conditions)} temporal conditions")
            return temporal_conditions
        else:
            print(f"   ‚ùå Temporal method not available")
            return []




# =========================================================        
# =========== PIPELINE PARA CONSULTAS EN ESPA√ëOL ==========
# =========================================================     


# clase dedicada al manejo de las consultas de principio a fin
class UnifiedNLPParser:
    """Parser NLP unificado - EXPANDIDO para datos referenciados"""
    
                
                
        # ====================================================
        # GRUPO 1: CONFIGURACI√ìN Y CONTROL 
        # Funciones de inicializaci√≥n y coordinaci√≥n principal
        # ====================================================
    
    
# ------ "Inicializador del Sistema" -------
    
    def __init__(self, enable_logging: bool = True):
        """Inicializador del Sistema - VERSI√ìN MEJORADA"""
        self.dictionaries = JSONDictionaryLoader()
        self.enable_logging = enable_logging
        self.query_history = []
        
        # Analizador pre-mapeo (NO afecta diccionarios)
        self.pre_mapping_analyzer = PreMappingSemanticAnalyzer()
        
        self.session_stats = {
            'total_queries': 0,
            'successful_queries': 0,
            'failed_queries': 0,
            'simple_queries': 0,
            'complex_queries': 0,
            'session_start': datetime.now()
        }
        
        
        
# -------------------------------------------------------------------------
# ---------------- CONTROL DE CONSULTAS DESCONOCIDAS ----------------------
# -------------------------------------------------------------------------        
        
        
# Sistema de palabras desconocidas
        self.unknown_words_log_path = "control/consultas_sin_respuestas/unknown_words_log.json"
        self.confidence_threshold = 0.6
        self.unknown_words_log = self._load_unknown_words_log()
        self.session_id = self._generate_session_id()
        
        print("üöÄ Parser NLP Unificado iniciado")
        print(f"üìö Diccionarios cargados: {self.dictionaries.get_statistics()}")
        print(f"üö® Sistema de palabras desconocidas activado")
        print(f"üìÅ Log de palabras desconocidas: {self.unknown_words_log_path}")


# M√©todo auxiliar deteccion de palabras desconocidas
    def _generate_session_id(self) -> str:
        """Generar ID √∫nico de sesi√≥n"""
        return f"session_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
    
    
# metodo de apoyo para la deteccion de palabras desconocidas
    def _load_unknown_words_log(self) -> Dict:
        """Cargar log existente de palabras desconocidas"""
        if os.path.exists(self.unknown_words_log_path):
            try:
                with open(self.unknown_words_log_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    print(f"üìã Log de palabras desconocidas cargado: {len(data.get('failures', []))} consultas previas")
                    return data
            except Exception as e:
                print(f"‚ö†Ô∏è Error cargando log: {e}")
        
        return {
            'failures': [],
            'statistics': {
                'total_failures': 0,
                'most_common_unknown_words': {},
                'last_updated': datetime.now().isoformat()
            }
        }
    


# =============================================
# M√âTODOS DE DETECCI√ìN DE PALABRAS DESCONOCIDAS
# =============================================


    # M√©todo para manejar palabras desconocidas
        
    def detect_unknown_words(self, tokens: List[str], classified_components: Dict) -> Tuple[List[UnknownWord], bool]:
        """
        üîç DETECTOR PRINCIPAL DE PALABRAS DESCONOCIDAS - VERSI√ìN CON TEMPORAL
        Retorna: (lista_palabras_desconocidas, hay_palabras_cr√≠ticas)
        """
        unknown_words = []
        has_critical_unknowns = False
        
        print(f"\nüîç VERIFICANDO PALABRAS DESCONOCIDAS:")
        
        for i, token in enumerate(tokens):
            # Obtener contexto
            context_before = tokens[max(0, i-2):i]
            context_after = tokens[i+1:min(len(tokens), i+3)]
            
            # Verificar si el token est√° clasificado
            component = classified_components.get(token)
            
            if component is None:
                # Buscar en diccionario temporal antes de marcar como desconocido
                temporal_entry = self.dictionaries.search_in_temporal_dictionary(token)
                
                if temporal_entry:
                    # Encontrado en temporal - crear componente temporal
                    temporal_type = self.dictionaries.get_temporal_component_type(token)
                    temporal_component = QueryComponent(
                        text=token,
                        type=temporal_type or ComponentType.VALUE,
                        confidence=temporal_entry.get('confidence', 0.9),
                        subtype='temporal_data',
                        value=temporal_entry.get('original_value'),
                        column_name=temporal_entry.get('column_name'),
                        linguistic_info={
                            'source': 'temporal_dictionary',
                            'original_value': temporal_entry.get('original_value'),
                            'column_name': temporal_entry.get('column_name'),
                            'column_type': temporal_entry.get('column_type')
                        }
                    )
                    
                    # Agregar al diccionario de componentes clasificados
                    classified_components[token] = temporal_component
                    
                    print(f"   ‚úÖ TEMPORAL: '{token}' encontrado como {temporal_entry.get('original_value')} en {temporal_entry.get('column_name')}")
                    continue
                
                # Si no est√° en temporal tampoco, entonces es desconocido
                unknown_word = UnknownWord(
                    word=token,
                    position=i,
                    context_before=context_before,
                    context_after=context_after,
                    suggested_type='unknown',
                    confidence=0.0,
                    timestamp=datetime.now().isoformat(),
                    full_query=' '.join(tokens)
                )
                unknown_words.append(unknown_word)
                has_critical_unknowns = True
                print(f"   ‚ùå CR√çTICO: '{token}' no encontrado en operacionales NI temporal")
                
            elif component.confidence < self.confidence_threshold:
                # Token con confianza muy baja - mantener l√≥gica existente
                unknown_word = UnknownWord(
                    word=token,
                    position=i,
                    context_before=context_before,
                    context_after=context_after,
                    suggested_type=component.type.value,
                    confidence=component.confidence,
                    timestamp=datetime.now().isoformat(),
                    full_query=' '.join(tokens)
                )
                unknown_words.append(unknown_word)
                
                if component.confidence < 0.4:
                    has_critical_unknowns = True
                    print(f"   üö® CR√çTICO: '{token}' confianza muy baja ({component.confidence:.2f})")
                else:
                    print(f"   ‚ö†Ô∏è SOSPECHOSO: '{token}' confianza baja ({component.confidence:.2f})")
        
        print(f"üìä Palabras desconocidas: {len(unknown_words)} | Cr√≠ticas: {has_critical_unknowns}")
        return unknown_words, has_critical_unknowns
        
        
    # DETENER PROCESAMIENTO EN CASO DE DATO DESCONOCIDO
        
    def should_stop_processing(self, unknown_words: List[UnknownWord], query_complexity: str) -> bool:
        """üõë DECISOR: ¬øDebe detenerse el procesamiento?"""
        if not unknown_words:
            return False
        
        critical_words = [w for w in unknown_words if w.confidence < 0.4]
        
        print(f"üõë EVALUANDO DETENCI√ìN: {len(critical_words)} cr√≠ticas, complejidad: {query_complexity}")
        
        # REGLAS DE DECISI√ìN
        if len(critical_words) >= 2:
            print(f"   üõë DETENER: Demasiadas palabras cr√≠ticas")
            return True
        
        if len(critical_words) >= 1 and query_complexity in ['compleja', 'muy_compleja']:
            print(f"   üõë DETENER: Palabra cr√≠tica en consulta compleja")
            return True
        
        total_tokens = len(unknown_words[0].full_query.split()) if unknown_words else 0
        unknown_percentage = len(unknown_words) / total_tokens if total_tokens > 0 else 0
        
        if unknown_percentage > 0.3:
            print(f"   üõë DETENER: Demasiados tokens desconocidos ({unknown_percentage:.1%})")
            return True
        
        if len(critical_words) > 0:
            print(f"   üõë DETENER: Modo conservador - hay palabra cr√≠tica")
            return True
        
        print(f"   ‚úÖ CONTINUAR: Sin problemas cr√≠ticos")
        return False
    
    
    # FEED BACK PARA EL USUARIO
        
    def generate_user_feedback(self, unknown_words: List[UnknownWord], original_query: str) -> Dict:
        """üí° GENERAR FEEDBACK √öTIL PARA EL USUARIO"""
        feedback = {
            'type': 'error',
            'original_query': original_query,
            'unknown_words': [],
            'suggestions': [],
            'similar_words': []
        }
        
        # Procesar cada palabra desconocida
        for word in unknown_words:
            word_info = {
                'word': word.word,
                'position': word.position,
                'context': f"...{' '.join(word.context_before)} [{word.word}] {' '.join(word.context_after)}...",
                'confidence': word.confidence,
                'severity': 'critical' if word.confidence < 0.4 else 'suspicious'
            }
            feedback['unknown_words'].append(word_info)
        
        # Generar sugerencias
        feedback['suggestions'] = self._generate_suggestions(unknown_words)
        feedback['similar_words'] = self._find_similar_words(unknown_words)
        
        return feedback
    
    
    # OFRECER SOLUCIONES TEMPORALES AL USUARIO
        
    def _generate_suggestions(self, unknown_words: List[UnknownWord]) -> List[str]:
        """Generar sugerencias √∫tiles"""
        suggestions = [
            "Verifica la ortograf√≠a de las palabras no reconocidas",
            "Usa t√©rminos del vocabulario: account, tienda, partner_code, ventas, inventario",
            "Para t√©rminos compuestos usa guiones bajos: sales_amount, customer_id",
            "Operaciones v√°lidas: mas, mayor, menor, suma, promedio, maximo, minimo"
        ]
        
        return suggestions
    
    
    # OFRECER PALABRAS SIMILARES 
    
    def _find_similar_words(self, unknown_words: List[UnknownWord]) -> List[Dict]:
        """Buscar palabras similares"""
        similar_words = []
        
        common_alternatives = {
            'cuenta': 'account', 'cuentas': 'account',
            'tiendas': 'tienda', 'producto': 'product',
            'venta': 'ventas', 'sale': 'ventas',
            'inventari': 'inventario', 'stock': 'inventario',
            'maximo': 'mas', 'm√°ximo': 'mas',
            'minimo': 'menor', 'm√≠nimo': 'menor'
        }
        
        for word in unknown_words:
            word_lower = word.word.lower()
            for incorrect, correct in common_alternatives.items():
                if incorrect in word_lower:
                    similar_words.append({
                        'original': word.word,
                        'suggested': correct,
                        'reason': 'T√©rmino similar encontrado'
                    })
        
        return similar_words
    
    
    
    # VERIFICAR LA CONSULTA FALLIDA
    
    def log_query_failure(self, original_query: str, unknown_words: List[UnknownWord]):
        """üìù REGISTRAR CONSULTA FALLIDA"""
        failure = QueryFailure(
            original_query=original_query,
            unknown_words=[asdict(word) for word in unknown_words],
            timestamp=datetime.now().isoformat(),
            session_id=self.session_id
        )
        
        self.unknown_words_log['failures'].append(asdict(failure))
        self._update_unknown_statistics(unknown_words)
        self._save_unknown_log()
        
        print(f"üìù Consulta fallida registrada con {len(unknown_words)} palabras desconocidas")
    
    
    # ACTUALIZAR LA LISTA DE PALABRAS NO RECONOCIDAS
    
    def _update_unknown_statistics(self, unknown_words: List[UnknownWord]):
        """Actualizar estad√≠sticas"""
        stats = self.unknown_words_log['statistics']
        stats['total_failures'] += 1
        
        if 'most_common_unknown_words' not in stats:
            stats['most_common_unknown_words'] = {}
        
        for word in unknown_words:
            word_key = word.word.lower()
            if word_key not in stats['most_common_unknown_words']:
                stats['most_common_unknown_words'][word_key] = {'count': 0, 'contexts': []}
            
            stats['most_common_unknown_words'][word_key]['count'] += 1
            stats['most_common_unknown_words'][word_key]['contexts'].append(word.full_query)
        
        stats['last_updated'] = datetime.now().isoformat()
    
    
    #  GUARDAR LA PALABRA NO RECONOCIDA
            
    def _save_unknown_log(self):
        """Guardar log en archivo JSON"""
        try:
            with open(self.unknown_words_log_path, 'w', encoding='utf-8') as f:
                json.dump(self.unknown_words_log, f, indent=2, ensure_ascii=False)
        except Exception as e:
            print(f"‚ùå Error guardando log: {e}")



# ====================================
# PROCESAMIENTO DE INPUT POST FILTRADO
# ====================================


# ------ "Punto de Entrada Principal" -------

    def process_user_input(self, user_input: str) -> Dict:
        """Punto de Entrada Principal"""
        self.session_stats['total_queries'] += 1
        
        query_entry = {
            'timestamp': datetime.now().strftime('%H:%M:%S'),
            'input': user_input,
            'processed': False
        }
        
        try:
            result = self.analyze_unified_query(user_input)
            
            if result.get('success', False):
                self.session_stats['successful_queries'] += 1
                query_entry['processed'] = True
                query_entry['result'] = result
                
                if result.get('complexity_level') in ['simple', 'moderada']:
                    self.session_stats['simple_queries'] += 1
                else:
                    self.session_stats['complex_queries'] += 1
            else:
                self.session_stats['failed_queries'] += 1
                query_entry['error'] = result.get('error', 'Error desconocido')
            
            self.query_history.append(query_entry)
            return result
            
        except Exception as e:
            self.session_stats['failed_queries'] += 1
            error_result = {
                'success': False,
                'error': f"Error procesando consulta: {str(e)}",
                'original_input': user_input,
                'suggestions': self.generate_error_suggestions(user_input)
            }
            
            query_entry['error'] = str(e)
            self.query_history.append(query_entry)
            return error_result
    
    
# ------  "Coordinador de Pipeline" -------
    

    def analyze_unified_query(self, query: str) -> Dict:
        """Cerebro Coordinador del Pipeline - ROUTER LIMPIO"""
        if not query or not query.strip():
            return {
                'success': False,
                'error': 'Consulta vac√≠a',
                'suggestions': ['Intenta con: "partner code con mas ventas"']
            }
        
        print(f"\nüîç ANALIZANDO CONSULTA: '{query}'")
        
        # PASO 0.1: NORMALIZAR FRASES COMPUESTAS PRIMERO
        pre_normalized_query = self.dictionaries._detect_compound_phrases_dictionary_based(query)
        
        # PASO 0.2: DETECCI√ìN DE IDIOMA CON TOKENS YA NORMALIZADOS
        preliminary_tokens = pre_normalized_query.lower().split()
        detected_language = self.dictionaries.detect_language_from_tokens(preliminary_tokens)
        
        print(f"üåç IDIOMA DETECTADO: {detected_language.upper()}")
        
        # üéØ ROUTER PRINCIPAL
        if detected_language == 'en':
            print(f"üá∫üá∏ CONSULTA EN INGL√âS DETECTADA - ENVIANDO A PIPELINE INGL√âS")
            
            # Crear instancia del parser ingl√©s
            english_parser = EnglishNLPParser(self.dictionaries)
            
            # Procesar con pipeline ingl√©s
            return english_parser.process_query(query, pre_normalized_query, preliminary_tokens)
        
        else:
            print(f"üá™üá∏ CONSULTA EN ESPA√ëOL - ENVIANDO A PIPELINE ESPA√ëOL")
            
            # Procesar con pipeline espa√±ol
            return self.process_spanish_query(query, pre_normalized_query, preliminary_tokens)
            
        
    def process_spanish_query(self, query: str, pre_normalized_query: str, preliminary_tokens: List[str]) -> Dict:
        """üá™üá∏ PIPELINE ESPA√ëOL - TODO TU C√ìDIGO ORIGINAL MOVIDO AQU√ç"""
        
        print(f"üá™üá∏ PROCESANDO CONSULTA EN ESPA√ëOL")
        
        # PASO 1: NORMALIZACI√ìN COMPLETA (ahora usa la query ya pre-normalizada)
        normalized_query = self.normalize_query_with_compounds(pre_normalized_query)
        tokens = normalized_query.split()
        
        print(f"üî§ Tokens: {tokens}")
            
        # PASO 1.5: AN√ÅLISIS SEM√ÅNTICO PRE-MAPEO
        original_intent = self.pre_mapping_analyzer.analyze_original_intent(tokens)
        print(f"üß† Intent sem√°ntico original: {original_intent}")
            
        # PASO 2: DETECCI√ìN DE PATRONES COMPLEJOS
        temporal_filters = self.detect_temporal_patterns_advanced(tokens)
        column_value_pairs = self.detect_column_value_patterns(tokens, temporal_filters)
            
        # PASO 3: CLASIFICACI√ìN DE COMPONENTES
        classified_components = self.classify_all_components(tokens, column_value_pairs)
        
        # PASO 3.5: VERIFICAR PALABRAS DESCONOCIDAS
        unknown_words, has_critical = self.detect_unknown_words(tokens, classified_components)
        
        # Calcular complejidad preliminar para tomar decisi√≥n
        preliminary_complexity = self._calculate_preliminary_complexity(
            classified_components, temporal_filters, column_value_pairs
        )
        
        # DECISI√ìN: ¬øContinuar o detener?
        should_stop = self.should_stop_processing(unknown_words, preliminary_complexity)
        
        if should_stop:
            print(f"üõë PROCESAMIENTO DETENIDO - Palabras desconocidas cr√≠ticas")
            
            # Generar feedback detallado
            feedback = self.generate_user_feedback(unknown_words, query)
            
            # Registrar falla
            self.log_query_failure(query, unknown_words)
            
            return {
                'success': False,
                'error': 'Consulta contiene palabras no reconocidas',
                'error_type': 'unknown_words',
                'unknown_words_feedback': feedback,
                'original_input': query,
                'processing_stopped': True,
                'suggestions': feedback['suggestions'],
                'unknown_words_count': len(unknown_words),
                'critical_words': [w.word for w in unknown_words if w.confidence < 0.4],
                'language': 'spanish'
            }
        
        # Si hay palabras sospechosas pero no cr√≠ticas, continuar con advertencia
        if unknown_words:
            print(f"‚ö†Ô∏è CONTINUANDO con {len(unknown_words)} palabras sospechosas")
        
        # PASO 4: CONSTRUCCI√ìN DE ESTRUCTURA
        self._current_original_intent = original_intent
        query_structure = self.build_unified_structure(classified_components, column_value_pairs, temporal_filters, tokens)
        
        # PASO 5: VALIDACI√ìN
        validation_result = self.validate_structure(query_structure)
        if not validation_result['valid']:
            return {
                'success': False,
                'error': validation_result['error'],
                'original_input': query,
                'suggestions': validation_result['suggestions'],
                'partial_analysis': self.structure_to_dict(query_structure),
                'language': 'spanish'
            }

        hierarchical_structure = self.generate_hierarchical_structure(query_structure)
        sql_query = self.generate_optimized_sql(query_structure)
        
        # PASO FINAL: Normalizaci√≥n de esquema SQL
        try:
            schema_mapper = SQLSchemaMapper()
            sql_query = schema_mapper.normalize_sql(sql_query)
            print(f"üîó SQL normalizado aplicado")
        except Exception as e:
            print(f"‚ö†Ô∏è Error en normalizaci√≥n SQL: {e}")
            print(f"üîÑ Continuando con SQL original")
                
        # RESULTADO CON INFORMACI√ìN ADICIONAL
        result = {
            'success': True,
            'language': 'spanish',
            'original_input': query,
            'normalized_query': normalized_query,
            'tokens': tokens,
            'query_structure': self.structure_to_dict(query_structure),
            'hierarchical_structure': hierarchical_structure,
            'classified_components': {token: self.component_to_dict(comp) 
                                    for token, comp in classified_components.items()},
            'column_value_pairs': [self.cvp_to_dict(cvp) for cvp in column_value_pairs],
            'temporal_filters': [self.temporal_to_dict(tf) for tf in temporal_filters],
            'sql_query': sql_query,
            'complexity_level': query_structure.get_complexity_level(),
            'confidence': self.calculate_overall_confidence(query_structure),
            'interpretation': self.generate_natural_interpretation(query_structure),
            'processing_method': 'unified_hybrid',
            'unknown_words_detected': len(unknown_words),
            'unknown_words_details': [asdict(word) for word in unknown_words] if unknown_words else []
        }
        
        return result    
            
        
        
        
    def _calculate_preliminary_complexity(self, classified_components: Dict, temporal_filters: List, column_value_pairs: List) -> str:
        """Calcular complejidad preliminar para tomar decisiones tempranas"""
        score = 0
        score += len([c for c in classified_components.values() if c.type.value == 'operation'])
        score += len([c for c in classified_components.values() if c.type.value == 'metric'])
        score += len(temporal_filters) * 2
        score += len(column_value_pairs) * 2
            
        if score <= 2:
            return "simple"
        elif score <= 4:
            return "moderada"
        elif score <= 6:
            return "compleja"
        else:
            return "muy_compleja"
        

# ------  "Generador de sugerencias de error" -------
    
    def generate_error_suggestions(self, query: str) -> List[str]:
        """Generador de Sugerencias de Error"""
        return [
            "Intenta con: 'partner code con mas ventas'",
            "Ejemplo: 'product group con mayor sell out'",
            "Estructura: [entidad] con [operaci√≥n] [m√©trica]",
            "Frases compuestas: partner_code, customer_id, sales_amount"
        ]
    
    
    
        # ===================================================
        # GRUPO 2: PROCESAMIENTO DE TEXTO 
        # Normalizaci√≥n, tokenizaci√≥n y detecci√≥n de patrones
        # ===================================================
    

# ------  "Limpiador y normalizador de texto" -------
            
    def normalize_query_with_compounds(self, query: str) -> str:
        """üîß NORMALIZADOR - REGLA ABSOLUTA PARA MAY√öSCULAS"""
        
        print(f"üîç DEBUG 0: Query despu√©s de frases compuestas: '{query}'")
        
        words = query.split()
        corrected_words = []
        
        for word in words:
            
# REGLA ABSOLUTA: NUNCA tocar letras may√∫sculas individuales
            if len(word) == 1 and word.isupper() and word.isalpha():
                corrected_words.append(word)  # PRESERVAR EXACTAMENTE
                print(f"üîí PRESERVANDO DATO ABSOLUTO: '{word}' (letra may√∫scula)")
            else:
                # Solo aplicar correcciones a palabras que NO sean datos
                corrected_word = self.dictionaries.correct_typo(word)
                corrected_words.append(corrected_word)
                if corrected_word != word:
                    print(f"üîß Correcci√≥n: '{word}' ‚Üí '{corrected_word}'")
        
        query = ' '.join(corrected_words)
        
# PASO 2: Limpiar caracteres especiales pero preservar espacios y guiones bajos
        query = re.sub(r'[^\w\s_]', '', query)
        
# PASO 3: Normalizar espacios m√∫ltiples
        query = re.sub(r'\s+', ' ', query).strip()
        
        print(f"üîç DEBUG FINAL: Query normalizada: '{query}'")
        
        return query


    def _detect_compound_phrases_layer1_dictionary(self, query: str) -> str:
        """
        ü•á CAPA 1: DETECCI√ìN BASADA EN DICCIONARIOS EXISTENTES
        Usa synonym_groups y diccionarios conocidos - M√ÅS R√ÅPIDA
        """
        print(f"üîç CAPA 1: Detecci√≥n por diccionarios")
        
        text_lower = query.lower()
        changes_made = []
        
        # Usar synonym_groups existente (tu l√≥gica actual mejorada)
        sorted_phrases = sorted(self.synonym_groups.keys(), key=len, reverse=True)
        
        for phrase in sorted_phrases:
            if phrase in text_lower:
                normalized = self.synonym_groups[phrase]
                text_lower = text_lower.replace(phrase, normalized)
                changes_made.append(f"'{phrase}' ‚Üí '{normalized}'")
        
        # Tambi√©n buscar directamente en dimensiones y m√©tricas con espacios
        all_known_phrases = set()
        
        # Agregar dimensiones que tienen espacios o guiones
        for dim in self.dimensiones:
            if '_' in dim:
                space_version = dim.replace('_', ' ')
                all_known_phrases.add((space_version, dim))
        
        # Agregar m√©tricas que tienen espacios o guiones  
        for metric in self.metricas:
            if '_' in metric:
                space_version = metric.replace('_', ' ')
                all_known_phrases.add((space_version, metric))
        
        # Aplicar reemplazos de frases conocidas
        for space_phrase, underscore_phrase in sorted(all_known_phrases, key=lambda x: len(x[0]), reverse=True):
            if space_phrase in text_lower:
                text_lower = text_lower.replace(space_phrase, underscore_phrase)
                changes_made.append(f"'{space_phrase}' ‚Üí '{underscore_phrase}'")
        
        if changes_made:
            print(f"   ‚úÖ CAPA 1 detect√≥: {changes_made}")
        
        return text_lower


# ------  "Detector de expresiones temporales" -------

    def detect_temporal_patterns_advanced(self, tokens: List[str]) -> List[TemporalFilter]:
        """
        üîß Detector de Expresiones Temporales - VERSI√ìN CORREGIDA CON ORDEN CORRECTO
        PRIORIDAD: Patrones largos PRIMERO, patrones cortos DESPU√âS
        """
        
        print(f"üîç DETECTANDO PATRONES TEMPORALES AVANZADOS:")
        print(f"   üî§ Tokens: {tokens}")
        
        temporal_filters = []
        advanced_temporal_info = []
        i = 0
        
        while i < len(tokens):
            
# üÜï PATR√ìN M√ÅS ESPEC√çFICO 1: "desde [UNIDAD] [N√öMERO] a [N√öMERO]" - desde semana 8 a 12
            if (i < len(tokens) - 4 and
                tokens[i].lower() == 'desde' and
                tokens[i + 1].lower() in self.dictionaries.unidades_tiempo and
                (tokens[i + 2].isdigit() or tokens[i + 2] in self.dictionaries.numeros_palabras) and
                tokens[i + 3].lower() == 'a' and
                (tokens[i + 4].isdigit() or tokens[i + 4] in self.dictionaries.numeros_palabras)):
                
                unit = self.dictionaries.unidades_tiempo[tokens[i + 1].lower()]
                
                if tokens[i + 2].isdigit():
                    start_value = int(tokens[i + 2])
                else:
                    start_value = self.dictionaries.numeros_palabras[tokens[i + 2]]
                    
                if tokens[i + 4].isdigit():
                    end_value = int(tokens[i + 4])
                else:
                    end_value = self.dictionaries.numeros_palabras[tokens[i + 4]]
                
                # Crear TemporalFilter b√°sico
                basic_filter = TemporalFilter(
                    indicator="desde_a",
                    quantity=abs(end_value - start_value) + 1,
                    unit=unit,
                    confidence=0.95,
                    filter_type="range_between"
                )
                
                # Crear informaci√≥n avanzada
                advanced_info = AdvancedTemporalInfo(
                    original_filter=basic_filter,
                    is_range_between=True,
                    start_value=start_value,
                    end_value=end_value,
                    raw_tokens=tokens[i:i+5]
                )
                
                temporal_filters.append(basic_filter)
                advanced_temporal_info.append(advanced_info)
                
                print(f"   ‚úÖ PATR√ìN 'DESDE_A': desde {tokens[i + 1]} {start_value} a {end_value}")
                i += 5  # Avanzar 5 tokens
                continue
            
# PATR√ìN EXISTENTE 1: "entre [UNIDAD] [N√öMERO] y [N√öMERO]" - entre semana 5 y 9
            if (i < len(tokens) - 4 and
                tokens[i].lower() == 'entre' and
                tokens[i + 1].lower() in self.dictionaries.unidades_tiempo and
                (tokens[i + 2].isdigit() or tokens[i + 2] in self.dictionaries.numeros_palabras) and
                tokens[i + 3].lower() == 'y' and
                (tokens[i + 4].isdigit() or tokens[i + 4] in self.dictionaries.numeros_palabras)):
                
                unit = self.dictionaries.unidades_tiempo[tokens[i + 1].lower()]
                
                if tokens[i + 2].isdigit():
                    start_value = int(tokens[i + 2])
                else:
                    start_value = self.dictionaries.numeros_palabras[tokens[i + 2]]
                    
                if tokens[i + 4].isdigit():
                    end_value = int(tokens[i + 4])
                else:
                    end_value = self.dictionaries.numeros_palabras[tokens[i + 4]]
                
                # Crear TemporalFilter b√°sico
                basic_filter = TemporalFilter(
                    indicator="entre_y",
                    quantity=abs(end_value - start_value) + 1,
                    unit=unit,
                    confidence=0.95,
                    filter_type="range_between"
                )
                
                # Crear informaci√≥n avanzada
                advanced_info = AdvancedTemporalInfo(
                    original_filter=basic_filter,
                    is_range_between=True,
                    start_value=start_value,
                    end_value=end_value,
                    raw_tokens=tokens[i:i+5]
                )
                
                temporal_filters.append(basic_filter)
                advanced_temporal_info.append(advanced_info)
                
                print(f"   ‚úÖ PATR√ìN 'ENTRE_Y': entre {tokens[i + 1]} {start_value} y {end_value}")
                i += 5
                continue
            
            # üîß PATR√ìN EXISTENTE 2: "de [UNIDAD] [N√öMERO] a [N√öMERO]" - de semana 8 a 4  
            if (i < len(tokens) - 4 and
                tokens[i].lower() == 'de' and
                tokens[i + 1].lower() in self.dictionaries.unidades_tiempo and
                (tokens[i + 2].isdigit() or tokens[i + 2] in self.dictionaries.numeros_palabras) and
                tokens[i + 3].lower() == 'a' and
                (tokens[i + 4].isdigit() or tokens[i + 4] in self.dictionaries.numeros_palabras)):
                
                unit = self.dictionaries.unidades_tiempo[tokens[i + 1].lower()]
                
                if tokens[i + 2].isdigit():
                    start_value = int(tokens[i + 2])
                else:
                    start_value = self.dictionaries.numeros_palabras[tokens[i + 2]]
                    
                if tokens[i + 4].isdigit():
                    end_value = int(tokens[i + 4])
                else:
                    end_value = self.dictionaries.numeros_palabras[tokens[i + 4]]
                
                # Crear TemporalFilter b√°sico
                basic_filter = TemporalFilter(
                    indicator="de_a",
                    quantity=abs(end_value - start_value) + 1,
                    unit=unit,
                    confidence=0.95,
                    filter_type="range_between"
                )
                
                # Crear informaci√≥n avanzada
                advanced_info = AdvancedTemporalInfo(
                    original_filter=basic_filter,
                    is_range_between=True,
                    start_value=start_value,
                    end_value=end_value,
                    raw_tokens=tokens[i:i+5]
                )
                
                temporal_filters.append(basic_filter)
                advanced_temporal_info.append(advanced_info)
                
                print(f"   ‚úÖ PATR√ìN 'DE_A': de {tokens[i + 1]} {start_value} a {end_value}")
                i += 5
                continue
            
#  PATR√ìN MODIFICADO: "desde [UNIDAD] [N√öMERO]" - desde semana 8 (SOLO si no es "desde...a")
            if (i < len(tokens) - 2 and
                tokens[i].lower() == 'desde' and
                tokens[i + 1].lower() in self.dictionaries.unidades_tiempo and
                (tokens[i + 2].isdigit() or tokens[i + 2] in self.dictionaries.numeros_palabras)):
                
                # üö® VERIFICACI√ìN CR√çTICA: ¬øEs realmente "desde X" o es "desde X a Y"?
                is_desde_a_pattern = False
                if i + 4 < len(tokens):
                    next_token = tokens[i + 3].lower()
                    fourth_token_is_number = (tokens[i + 4].isdigit() or tokens[i + 4] in self.dictionaries.numeros_palabras)
                    if next_token == 'a' and fourth_token_is_number:
                        is_desde_a_pattern = True
                        print(f"   üîç Detectado patr√≥n 'desde...a' - saltando procesamiento como 'desde' simple")
                
                # Solo procesar como "desde" simple si NO es "desde...a"
                if not is_desde_a_pattern:
                    unit = self.dictionaries.unidades_tiempo[tokens[i + 1].lower()]
                    
                    if tokens[i + 2].isdigit():
                        start_value = int(tokens[i + 2])
                    else:
                        start_value = self.dictionaries.numeros_palabras[tokens[i + 2]]
                    
                    # Crear TemporalFilter b√°sico
                    basic_filter = TemporalFilter(
                        indicator="desde",
                        quantity=start_value,
                        unit=unit,
                        confidence=0.95,
                        filter_type="range_from"
                    )
                    
                    # Crear informaci√≥n avanzada complementaria
                    advanced_info = AdvancedTemporalInfo(
                        original_filter=basic_filter,
                        is_range_from=True,
                        start_value=start_value,
                        raw_tokens=tokens[i:i+3]
                    )
                    
                    temporal_filters.append(basic_filter)
                    advanced_temporal_info.append(advanced_info)
                    
                    print(f"   ‚úÖ PATR√ìN 'DESDE' (simple): desde {tokens[i + 1]} {start_value}")
                    i += 3
                    continue
                
            # üîß PATR√ìN EXISTENTE: "hasta [UNIDAD] [N√öMERO]" - hasta semana 5
            if (i < len(tokens) - 2 and
                tokens[i].lower() == 'hasta' and
                tokens[i + 1].lower() in self.dictionaries.unidades_tiempo and
                (tokens[i + 2].isdigit() or tokens[i + 2] in self.dictionaries.numeros_palabras)):
                
                unit = self.dictionaries.unidades_tiempo[tokens[i + 1].lower()]
                
                if tokens[i + 2].isdigit():
                    end_value = int(tokens[i + 2])
                else:
                    end_value = self.dictionaries.numeros_palabras[tokens[i + 2]]
                
                # Crear TemporalFilter b√°sico
                basic_filter = TemporalFilter(
                    indicator="hasta",
                    quantity=end_value,
                    unit=unit,
                    confidence=0.95,
                    filter_type="range_to"
                )
                
                # Crear informaci√≥n avanzada
                advanced_info = AdvancedTemporalInfo(
                    original_filter=basic_filter,
                    is_range_to=True,
                    end_value=end_value,
                    raw_tokens=tokens[i:i+3]
                )
                
                temporal_filters.append(basic_filter)
                advanced_temporal_info.append(advanced_info)
                
                print(f"   ‚úÖ PATR√ìN 'HASTA': hasta {tokens[i + 1]} {end_value}")
                i += 3
                continue
            
            # üîß PATR√ìN EXISTENTE: [INDICADOR] [N√öMERO] [UNIDAD] - "ultimas 8 semanas"
            if (i < len(tokens) - 2 and
                tokens[i] in self.dictionaries.indicadores_temporales and
                (tokens[i + 1].isdigit() or tokens[i + 1] in self.dictionaries.numeros_palabras) and
                tokens[i + 2] in self.dictionaries.unidades_tiempo):
                
                indicator = self.dictionaries.indicadores_temporales[tokens[i]]
                
                if tokens[i + 1].isdigit():
                    quantity = int(tokens[i + 1])
                else:
                    quantity = self.dictionaries.numeros_palabras[tokens[i + 1]]
                
                unit = self.dictionaries.unidades_tiempo[tokens[i + 2]]
                
                basic_filter = TemporalFilter(
                    indicator=indicator,
                    quantity=quantity,
                    unit=unit,
                    confidence=0.95,
                    filter_type="range"
                )
                
                # Informaci√≥n b√°sica para mantener compatibilidad
                advanced_info = AdvancedTemporalInfo(
                    original_filter=basic_filter,
                    raw_tokens=tokens[i:i+3]
                )
                
                temporal_filters.append(basic_filter)
                advanced_temporal_info.append(advanced_info)
                
                print(f"   ‚è∞ Filtro temporal (rango): {indicator} {quantity} {unit.value}")
                i += 3
                continue
            
            # üîß PATR√ìN EXISTENTE: [UNIDAD] [N√öMERO] - "semana 8", "week 8"
            elif (i < len(tokens) - 1 and
                tokens[i] in self.dictionaries.unidades_tiempo and
                (tokens[i + 1].isdigit() or tokens[i + 1] in self.dictionaries.numeros_palabras)):
                
                unit = self.dictionaries.unidades_tiempo[tokens[i]]
                
                if tokens[i + 1].isdigit():
                    quantity = int(tokens[i + 1])
                else:
                    quantity = self.dictionaries.numeros_palabras[tokens[i + 1]]
                
                basic_filter = TemporalFilter(
                    indicator="espec√≠fica",
                    quantity=quantity,
                    unit=unit,
                    confidence=0.90,
                    filter_type="specific"
                )
                
                # Informaci√≥n b√°sica para mantener compatibilidad
                advanced_info = AdvancedTemporalInfo(
                    original_filter=basic_filter,
                    raw_tokens=tokens[i:i+2]
                )
                
                temporal_filters.append(basic_filter)
                advanced_temporal_info.append(advanced_info)
                
                print(f"   ‚è∞ Filtro temporal (espec√≠fico): {tokens[i]} {quantity}")
                i += 2
                continue
            
            i += 1
        
        # GUARDAR informaci√≥n avanzada para uso posterior
        self.advanced_temporal_info = advanced_temporal_info
        
        print(f"üîç TOTAL FILTROS TEMPORALES DETECTADOS: {len(temporal_filters)}")
        for i, tf in enumerate(temporal_filters, 1):
            print(f"   {i}. Tipo: {tf.filter_type}, Unidad: {tf.unit.value}")
        
        return temporal_filters


# ------  "Detector de pares Columna valor" -------

    def detect_column_value_patterns(self, tokens: List[str], temporal_filters: List[TemporalFilter]) -> List[ColumnValuePair]:
        """Detector de Pares Columna-Valor - VERSI√ìN GEN√âRICA AMPLIADA"""
        print(f"üéØ DEBUG 3: Tokens recibidos: {tokens}")
        
        column_value_pairs = []
        
        # Identificar TODAS las columnas temporales (mantener l√≥gica existente)
        temporal_columns = set()
        for tf in temporal_filters:
            if tf.unit == TemporalUnit.WEEKS:
                temporal_columns.update(['semana', 'semanas', 'week', 'weeks'])
            elif tf.unit == TemporalUnit.MONTHS:
                temporal_columns.update(['mes', 'meses', 'month', 'months'])
            elif tf.unit == TemporalUnit.DAYS:
                temporal_columns.update(['dia', 'dias', 'day', 'days'])
            elif tf.unit == TemporalUnit.YEARS:
                temporal_columns.update(['a√±o', 'a√±os', 'year', 'years'])
        
        print(f"‚è∞ Columnas temporales a excluir: {temporal_columns}")
        
        i = 0
        while i < len(tokens) - 1:
            
            # üÜï PATR√ìN 1: [preposici√≥n] [columna] [valor] (ej: "de sku QN55S90DAFXZX")
            if i < len(tokens) - 2:
                pattern_result = self._detect_preposition_column_value_pattern(tokens, i, temporal_columns)
                if pattern_result:
                    column_value_pairs.append(pattern_result['pair'])
                    print(f"‚úÖ DEBUG 5: FILTRO CREADO (preposici√≥n): {pattern_result['raw_text']}")
                    i += pattern_result['tokens_consumed']
                    continue
            
            # PATR√ìN ORIGINAL: [columna] [valor] (mantener l√≥gica existente)
            current_token = tokens[i]
            next_token = tokens[i + 1]
            
            print(f"üîç DEBUG 4: Analizando '{current_token}' + '{next_token}'")
            
            column_info = self._identify_potential_column(current_token)
            
            print(f"     Columna? {column_info}")
            
            if column_info['is_column']:
                if column_info['normalized_name'] in temporal_columns:
                    print(f"‚è∞ Saltando '{current_token}' - ya procesado como temporal")
                    i += 1
                    continue
                
                value_info = self._identify_potential_value(next_token, i + 1, tokens)
                
                print(f"     Valor? {value_info}")
                
                if value_info['is_value']:
                    column_value_pairs.append(ColumnValuePair(
                        column_name=column_info['normalized_name'],
                        value=value_info['normalized_value'], 
                        confidence=min(column_info['confidence'], value_info['confidence']),
                        raw_text=f"{current_token} {next_token}"
                    ))
                    
                    print(f"‚úÖ DEBUG 5: FILTRO CREADO: {current_token} = '{next_token}'")
                    
                    i += 2
                    continue
            
            i += 1
        
        print(f"üéØ DEBUG 6: Total filtros detectados: {len(column_value_pairs)}")
        
        return column_value_pairs


    # üÜï M√âTODO AUXILIAR GEN√âRICO: Detectar patrones con preposiciones
    def _detect_preposition_column_value_pattern(self, tokens: List[str], start_idx: int, temporal_columns: set) -> Optional[Dict]:
        """
        Detecta patrones gen√©ricos: [preposici√≥n] [columna] [valor]
        
        Args:
            tokens: Lista completa de tokens
            start_idx: √çndice donde empezar a buscar
            temporal_columns: Columnas temporales a excluir
        
        Returns:
            Dict con 'pair', 'tokens_consumed', 'raw_text' o None
        """
        
        if start_idx + 2 >= len(tokens):
            return None
        
        preposition_token = tokens[start_idx]
        column_token = tokens[start_idx + 1] 
        value_token = tokens[start_idx + 2]
        
        # üîß PREPOSICIONES GEN√âRICAS (usando conectores del diccionario + espec√≠ficas)
        common_prepositions = {'de', 'en', 'para', 'con', 'desde', 'por'}
        # Agregar conectores del diccionario que puedan ser preposiciones
        all_prepositions = common_prepositions.union(
            {conn for conn in self.dictionaries.conectores if conn in common_prepositions}
        )
        
        if preposition_token.lower() not in all_prepositions:
            return None
        
        print(f"üîç DEBUG 4.1: Analizando patr√≥n preposici√≥n: '{preposition_token}' + '{column_token}' + '{value_token}'")
        
        # Verificar si es columna v√°lida
        column_info = self._identify_potential_column(column_token)
        print(f"     Columna? {column_info}")
        
        if not column_info['is_column']:
            return None
        
        # Excluir columnas temporales
        if column_info['normalized_name'] in temporal_columns:
            print(f"‚è∞ Saltando '{column_token}' - ya procesado como temporal")
            return None
        
        # Verificar si es valor v√°lido
        value_info = self._identify_potential_value(value_token, start_idx + 2, tokens)
        print(f"     Valor? {value_info}")
        
        if not value_info['is_value']:
            return None
        
        # üÜï AJUSTE DE CONFIANZA: Reducir ligeramente por ser patr√≥n indirecto
        confidence_adjustment = 0.95  # 5% de reducci√≥n por indirecci√≥n
        final_confidence = min(column_info['confidence'], value_info['confidence']) * confidence_adjustment
        
        # Crear par columna-valor
        pair = ColumnValuePair(
            column_name=column_info['normalized_name'],
            value=value_info['normalized_value'],
            confidence=final_confidence,
            raw_text=f"{preposition_token} {column_token} {value_token}"
        )
        
        return {
            'pair': pair,
            'tokens_consumed': 3,  # preposici√≥n + columna + valor
            'raw_text': f"{preposition_token} {column_token} = '{value_token}'"
        }


# ------  "Identificador de columnas potenciales" -------

    def _identify_potential_column(self, token: str) -> Dict:
        """Identificador de Columnas Potenciales"""
        token_lower = token.lower()
        
        if token_lower in self.dictionaries.dimensiones:
            return {
                'is_column': True,
                'normalized_name': token_lower,
                'type': 'dimension',
                'confidence': 0.95
            }
        
        if token_lower in self.dictionaries.metricas:
            return {
                'is_column': True,
                'normalized_name': token_lower,
                'type': 'metric',
                'confidence': 0.90
            }
        
        if token_lower in self.dictionaries.frases_compuestas:
            normalized = self.dictionaries.frases_compuestas[token_lower]
            return {
                'is_column': True,
                'normalized_name': normalized,
                'type': 'compound',
                'confidence': 0.95
            }
        
        if self._looks_like_column_name(token):
            return {
                'is_column': True,
                'normalized_name': token_lower,
                'type': 'inferred',
                'confidence': 0.70
            }
        
        return {
            'is_column': False,
            'normalized_name': None,
            'type': None,
            'confidence': 0.0
        }


# ------  "Identificador de valores especificos" -------

    def _identify_potential_value(self, token: str, position: int, tokens: List[str]) -> Dict:
        """Identificador de Valores Espec√≠ficos - VERSI√ìN GEN√âRICA MEJORADA"""
        
        # PRIORIDAD M√ÅXIMA: Letras individuales may√∫sculas (mantener l√≥gica existente)
        if len(token) == 1 and token.isupper() and token.isalpha():
            return {
                'is_value': True,
                'normalized_value': token,
                'confidence': 0.98
            }
        
        token_lower = token.lower()
        token_upper = token.upper()
        
        # DESCARTAR: Palabras del lenguaje natural usando diccionarios existentes
        language_words = self.dictionaries.conectores.union({
            'entre', 'desde', 'hasta', 'con'  # Solo conectores temporales/contextuales
        })
        
        if token_lower in language_words and token != 'Y':
            return {'is_value': False, 'normalized_value': None, 'confidence': 0.0}
        
        # DESCARTAR: Usar diccionarios existentes para operaciones y m√©tricas
        if token_lower in self.dictionaries.operaciones:
            return {'is_value': False, 'normalized_value': None, 'confidence': 0.0}
        
        if token_lower in self.dictionaries.metricas:
            return {'is_value': False, 'normalized_value': None, 'confidence': 0.0}
        
        if token_lower in self.dictionaries.dimensiones:
            return {'is_value': False, 'normalized_value': None, 'confidence': 0.0}

        # REGLA GEN√âRICA: C√≥digos alfanum√©ricos largos (sin patrones espec√≠ficos)
        if self._is_generic_code_value(token):
            context_confidence = self._calculate_generic_context_confidence(token, position, tokens)
            return {
                'is_value': True,
                'normalized_value': token_upper,
                'confidence': context_confidence
            }

        # REGLAS EXISTENTES (mantener intactas)
        if len(token) == 1 and token.isalpha():
            return {
                'is_value': True,
                'normalized_value': token_upper,
                'confidence': 0.90
            }
        
        if token.isdigit():
            return {
                'is_value': True,
                'normalized_value': token,
                'confidence': 0.95
            }
        
        # üîß REGLA EXPANDIDA: C√≥digos alfanum√©ricos cortos/medianos
        if re.match(r'^[A-Za-z0-9\-/\.]+$', token) and 2 <= len(token) <= 30:
            context_confidence = self._calculate_generic_context_confidence(token, position, tokens)
            return {
                'is_value': True,
                'normalized_value': token_upper,
                'confidence': context_confidence
            }
        
        # REGLAS EXISTENTES para estados comunes (mantener)
        common_states = {
            'activo', 'inactivo', 'pendiente', 'completado', 'cancelado',
            'si', 'no', 'yes', 'true', 'false', 'on', 'off',
            'alto', 'medio', 'bajo', 'premium', 'basico', 'vip'
        }
        if token_lower in common_states:
            return {
                'is_value': True,
                'normalized_value': token_upper,
                'confidence': 0.85
            }
        
        return {'is_value': False, 'normalized_value': None, 'confidence': 0.0}



    # M√âTODO AUXILIAR GEN√âRICO: Detectar c√≥digos sin patrones espec√≠ficos
    def _is_generic_code_value(self, token: str) -> bool:
        """Detecta si un token parece un c√≥digo/valor gen√©rico usando reglas universales"""
        
        # REGLA 1: Debe ser alfanum√©rico (puede incluir guiones, puntos, barras)
        if not re.match(r'^[A-Za-z0-9\-/\.]+$', token):
            return False
        
        # REGLA 2: Longitud m√≠nima para ser considerado c√≥digo
        if len(token) < 3:
            return False
        
        # REGLA 3: Debe tener al menos una letra Y un n√∫mero (caracter√≠stica de c√≥digos)
        has_letter = any(c.isalpha() for c in token)
        has_number = any(c.isdigit() for c in token)
        
        if has_letter and has_number:
            return True
        
        # REGLA 4: Solo letras pero de longitud significativa (ej: c√≥digos de pa√≠s, estados)
        if has_letter and not has_number and len(token) >= 2:
            return True
        
        # REGLA 5: Solo n√∫meros pero de longitud significativa (ej: c√≥digos num√©ricos)
        if has_number and not has_letter and len(token) >= 4:
            return True
        
        return False


    # M√âTODO AUXILIAR GEN√âRICO: Confianza basada en contexto usando diccionarios
    def _calculate_generic_context_confidence(self, token: str, position: int, tokens: List[str]) -> float:
        """Calcula confianza usando el contexto y los diccionarios existentes"""
        
        base_confidence = 0.70  # Confianza base para c√≥digos gen√©ricos
        
        # CONTEXTO +: Token anterior es una dimensi√≥n conocida (del diccionario)
        if position > 0:
            prev_token = tokens[position - 1].lower()
            if prev_token in self.dictionaries.dimensiones:
                base_confidence += 0.20  # Gran boost si est√° despu√©s de dimensi√≥n
                print(f"      üéØ Contexto dimensi√≥n: '{prev_token}' ‚Üí +0.20 confianza")
        
        # CONTEXTO +: Patr√≥n "de [DIMENSI√ìN] [VALOR]"
        if position >= 2:
            two_before = tokens[position - 2].lower()
            one_before = tokens[position - 1].lower()
            if two_before == 'de' and one_before in self.dictionaries.dimensiones:
                base_confidence += 0.15
                print(f"      üéØ Patr√≥n 'de dimensi√≥n valor': +0.15 confianza")
        
        # CONTEXTO +: Caracter√≠sticas del token
        # M√°s confianza para c√≥digos con buena mezcla alfanum√©rica
        has_letter = any(c.isalpha() for c in token)
        has_number = any(c.isdigit() for c in token)
        
        if has_letter and has_number:
            if 5 <= len(token) <= 15:  # Longitud t√≠pica de c√≥digos
                base_confidence += 0.10
            elif 3 <= len(token) <= 20:  # Rango m√°s amplio
                base_confidence += 0.05
        
        # CONTEXTO -: Penalizar si es demasiado largo (podr√≠a ser texto)
        if len(token) > 25:
            base_confidence -= 0.15
        
        # CONTEXTO +: Si contiene patrones t√≠picos de c√≥digos (sin ser espec√≠ficos)
        if any(char in token for char in ['-', '/', '.']):
            base_confidence += 0.05  # Separadores t√≠picos de c√≥digos
        
        return min(0.95, max(0.40, base_confidence))  # Entre 0.40 y 0.95


# ------  "Verificador de nombres de columna" -------

    def _looks_like_column_name(self, token: str) -> bool:
        """Verificador de Nombres de Columna"""
        if '_' in token:
            return True
        
        column_suffixes = ['_id', '_code', '_number', '_key', '_ref', '_name', '_type', '_status']
        if any(token.lower().endswith(suffix) for suffix in column_suffixes):
            return True
        
        column_prefixes = ['id_', 'code_', 'num_', 'ref_']
        if any(token.lower().startswith(prefix) for prefix in column_prefixes):
            return True
        
        return False



        # =================================================
        # GRUPO 3: AN√ÅLISIS SEM√ÅNTICO 
        # Clasificaci√≥n de componentes y an√°lisis sem√°ntico
        # =================================================



# ------  "Clasificador principal de tokens" -------

    def classify_all_components(self, tokens: List[str], column_value_pairs: List[ColumnValuePair]) -> Dict[str, QueryComponent]:
        """Clasificador Principal de Tokens"""
        classified = {}
        processed_tokens = set()
        
        # Marcar tokens procesados en pares columna-valor
        for cvp in column_value_pairs:
            pair_tokens = cvp.raw_text.split()
            processed_tokens.update(pair_tokens)
            print(f"üîó Filtro detectado: {cvp.column_name} = '{cvp.value}' (tokens: {pair_tokens})")
        
        # Clasificar tokens individuales
        for token in tokens:
            classified[token] = self.classify_single_component(token)
            
            if token in processed_tokens:
                classified[token].linguistic_info['used_in_filter'] = True
                print(f"üéØ Token '{token}' clasificado como {classified[token].type.value} (usado en filtro)")
            else:
                print(f"üîç Token '{token}' clasificado como {classified[token].type.value}")
        
        return classified


# ------  "Clasificador individual de tokens" -------

    def classify_single_component(self, token: str) -> QueryComponent:
        """Clasificador Individual de Tokens - VERSI√ìN MEJORADA"""
        
        # NUEVO: VERIFICACI√ìN ESPECIAL PARA INDICADORES DE RANKING
        ranking_indicators = {
            'top', 'mejores', 'mejore', 'mejor', 'primeros', 'primero', 
            'highest', 'best', 'm√°ximos', 'm√°ximo', 'worst', 'peores', 
            'peor', '√∫ltimos', '√∫ltimo', 'bottom', 'lowest', 'm√≠nimos', 'm√≠nimo'
        }
        
        if token.lower() in ranking_indicators:
            return QueryComponent(
                text=token,
                type=ComponentType.OPERATION,  # Cambiar de UNKNOWN a OPERATION
                confidence=0.90,
                subtype='ranking_indicator',
                value=token.lower(),
                linguistic_info={'source': 'ranking_indicator'}
            )
        
        # VERIFICACI√ìN TEMPRANA: Letras individuales may√∫sculas
        if len(token) == 1 and token.isupper() and token.isalpha():
            return QueryComponent(
                text=token,
                type=ComponentType.VALUE,
                confidence=0.98,
                subtype='letter',
                value=token,
                linguistic_info={'source': 'uppercase_letter_value'}
            )
        
        corrected_token = self.dictionaries.correct_typo(token)
        if corrected_token != token:
            corrected_component = self.classify_single_component(corrected_token)
            if corrected_component.type != ComponentType.UNKNOWN:
                corrected_component.linguistic_info = {
                    'source': 'typo_correction',
                    'original': token,
                    'corrected': corrected_token
                }
                corrected_component.confidence *= 0.85
                return corrected_component
        
        component_type = self.dictionaries.get_component_type(token)
        
        if component_type == ComponentType.DIMENSION:
            return QueryComponent(
                text=token,
                type=ComponentType.DIMENSION,
                confidence=0.95,
                linguistic_info={'source': 'dimension_dictionary'}
            )
        elif component_type == ComponentType.OPERATION:
            operation_type = self.dictionaries.get_operation_type(token)
            return QueryComponent(
                text=token,
                type=ComponentType.OPERATION,
                confidence=0.95,
                value=operation_type.value if operation_type else None,
                linguistic_info={'source': 'operation_dictionary'}
            )
        elif component_type == ComponentType.METRIC:
            return QueryComponent(
                text=token,
                type=ComponentType.METRIC,
                confidence=0.95,
                linguistic_info={'source': 'metric_dictionary'}
            )
        elif component_type == ComponentType.TEMPORAL:
            if token in self.dictionaries.indicadores_temporales:
                return QueryComponent(
                    text=token,
                    type=ComponentType.TEMPORAL,
                    confidence=0.9,
                    subtype='indicator',
                    value=self.dictionaries.indicadores_temporales[token],
                    linguistic_info={'source': 'temporal_dictionary'}
                )
            elif token in self.dictionaries.unidades_tiempo:
                return QueryComponent(
                    text=token,
                    type=ComponentType.TEMPORAL,
                    confidence=0.95,
                    subtype='unit',
                    value=self.dictionaries.unidades_tiempo[token],
                    linguistic_info={'source': 'temporal_dictionary'}
                )
        elif component_type == ComponentType.VALUE:
            if token.isdigit():
                return QueryComponent(
                    text=token,
                    type=ComponentType.VALUE,
                    confidence=0.95,
                    subtype='number',
                    value=int(token),
                    linguistic_info={'source': 'numeric_literal'}
                )
            elif token in self.dictionaries.numeros_palabras:
                return QueryComponent(
                    text=token,
                    type=ComponentType.VALUE,
                    confidence=0.9,
                    subtype='number',
                    value=self.dictionaries.numeros_palabras[token],
                    linguistic_info={'source': 'number_word'}
                )
        elif component_type == ComponentType.CONNECTOR:
            return QueryComponent(
                text=token,
                type=ComponentType.CONNECTOR,
                confidence=0.8,
                linguistic_info={'source': 'connector_dictionary'}
            )

    # Buscar en diccionario temporal antes de marcar como UNKNOWN
        temporal_entry = self.dictionaries.search_in_temporal_dictionary(token)
                    
        if temporal_entry:
            temporal_type = self.dictionaries.get_temporal_component_type(token)
            
            # üîß VERIFICACI√ìN: Confirmar que es VALUE
            print(f"   üóÑÔ∏è TEMPORAL CLASIFICADO: '{token}' ‚Üí {temporal_type.value}")
            
            return QueryComponent(
                text=token,
                type=temporal_type or ComponentType.VALUE,  # Fallback a VALUE
                confidence=temporal_entry.get('confidence', 0.9),
                subtype='temporal_data',
                value=temporal_entry.get('original_value'),
                column_name=temporal_entry.get('column_name'),
                linguistic_info={
                    'source': 'temporal_dictionary',
                    'original_value': temporal_entry.get('original_value'),
                    'column_name': temporal_entry.get('column_name'),
                    'column_type': temporal_entry.get('column_type'),
                    'forced_as_value': True  # üÜï Marcador para debugging
                }
            )
        
        # Si no est√° en temporal tampoco, entonces es UNKNOWN
        return QueryComponent(
            text=token,
            type=ComponentType.UNKNOWN,
            confidence=0.3,
            linguistic_info={'source': 'unknown'}
        )


# ------  "Detector de tipo de consulta" -------

    def detect_query_pattern(self, structure: QueryStructure) -> QueryPattern:
        """Detector de Tipo de Consulta - VERSI√ìN CORREGIDA PARA RANKINGS MULTI-DIMENSIONALES"""
        print(f"üîç DETECTANDO PATR√ìN DE CONSULTA:")
        print(f"   üìç Dimensi√≥n: {structure.main_dimension.text if structure.main_dimension else 'N/A'}")
        print(f"   üîó M√∫ltiples dimensiones: {len(structure.main_dimensions) if structure.main_dimensions else 0}")
        print(f"   ‚ö° Operaciones: {[op.text for op in structure.operations]}")
        print(f"   üìä M√©tricas: {[m.text for m in structure.metrics]}")
        print(f"   üéõÔ∏è Filtros: {len(structure.column_conditions)}")
        print(f"   ‚è∞ Filtros temporales: {len(structure.temporal_filters)}")
        print(f"   üîó Es compuesta: {structure.is_compound_query}")
        print(f"   üîó Criterios compuestos: {len(structure.compound_criteria)}")
        print(f"   üèÜ Es ranking: {structure.is_ranking_query}")
        print(f"   üìê Es multi-dimensional: {structure.is_multi_dimension_query}")
        
        # üîß PATR√ìN PRIORITARIO CORREGIDO: RANKING (incluyendo multi-dimensionales)
        if structure.is_ranking_query and structure.ranking_criteria:
            confidence = self.calculate_ranking_confidence(structure)
            if confidence >= 0.7:
                print(f"   üèÜ PATR√ìN DETECTADO: TOP_N (ranking con {len(structure.main_dimensions) if structure.main_dimensions else 1} dimensiones, confianza: {confidence:.2f})")
                structure.confidence_score = confidence
                return QueryPattern.TOP_N
        
        # PATR√ìN 2: M√öLTIPLES DIMENSIONES SIN RANKING
        if (structure.is_multi_dimension_query and 
            len(structure.main_dimensions) >= 2 and 
            not structure.is_ranking_query):
            confidence = self.calculate_multi_dimension_confidence(structure)
            if confidence >= 0.7:
                print(f"   üîó PATR√ìN DETECTADO: MULTI_DIMENSION ({len(structure.main_dimensions)} dimensiones sin ranking, confianza: {confidence:.2f})")
                structure.confidence_score = confidence
                return QueryPattern.MULTI_DIMENSION
        
        # PATR√ìN 3: CONSULTAS COMPUESTAS REFERENCIADAS
        if (structure.is_compound_query and 
            structure.main_dimension and 
            len(structure.compound_criteria) >= 2):
            
            all_reference_operations = True
            reference_operations = ['m√°ximo', 'm√≠nimo', 'mayor', 'menor']
            
            for criteria in structure.compound_criteria:
                if criteria.operation.value not in reference_operations:
                    all_reference_operations = False
                    break
            
            if all_reference_operations:
                confidence = self.calculate_compound_reference_confidence(structure)
                if confidence >= 0.7:
                    print(f"   üéØ PATR√ìN DETECTADO: REFERENCED (compuesta, confianza: {confidence:.2f})")
                    structure.confidence_score = confidence
                    return QueryPattern.REFERENCED
            
        # PATR√ìN 4: DATOS REFERENCIADOS SIMPLES
        if (structure.main_dimension and 
            len(structure.operations) >= 1 and 
            len(structure.metrics) >= 1 and 
            len(structure.column_conditions) == 0 and
            not structure.is_ranking_query):
            
            operation = structure.operations[0]
            reference_operations = ['m√°ximo', 'm√≠nimo', 'mayor', 'menor']
            
            if operation.value in reference_operations:
                confidence = self.calculate_reference_confidence(structure)
                if confidence >= 0.7:
                    print(f"   üéØ PATR√ìN DETECTADO: REFERENCED (simple, confianza: {confidence:.2f})")
                    structure.confidence_score = confidence
                    return QueryPattern.REFERENCED
        
        # PATR√ìN 5: AGREGACI√ìN COMPLETA
        if (len(structure.operations) >= 1 and 
            len(structure.metrics) >= 1 and 
            not structure.main_dimension):
            
            print(f"   üìä PATR√ìN DETECTADO: AGGREGATION (agregaci√≥n global)")
            structure.confidence_score = 0.90
            return QueryPattern.AGGREGATION
        
        # PATR√ìN 6: AGREGACI√ìN CON DIMENSI√ìN
        if (structure.main_dimension and 
            len(structure.operations) >= 1 and 
            len(structure.metrics) >= 1):
            
            print(f"   üìä PATR√ìN DETECTADO: AGGREGATION (con agrupaci√≥n)")
            structure.confidence_score = 0.85
            return QueryPattern.AGGREGATION
        
        # PATR√ìN 7: LISTAR TODOS
        if (structure.main_dimension and 
            len(structure.operations) == 0):
            
            print(f"   üìã PATR√ìN DETECTADO: LIST_ALL")
            structure.confidence_score = 0.80
            return QueryPattern.LIST_ALL
        
        # PATR√ìN 8: FILTRADO CON AGREGACI√ìN
        if len(structure.column_conditions) >= 1:
            print(f"   üéõÔ∏è PATR√ìN DETECTADO: AGGREGATION (con filtros)")
            structure.confidence_score = 0.75
            return QueryPattern.AGGREGATION
        
        print(f"   ‚ùì PATR√ìN DETECTADO: UNKNOWN (no se pudo determinar)")
        structure.confidence_score = 0.4
        return QueryPattern.UNKNOWN


# ==================================================
# ------  "Detector de consultas compuestas" -------
# ==================================================

    def detect_compound_criteria(self, tokens: List[str], classified_components: Dict) -> List[CompoundCriteria]:
        """Detector de Consultas Compuestas"""
        print(f"üîó DETECTANDO CRITERIOS COMPUESTOS:")
        print(f"   üî§ Tokens: {tokens}")
        
        compound_criteria = []
        
        segments = self.split_by_connector(tokens, 'y')
        
        print(f"   üìä Segmentos detectados: {segments}")
        
        for i, segment in enumerate(segments):
            print(f"\n   üéØ Procesando segmento {i+1}: {segment}")
            
            criteria = self.extract_criteria_from_segment(segment, classified_components)
            if criteria:
                compound_criteria.append(criteria)
                print(f"      ‚úÖ Criterio extra√≠do: {criteria.operation.text} {criteria.metric.text}")
            else:
                print(f"      ‚ùå No se pudo extraer criterio del segmento")
        
        print(f"\nüîó TOTAL CRITERIOS DETECTADOS: {len(compound_criteria)}")
        for i, criteria in enumerate(compound_criteria):
            print(f"   {i+1}. {criteria.operation.text} {criteria.metric.text} (confianza: {criteria.confidence:.2f})")
        
        return compound_criteria



# ------  "Divisor por conectores" -------

    def split_by_connector(self, tokens: List[str], connector: str) -> List[List[str]]:
        """Divisor por Conectores (Y, O)"""
        segments = []
        current_segment = []
        
        for token in tokens:
            if token.lower() == connector.lower():
                if current_segment:
                    segments.append(current_segment)
                    current_segment = []
            else:
                current_segment.append(token)
        
        if current_segment:
            segments.append(current_segment)
        
        return segments


# ------  "Extractor de criterios de segmentos" -------

    def extract_criteria_from_segment(self, segment: List[str], classified_components: Dict) -> Optional[CompoundCriteria]:
        """
        üîß Extractor de Criterios de Segmentos - VERSI√ìN CORREGIDA
        Prioriza m√©tricas reales antes de convertir dimensiones
        """
        operation_found = None
        metric_found = None
        dimension_candidate = None  # üÜï Guardar dimensi√≥n como candidato
        confidence_sum = 0.0
        count = 0
        
        print(f"      üîç Analizando segmento: {segment}")
        
        # üÜï PRIMERA PASADA: Buscar operaciones y m√©tricas REALES
        for token in segment:
            if token in classified_components:
                component = classified_components[token]
                
                # Buscar operaci√≥n
                if component.type == ComponentType.OPERATION and not operation_found:
                    operation_found = component
                    confidence_sum += component.confidence
                    count += 1
                    print(f"         ‚ö° Operaci√≥n encontrada: {token}")
                
                # üîß PRIORIDAD: M√©tricas reales PRIMERO
                elif component.type == ComponentType.METRIC and not metric_found:
                    metric_found = component
                    confidence_sum += component.confidence
                    count += 1
                    print(f"         üìä M√©trica REAL encontrada: {token}")
                
                # üÜï GUARDAR dimensi√≥n como candidato (NO convertir a√∫n)
                elif component.type == ComponentType.DIMENSION and not dimension_candidate:
                    dimension_candidate = component
                    print(f"         üìç Dimensi√≥n candidata: {token} (no convertida a√∫n)")
        
        # üÜï SEGUNDA PASADA: Solo si NO hay m√©trica real, usar dimensi√≥n
        if not metric_found and dimension_candidate:
            metric_component = QueryComponent(
                text=dimension_candidate.text,
                type=ComponentType.METRIC,
                confidence=dimension_candidate.confidence * 0.85,
                subtype='converted_from_dimension',
                value=dimension_candidate.value,
                column_name=dimension_candidate.column_name,
                linguistic_info={'converted_from': 'dimension'}
            )
            metric_found = metric_component
            confidence_sum += metric_component.confidence
            count += 1
            print(f"         üîÑ Dimensi√≥n convertida a m√©trica (fallback): {dimension_candidate.text}")
        
        # üîß VALIDACI√ìN FINAL
        if operation_found and metric_found:
            avg_confidence = confidence_sum / count if count > 0 else 0.0
            
            print(f"         ‚úÖ Criterio completo: {operation_found.text} + {metric_found.text}")
            
            return CompoundCriteria(
                operation=operation_found,
                metric=metric_found,
                confidence=avg_confidence,
                raw_tokens=segment
            )
        
        # üö® DIAGN√ìSTICO DE ERROR
        print(f"         ‚ùå Criterio incompleto:")
        print(f"             Operaci√≥n: {operation_found.text if operation_found else 'NO ENCONTRADA'}")
        print(f"             M√©trica: {metric_found.text if metric_found else 'NO ENCONTRADA'}")
        print(f"             Dimensi√≥n candidata: {dimension_candidate.text if dimension_candidate else 'NO ENCONTRADA'}")
        
        return None


# ------  "Verificador de consultas compuestas" -------

    def is_compound_query(self, compound_criteria: List[CompoundCriteria]) -> bool:
        """Verificador de Consulta Compuesta"""
        valid_criteria = [c for c in compound_criteria if c.confidence >= 0.6]
        
        is_compound = len(valid_criteria) >= 2
        
        print(f"üîó EVALUANDO SI ES CONSULTA COMPUESTA:")
        print(f"   üìä Criterios v√°lidos: {len(valid_criteria)}")
        print(f"   üéØ Es compuesta: {is_compound}")
        
        return is_compound


# ------  "Detector de criterios de ranking" -------

    # def detect_ranking_criteria(self, tokens: List[str], classified_components: Dict) -> Optional[RankingCriteria]:
    #     """Detector de Criterios de Ranking"""
    #     print(f"üèÜ DETECTANDO CRITERIOS DE RANKING:")
    #     print(f"   üî§ Tokens: {tokens}")
        
    #     top_indicators = {
    #         'top', 'mejores', 'mejore', 'mejor', 'primeros', 'primero', 'highest', 'best', 'm√°ximos', 'm√°ximo'
    #     }
        
    #     bottom_indicators = {
    #         'worst', 'peores', 'peor', '√∫ltimos', '√∫ltimo', 'bottom', 'lowest', 'm√≠nimos', 'm√≠nimo'
    #     }
        
    #     ranking_direction = None
    #     ranking_start_idx = -1
        
    #     for i, token in enumerate(tokens):
    #         token_lower = token.lower()
            
    #         if token_lower in top_indicators:
    #             ranking_direction = RankingDirection.TOP
    #             ranking_start_idx = i
    #             print(f"   üîù Indicador TOP encontrado: '{token}' en posici√≥n {i}")
    #             break
    #         elif token_lower in bottom_indicators:
    #             ranking_direction = RankingDirection.BOTTOM
    #             ranking_start_idx = i
    #             print(f"   üìâ Indicador BOTTOM encontrado: '{token}' en posici√≥n {i}")
    #             break
        
    #     if not ranking_direction:
    #         print(f"   ‚ùå No se encontraron indicadores de ranking")
    #         return None
        
    #     ranking_value = None
    #     ranking_unit = None
    #     value_tokens = []
        
    #     search_end = min(ranking_start_idx + 4, len(tokens))
        
    #     for i in range(ranking_start_idx + 1, search_end):
    #         if i >= len(tokens):
    #             break
                
    #         token = tokens[i]
            
    #         if token.endswith('%'):
    #             try:
    #                 percent_value = float(token[:-1])
    #                 ranking_value = percent_value
    #                 ranking_unit = RankingUnit.PERCENTAGE
    #                 value_tokens.append(token)
    #                 print(f"   üìä Porcentaje detectado: {percent_value}%")
    #                 break
    #             except ValueError:
    #                 continue
            
    #         elif token.isdigit():
    #             ranking_value = int(token)
    #             ranking_unit = RankingUnit.COUNT
    #             value_tokens.append(token)
    #             print(f"   üî¢ N√∫mero detectado: {ranking_value}")
    #             break
            
    #         elif token.lower() in self.dictionaries.numeros_palabras:
    #             ranking_value = self.dictionaries.numeros_palabras[token.lower()]
    #             ranking_unit = RankingUnit.COUNT
    #             value_tokens.append(token)
    #             print(f"   üî§ N√∫mero en palabras detectado: {token} = {ranking_value}")
    #             break
        
    #     if ranking_value is None:
    #         print(f"   ‚ùå No se encontr√≥ valor num√©rico despu√©s del indicador")
    #         return None
        
    #     ranking_metric = None
    #     ranking_operation = None
        
    #     for token, component in classified_components.items():
    #         if component.type == ComponentType.METRIC and not ranking_metric:
    #             ranking_metric = component
    #             print(f"   üìä M√©trica de ranking: {component.text}")
    #         elif component.type == ComponentType.OPERATION and not ranking_operation:
    #             ranking_operation = component
    #             print(f"   ‚ö° Operaci√≥n de ranking: {component.text}")
        
    #     confidence_factors = []
    #     base_confidence = 0.5
        
    #     base_confidence += 0.3
    #     confidence_factors.append("indicador_ranking")
        
    #     base_confidence += 0.2
    #     confidence_factors.append("valor_num√©rico")
        
    #     if ranking_metric:
    #         base_confidence += 0.1
    #         confidence_factors.append("m√©trica_encontrada")
        
    #     if ranking_operation:
    #         base_confidence += 0.1
    #         confidence_factors.append("operaci√≥n_encontrada")
        
    #     final_confidence = min(1.0, base_confidence)
        
    #     raw_tokens = tokens[ranking_start_idx:ranking_start_idx + len(value_tokens) + 1]
        
    #     ranking_criteria = RankingCriteria(
    #         direction=ranking_direction,
    #         unit=ranking_unit,
    #         value=ranking_value,
    #         metric=ranking_metric,
    #         operation=ranking_operation,
    #         confidence=final_confidence,
    #         raw_tokens=raw_tokens
    #     )
        
    #     print(f"üèÜ CRITERIO DE RANKING DETECTADO:")
    #     print(f"   üéØ Direcci√≥n: {ranking_direction.value}")
    #     print(f"   üìä Unidad: {ranking_unit.value}")
    #     print(f"   üî¢ Valor: {ranking_value}")
    #     print(f"   üìà M√©trica: {ranking_metric.text if ranking_metric else 'N/A'}")
    #     print(f"   ‚ö° Operaci√≥n: {ranking_operation.text if ranking_operation else 'N/A'}")
    #     print(f"   ‚≠ê Confianza: {final_confidence:.2f}")
    #     print(f"   üî§ Tokens: {raw_tokens}")
        
    #     return ranking_criteria


# --- DETECTAR MULTIDIMENSIONES ---

    def detect_multi_dimensions(self, tokens: List[str], classified_components: Dict) -> List[QueryComponent]:
            """üîß DETECTOR GEN√âRICO DE M√öLTIPLES DIMENSIONES"""
            
            print(f"üîó DETECTANDO M√öLTIPLES DIMENSIONES:")
            
            # PASO 1: Identificar dimensiones y conectores
            dimension_candidates = []
            connector_positions = []
            
            for i, token in enumerate(tokens):
                if token in classified_components:
                    component = classified_components[token]
                    if component.type == ComponentType.DIMENSION:
                        dimension_candidates.append((i, component))
                    elif (component.type == ComponentType.CONNECTOR and 
                        token.lower() in ['y', 'and', ',']):
                        connector_positions.append(i)
            
            print(f"   üìç Dimensiones encontradas: {[(i, comp.text) for i, comp in dimension_candidates]}")
            print(f"   üîó Conectores en posiciones: {connector_positions}")
            
            # PASO 2: Validar patr√≥n secuencial
            if len(dimension_candidates) >= 2 and len(connector_positions) >= 1:
                valid_dimensions = self._validate_dimension_sequence(
                    dimension_candidates, connector_positions, tokens
                )
                
                if len(valid_dimensions) >= 2:
                    print(f"   ‚úÖ M√öLTIPLES DIMENSIONES v√°lidas: {[d.text for d in valid_dimensions]}")
                    return valid_dimensions
            
            print(f"   ‚ùå No se detect√≥ patr√≥n multi-dimensional v√°lido")
            return []
        
        
    def _is_complex_multi_dimensional_case(self, tokens: List[str], classified_components: Dict) -> bool:
        """üîí Detecta si es un caso complejo que necesita nueva l√≥gica - CONSERVADOR"""
        
        # CONDICI√ìN 1: Debe tener ranking (top/mejores)
        has_ranking = any(
            token.lower() in ['top', 'mejores', 'mejor', 'primeros'] 
            for token in tokens
        )
        
        # CONDICI√ìN 2: Debe tener m√∫ltiples dimensiones conectadas por 'y'
        dimension_count = sum(
            1 for comp in classified_components.values() 
            if comp.type == ComponentType.DIMENSION
        )
        has_connector_y = 'y' in [token.lower() for token in tokens]
        
        # CONDICI√ìN 3: Debe tener filtros temporales complejos
        has_complex_temporal = any(
            token.lower() in ['entre', 'desde'] 
            for token in tokens
        )
        
        # CONDICI√ìN 4: Verificar que NO sea un caso simple conocido
        is_simple_case = (
            dimension_count == 1 and 
            not has_complex_temporal and
            len(tokens) <= 6
        )
        
        # SOLO aplicar nueva l√≥gica si TODAS las condiciones se cumplen Y no es simple
        is_complex_case = (
            has_ranking and 
            dimension_count >= 2 and 
            has_connector_y and 
            has_complex_temporal and
            not is_simple_case
        )
        
        print(f"üîç ¬øEs caso complejo multi-dimensional? {is_complex_case}")
        print(f"   üìä Ranking: {has_ranking}, Dims: {dimension_count}, Conector: {has_connector_y}")
        print(f"   ‚è∞ Temporal complejo: {has_complex_temporal}, Simple: {is_simple_case}")
        
        return is_complex_case


    def _validate_compatibility_requirements(self, structure: QueryStructure) -> bool:
        """üîí Valida que nueva l√≥gica sea realmente necesaria"""
        
        # Evitar nueva l√≥gica para casos que ya funcionan bien
        simple_patterns = [
            structure.query_pattern == QueryPattern.AGGREGATION and len(structure.column_conditions) <= 1,
            structure.query_pattern == QueryPattern.REFERENCED and not structure.is_multi_dimension_query,
            len(structure.operations) == 1 and len(structure.metrics) == 1 and not structure.is_ranking_query
        ]
        
        if any(simple_patterns):
            print(f"üîí Caso simple detectado - mantener l√≥gica original")
            return False
        
        return True
            
    
        
    def _validate_dimension_sequence(self, dimension_candidates: List, connector_positions: List, tokens: List[str]) -> List[QueryComponent]:
        """Validador de secuencia dimensional"""
        valid_dimensions = []
        
        # REGLA: dim1 + conector + dim2 [+ conector + dim3...]
        for i, (pos, component) in enumerate(dimension_candidates):
            if i == 0:
                # Primera dimensi√≥n siempre v√°lida
                valid_dimensions.append(component)
            else:
                # Verificar que hay conector antes de esta dimensi√≥n
                prev_dim_pos = dimension_candidates[i-1][0]
                has_connector_between = any(
                    prev_dim_pos < conn_pos < pos 
                    for conn_pos in connector_positions
                )
                
                if has_connector_between:
                    valid_dimensions.append(component)
                    print(f"      ‚úÖ '{component.text}' v√°lida (conector encontrado)")
                else:
                    print(f"      ‚ùå '{component.text}' inv√°lida (sin conector)")
                    break
        
        return valid_dimensions



        # ====================================
        # GRUPO 4: CONSTRUCCI√ìN DE ESTRUCTURA 
        # construcci√≥n de estructura principal
        # ====================================


# ------  "Conatructor principal de estructura" -------

    def build_unified_structure(self, classified_components: Dict, column_value_pairs: List[ColumnValuePair], temporal_filters: List[TemporalFilter], tokens: List[str]) -> QueryStructure:
        """Constructor Principal de Estructura - VERSI√ìN CORREGIDA PARA MULTI-DIMENSIONES"""
        
        # PASO 0: Calcular columnas temporales
        temporal_columns = set()
        for tf in temporal_filters:
            if tf.unit == TemporalUnit.WEEKS:
                temporal_columns.update(['semana', 'semanas', 'week', 'weeks'])
            elif tf.unit == TemporalUnit.MONTHS:
                temporal_columns.update(['mes', 'meses', 'month', 'months'])
            elif tf.unit == TemporalUnit.DAYS:
                temporal_columns.update(['dia', 'dias', 'day', 'days'])
            elif tf.unit == TemporalUnit.YEARS:
                temporal_columns.update(['a√±o', 'a√±os', 'year', 'years'])
        
        print(f"‚è∞ Columnas temporales calculadas: {temporal_columns}")
        
        main_dimension = None
        operations = []
        metrics = []
        values = []
        connectors = []
        unknown_tokens = []
        
        print(f"üîç DEBUG: Buscando dimensi√≥n principal...")
        
        # PASO 1: Detectar rankings y exclusiones primero
        ranking_criteria = self.detect_ranking_criteria(tokens, classified_components)
        exclusion_filters = self.detect_exclusion_filters(tokens, classified_components)
        is_ranking = self.is_ranking_query(ranking_criteria, exclusion_filters)
        
        # PASO 1.2: Detectar m√∫ltiples dimensiones
        multi_dimensions = self.detect_multi_dimensions(tokens, classified_components)
        is_multi_dimension = len(multi_dimensions) >= 2
        
        # PASO 1.5: Solo SI NO es ranking, procesar otros patrones
        if not is_ranking:
            compound_criteria = self.detect_compound_criteria(tokens, classified_components)
            is_compound = self.is_compound_query(compound_criteria)
        else:
            compound_criteria = []
            is_compound = False
        
        # PASO 2: Recopilar todas las dimensiones candidatas (con filtro temporal mejorado)
        dimension_candidates = []
        for token, component in classified_components.items():
            
            # Exclusi√≥n temporal espec√≠fica para rankings
            if (is_ranking and 
                component.type == ComponentType.DIMENSION and
                component.text.lower() in temporal_columns):
                print(f"üèÜ‚è∞ Excluyendo '{component.text}' en contexto de ranking")
                continue
            
            if component.type == ComponentType.DIMENSION:
                
                # Usar la versi√≥n mejorada de exclusi√≥n temporal
                if self.should_exclude_temporal_dimension_enhanced(component, temporal_filters, is_ranking):
                    continue
                    
                dimension_candidates.append((token, component))
                print(f"   üìç Candidato v√°lido: '{component.text}' (tipo: {component.type.value})")
        
        # PASO 3: Construir estructura temporal para verificar agregaci√≥n global
        temp_structure = QueryStructure(
            main_dimension=None,
            operations=[comp for comp in classified_components.values() if comp.type == ComponentType.OPERATION],
            metrics=[comp for comp in classified_components.values() if comp.type == ComponentType.METRIC],
            column_conditions=column_value_pairs,
            temporal_filters=temporal_filters,
            values=[comp for comp in classified_components.values() if comp.type == ComponentType.VALUE],
            connectors=[comp for comp in classified_components.values() if comp.type == ComponentType.CONNECTOR],
            unknown_tokens=[comp for comp in classified_components.values() if comp.type == ComponentType.UNKNOWN]
        )
        
        # PASO 4: Verificar agregaci√≥n global
        available_dimension_components = [candidate[1] for candidate in dimension_candidates]
        
        if self.is_global_aggregation_query(temp_structure, available_dimension_components):
            print(f"üåê Consulta identificada como AGREGACI√ìN GLOBAL - sin dimensi√≥n principal")
            main_dimension = None
        else:
            
            # PASO 5: Determinar dimensi√≥n principal
            if dimension_candidates:
                if len(dimension_candidates) == 1:
                    main_dimension = dimension_candidates[0][1]
                    print(f"‚úÖ Dimensi√≥n √∫nica: '{main_dimension.text}'")
                else:
                    print(f"ü§î M√∫ltiples dimensiones detectadas: {[d[1].text for d in dimension_candidates]}")
                    
                    # üîß NUEVA L√ìGICA: Para m√∫ltiples dimensiones, usar la primera como principal
                    # y NO convertir las otras a m√©tricas si es ranking multi-dimensional
                    if is_multi_dimension and is_ranking:
                        main_dimension = dimension_candidates[0][1]
                        print(f"‚úÖ RANKING MULTI-DIMENSIONAL: Primera dimensi√≥n como principal: '{main_dimension.text}'")
                        print(f"üîó Manteniendo otras dimensiones en main_dimensions (NO convertir)")
                        
                    else:
                        # Aplicar heur√≠sticas existentes para casos NO multi-dimensionales
                        dimensions_not_in_filters = []
                        dimensions_in_filters = []
                        
                        for token, dimension in dimension_candidates:
                            has_filter = any(cvp.column_name == dimension.text for cvp in column_value_pairs)
                            has_exclusion = any(ef.column_name == dimension.text for ef in exclusion_filters)
                            
                            if has_filter or has_exclusion:
                                dimensions_in_filters.append((token, dimension))
                                print(f"   üéõÔ∏è '{dimension.text}' tiene filtro o exclusi√≥n asociada")
                            else:
                                dimensions_not_in_filters.append((token, dimension))
                                print(f"   üìç '{dimension.text}' NO tiene filtro ni exclusi√≥n")
                        
                        if dimensions_not_in_filters:
                            main_dimension = dimensions_not_in_filters[0][1]
                            print(f"‚úÖ Dimensi√≥n principal (sin filtro): '{main_dimension.text}'")
                            
                            # Solo convertir si NO es ranking multi-dimensional
                            for i in range(1, len(dimensions_not_in_filters)):
                                _, remaining_dimension = dimensions_not_in_filters[i]
                                
                                # Verificar si esta dimensi√≥n ya est√° en criterios compuestos
                                used_in_compound = any(
                                    criteria.metric.text == remaining_dimension.text 
                                    for criteria in compound_criteria
                                )
                                
                                # Verificar si est√° en criterios de ranking
                                used_in_ranking = (ranking_criteria and 
                                                ranking_criteria.metric and 
                                                ranking_criteria.metric.text == remaining_dimension.text)
                                
                                if not used_in_compound and not used_in_ranking and not is_multi_dimension:
                                    # Solo convertir si NO es multi-dimensional
                                    metric_component = QueryComponent(
                                        text=remaining_dimension.text,
                                        type=ComponentType.METRIC,
                                        confidence=remaining_dimension.confidence * 0.9,
                                        subtype='converted_from_dimension',
                                        value=remaining_dimension.value,
                                        column_name=remaining_dimension.column_name,
                                        linguistic_info={'converted_from': 'dimension', 'original_type': 'dimension'}
                                    )
                                    metrics.append(metric_component)
                                    print(f"üîÑ Convirtiendo '{remaining_dimension.text}' de dimensi√≥n a m√©trica")
                                else:
                                    print(f"üîó Manteniendo '{remaining_dimension.text}' como dimensi√≥n (multi-dimensional o en uso)")
                            
                        elif dimensions_in_filters:
                            main_dimension = dimensions_in_filters[0][1]
                            print(f"‚úÖ Dimensi√≥n principal (con filtro): '{main_dimension.text}'")

        # PASO 6: Separar resto de componentes (üîß L√ìGICA CORREGIDA)
        for token, component in classified_components.items():
            
            # üîß NUEVO: NO auto-convertir dimensiones si es ranking multi-dimensional
            if component.type == ComponentType.DIMENSION:
                is_main_dimension = (main_dimension and component.text == main_dimension.text)
                is_in_multi_dimensions = any(dim.text == component.text for dim in multi_dimensions)
                
                # Si es ranking multi-dimensional, NO convertir ninguna dimensi√≥n
                if is_ranking and is_multi_dimension and is_in_multi_dimensions:
                    print(f"üîó MANTENIENDO '{component.text}' como dimensi√≥n (ranking multi-dimensional)")
                    continue
                    
                # L√≥gica original para otros casos
                existing_real_metrics = [
                    comp for comp in classified_components.values() 
                    if comp.type == ComponentType.METRIC and 
                    not comp.linguistic_info.get('converted_from') == 'dimension'
                ]
                
                has_real_metrics = len(existing_real_metrics) > 0
                
                if not is_main_dimension and not has_real_metrics and not is_multi_dimension:
                    # Solo auto-convertir si NO es multi-dimensional
                    metric_component = QueryComponent(
                        text=component.text,
                        type=ComponentType.METRIC,
                        confidence=component.confidence * 0.85,
                        subtype='converted_from_dimension',
                        value=component.value,
                        column_name=component.column_name,
                        linguistic_info={'converted_from': 'dimension', 'original_type': 'dimension'}
                    )
                    metrics.append(metric_component)
                    print(f"üîÑ Auto-convirtiendo '{component.text}' de dimensi√≥n secundaria a m√©trica")
                else:
                    print(f"üõë NO auto-convertir '{component.text}': es_principal={is_main_dimension}, hay_m√©tricas_reales={has_real_metrics}, multi_dim={is_multi_dimension}")
                    
            elif component.type == ComponentType.OPERATION:
                operations.append(component)
            elif component.type == ComponentType.METRIC:
                metrics.append(component)
                print(f"‚úÖ M√©trica real detectada: '{component.text}'")

        # PASO 7: Para consultas compuestas, extraer operaciones y m√©tricas de criterios
        if is_compound:
            print(f"üîó PROCESANDO CONSULTA COMPUESTA:")
            for criteria in compound_criteria:
                # Agregar operaciones y m√©tricas desde criterios compuestos
                if not any(op.text == criteria.operation.text for op in operations):
                    operations.append(criteria.operation)
                    print(f"   ‚ö° Agregando operaci√≥n desde criterio: {criteria.operation.text}")
                
                if not any(m.text == criteria.metric.text for m in metrics):
                    metrics.append(criteria.metric)
                    print(f"   üìä Agregando m√©trica desde criterio: {criteria.metric.text}")
        
        # PASO 8: Para consultas de ranking, extraer operaciones y m√©tricas de criterios
        if is_ranking and ranking_criteria:
            print(f"üèÜ PROCESANDO CONSULTA DE RANKING:")
            
            if ranking_criteria.operation and not any(op.text == ranking_criteria.operation.text for op in operations):
                operations.append(ranking_criteria.operation)
                print(f"   ‚ö° Agregando operaci√≥n desde ranking: {ranking_criteria.operation.text}")
            
            if ranking_criteria.metric and not any(m.text == ranking_criteria.metric.text for m in metrics):
                metrics.append(ranking_criteria.metric)
                print(f"   üìä Agregando m√©trica desde ranking: {ranking_criteria.metric.text}")
        
        # PASO 9: Construir estructura final
        structure = QueryStructure(
            main_dimension=main_dimension,
            main_dimensions=multi_dimensions if is_multi_dimension else ([main_dimension] if main_dimension else []),  
            is_multi_dimension_query=is_multi_dimension,  
            operations=operations,
            metrics=metrics,
            column_conditions=column_value_pairs,
            temporal_filters=temporal_filters,
            values=values,
            connectors=connectors,
            unknown_tokens=unknown_tokens,
            compound_criteria=compound_criteria,
            is_compound_query=is_compound,
            ranking_criteria=ranking_criteria,
            exclusion_filters=exclusion_filters,
            is_ranking_query=is_ranking
        )
        
        # DETECTAR PATR√ìN DE CONSULTA
        query_pattern = self.detect_query_pattern(structure)
        structure.query_pattern = query_pattern
        
        # CONFIGURAR L√çMITES seg√∫n el tipo de consulta
        if query_pattern == QueryPattern.TOP_N and structure.ranking_criteria:
            if structure.ranking_criteria.unit == RankingUnit.COUNT:
                structure.limit_value = int(structure.ranking_criteria.value)
            elif structure.ranking_criteria.unit == RankingUnit.PERCENTAGE:
                structure.limit_value = None  # Se calcular√° en tiempo de ejecuci√≥n
            structure.is_single_result = False
            
            print(f"üèÜ CONFIGURACI√ìN DE RANKING:")
            print(f"   üìç Dimensi√≥n objetivo: {structure.main_dimension.text}")
            print(f"   üìä M√©trica de ranking: {structure.ranking_criteria.metric.text if structure.ranking_criteria.metric else 'N/A'}")
            print(f"   üéØ Direcci√≥n: {structure.ranking_criteria.direction.value}")
            print(f"   üìà Unidad: {structure.ranking_criteria.unit.value}")
            print(f"   üî¢ Valor: {structure.ranking_criteria.value}")
            print(f"   üö´ Exclusiones: {len(structure.exclusion_filters)}")
            print(f"   üî¢ L√≠mite: {structure.limit_value}")
        
        elif query_pattern == QueryPattern.REFERENCED:
            structure.reference_metric = metrics[0] if metrics else None
            structure.is_single_result = True
            structure.limit_value = 1
            
            print(f"üéØ CONFIGURACI√ìN DE DATOS REFERENCIADOS:")
            print(f"   üìç Dimensi√≥n objetivo: {structure.main_dimension.text}")
            print(f"   üìä M√©trica de referencia: {structure.reference_metric.text if structure.reference_metric else 'N/A'}")
            print(f"   ‚ö° Operaci√≥n de referencia: {operations[0].value if operations else 'N/A'}")
            print(f"   üîó Es compuesta: {structure.is_compound_query}")
            print(f"   üî¢ L√≠mite: {structure.limit_value}")
        
        # DEBUG: Mostrar estructura final
        print(f"üèóÔ∏è ESTRUCTURA FINAL:")
        print(f"   üìç Dimensi√≥n principal: {main_dimension.text if main_dimension else 'NINGUNA (agregaci√≥n global)'}")
        print(f"   üîó M√∫ltiples dimensiones: {[d.text for d in multi_dimensions] if is_multi_dimension else 'No'}")
        print(f"   üéõÔ∏è Filtros: {[f'{cvp.column_name} = {cvp.value}' for cvp in column_value_pairs]}")
        print(f"   üö´ Exclusiones: {[f'{ef.column_name} != {ef.value}' for ef in exclusion_filters]}")
        print(f"   ‚ö° Operaciones: {[op.text for op in operations]}")
        print(f"   üìä M√©tricas: {[m.text for m in metrics]}")
        print(f"   üîó Criterios compuestos: {len(compound_criteria)}")
        print(f"   üèÜ Es ranking: {is_ranking}")
        print(f"   ‚è∞ Filtros temporales: {len(temporal_filters)}")
        print(f"   üéØ Patr√≥n de consulta: {query_pattern.value}")
        
        if hasattr(self, '_current_original_intent'):
            structure.original_semantic_intent = self._current_original_intent
            print(f"   üß† Intent sem√°ntico: {structure.original_semantic_intent}") 
        
        return structure


# ------  "Detector de agregacion global" -------

    def is_global_aggregation_query(self, structure: QueryStructure, available_dimensions: List[QueryComponent] = None) -> bool:
        """Detector de Agregaci√≥n Global"""
        has_operation = len(structure.operations) > 0
        has_metric = len(structure.metrics) > 0
        has_column_filters = len(structure.column_conditions) > 0
        has_available_dimensions = available_dimensions and len(available_dimensions) > 0
        
        print(f"üîç EVALUANDO AGREGACI√ìN GLOBAL:")
        print(f"   üìä Tiene operaci√≥n: {has_operation}")
        print(f"   üìà Tiene m√©trica: {has_metric}")
        print(f"   üéõÔ∏è Tiene filtros de columna: {has_column_filters}")
        print(f"   üìç Tiene dimensiones disponibles: {has_available_dimensions}")
        
        # CRITERIO REFINADO: Es agregaci√≥n global SOLO si:
        # 1. Tiene operaci√≥n + m√©trica
        # 2. NO tiene filtros de columna espec√≠ficos
        # 3. NO tiene dimensiones principales v√°lidas disponibles
        
        if has_operation and has_metric and not has_column_filters and not has_available_dimensions:
            print(f"üåê Detectada agregaci√≥n global: operaci√≥n + m√©trica sin filtros ni dimensiones")
            return True
        
        # Si hay dimensiones disponibles, NO es agregaci√≥n global
        if has_available_dimensions:
            print(f"üìç NO es agregaci√≥n global: hay dimensiones principales disponibles")
            return False
        
        return False


# ------  "Exclusor de dimensiones temporales" -------

    def should_exclude_temporal_dimension(self, dimension_candidate: QueryComponent, temporal_filters: List[TemporalFilter]) -> bool:
        """Exclusor de Dimensiones Temporales"""
        temporal_units = {'semana', 'semanas', 'mes', 'meses', 'a√±o', 'a√±os', 'dia', 'dias'}
        
        if dimension_candidate.text.lower() in temporal_units and len(temporal_filters) > 0:
            print(f"‚è∞ Excluyendo '{dimension_candidate.text}' como dimensi√≥n principal (es parte de filtro temporal)")
            return True
        
        return False


# ------  "Detector temporal mejorado" -------

    def should_exclude_temporal_dimension_enhanced(self, dimension_candidate: QueryComponent, temporal_filters: List[TemporalFilter], is_ranking_query: bool = False) -> bool:
        """Exclusor Temporal Mejorado"""
        temporal_units = {'semana', 'semanas', 'week', 'weeks', 'mes', 'meses', 'dia', 'dias'}
        token_lower = dimension_candidate.text.lower()
        
        # CRITERIO 1: Siempre excluir unidades temporales si hay filtros temporales
        if token_lower in temporal_units and len(temporal_filters) > 0:
            print(f"‚è∞ Excluyendo '{dimension_candidate.text}' (unidad temporal con filtros)")
            return True
        
        # CRITERIO 2: En rankings, ser m√°s agresivo excluyendo temporales
        if is_ranking_query and token_lower in temporal_units:
            print(f"üèÜ‚è∞ Excluyendo '{dimension_candidate.text}' (temporal en ranking)")
            return True
        
        return False


# ------  "Detector de filtros de exclusion" -------

    def detect_exclusion_filters(self, tokens: List[str], classified_components: Dict) -> List[ExclusionFilter]:
        """Detector de Filtros de Exclusi√≥n"""
        print(f"üö´ DETECTANDO FILTROS DE EXCLUSI√ìN:")
        
        exclusion_filters = []
        
        # DICCIONARIOS DE INDICADORES DE EXCLUSI√ìN
        exclusion_indicators = {
            'excluyendo', 'exceptuando', 'excepto', 'sin', 'excluding', 'except', 'without',
            'menos', 'quitando', 'omitiendo', 'descartando'
        }
        
        # PASO 1: Buscar indicadores de exclusi√≥n
        for i, token in enumerate(tokens):
            token_lower = token.lower()
            
            if token_lower in exclusion_indicators:
                print(f"   üö´ Indicador de exclusi√≥n encontrado: '{token}' en posici√≥n {i}")
                
                # PASO 2: Buscar patr√≥n [COLUMNA] [VALOR] despu√©s del indicador
                exclusion_filter = self.extract_exclusion_from_position(tokens, i + 1, classified_components)
                
                if exclusion_filter:
                    exclusion_filters.append(exclusion_filter)
                    print(f"   ‚úÖ Filtro de exclusi√≥n extra√≠do: {exclusion_filter.column_name} != '{exclusion_filter.value}'")
        
        print(f"üö´ TOTAL FILTROS DE EXCLUSI√ìN: {len(exclusion_filters)}")
        return exclusion_filters


# ------  "Extractor de exclusiones posicionales" -------

    def extract_exclusion_from_position(self, tokens: List[str], start_pos: int, classified_components: Dict) -> Optional[ExclusionFilter]:
        """Extractor de Exclusiones Posicionales"""
        if start_pos >= len(tokens) - 1:
            return None
        
        # Buscar patr√≥n [COLUMNA] [VALOR] en las siguientes posiciones
        search_end = min(start_pos + 3, len(tokens))
        
        for i in range(start_pos, search_end - 1):
            if i + 1 >= len(tokens):
                break
                
            current_token = tokens[i]
            next_token = tokens[i + 1]
            
            print(f"      üîç Analizando exclusi√≥n: '{current_token}' + '{next_token}'")
            
            # Verificar si current_token es una columna potencial
            column_info = self._identify_potential_column(current_token)
            
            if column_info['is_column']:
                # Verificar si next_token es un valor
                value_info = self._identify_potential_value(next_token, i + 1, tokens)
                
                if value_info['is_value']:
                    # Construir filtro de exclusi√≥n
                    confidence = min(column_info['confidence'], value_info['confidence']) * 0.9  # Reducir por ser exclusi√≥n
                    
                    return ExclusionFilter(
                        exclusion_type=ExclusionType.NOT_EQUALS,  # Por defecto, NOT_EQUALS
                        column_name=column_info['normalized_name'],
                        value=value_info['normalized_value'],
                        confidence=confidence,
                        raw_tokens=tokens[start_pos-1:i+2]  # Incluir indicador de exclusi√≥n
                    )
        
        return None


# ------  "Verificador de consultas de ranking" -------

    def is_ranking_query(self, ranking_criteria: Optional[RankingCriteria], exclusion_filters: List[ExclusionFilter]) -> bool:
        """Verificador de Consulta de Ranking"""
        # Es ranking si tiene criterios v√°lidos
        has_valid_ranking = ranking_criteria and ranking_criteria.confidence >= 0.6
        
        is_ranking = bool(has_valid_ranking)
        
        print(f"üèÜ EVALUANDO SI ES CONSULTA DE RANKING:")
        print(f"   üìä Tiene criterios v√°lidos: {has_valid_ranking}")
        print(f"   üö´ Filtros de exclusi√≥n: {len(exclusion_filters)}")
        print(f"   üéØ Es ranking: {is_ranking}")
        
        return is_ranking



        # ================================================
        # GRUPO 5: VALIDACI√ìN Y CONFIANZA 
        # Validaci√≥n de estructura y c√°lculos de confianza
        # ================================================



# ------  "Validador de estructura completa" -------

    def validate_structure(self, structure: QueryStructure) -> Dict:
        """Validador de Estructura Completa"""
        errors = []
        suggestions = []
        
        # NUEVA VALIDACI√ìN: Permitir consultas sin dimensi√≥n principal si son agregaciones globales
        if not structure.main_dimension:
            # Verificar si es una agregaci√≥n global v√°lida
            if self.is_global_aggregation_query(structure):
                print(f"‚úÖ Agregaci√≥n global v√°lida detectada - dimensi√≥n principal no requerida")
            else:
                # Solo es error si NO es agregaci√≥n global
                if structure.column_conditions:
                    available_columns = [cvp.column_name for cvp in structure.column_conditions]
                    suggestions.append(f"Columnas detectadas: {', '.join(available_columns)}")
                errors.append("Falta dimensi√≥n principal")
                suggestions.append("Agrega una entidad como: partner_code, product_group, cuentas, tienda")
        
        # Validaci√≥n para contenido significativo
        has_meaningful_content = (
            structure.metrics or 
            structure.operations or 
            structure.column_conditions or
            structure.temporal_filters
        )
        
        if not has_meaningful_content:
            errors.append("Falta m√©trica, operaci√≥n o condici√≥n")
            suggestions.append("Agrega una m√©trica como: ventas, sell_out, sales_amount")
        
        # Advertencias para tokens desconocidos
        if structure.unknown_tokens:
            unknown_words = [token.text for token in structure.unknown_tokens]
            suggestions.append(f"Palabras no reconocidas: {', '.join(unknown_words)}")
        
        return {
            'valid': len(errors) == 0,
            'error': '; '.join(errors) if errors else None,
            'suggestions': suggestions
        }


# ------  "Calculador de nivel de complejidad" -------

    def get_complexity_level(self) -> str:
        """Calculador de Nivel de Complejidad"""
        complexity_score = 0

        complexity_score += len(self.column_conditions) * 2
        complexity_score += len(self.temporal_filters) * 3
        complexity_score += len(self.operations) * 1
        complexity_score += len(self.unknown_tokens) * -1
        
        # NUEVA L√ìGICA: Complejidad por consultas compuestas
        if self.is_compound_query:
            complexity_score += len(self.compound_criteria) * 2
        
        # Agregar complejidad por patr√≥n
        if self.query_pattern == QueryPattern.REFERENCED:
            complexity_score += 2
        elif self.query_pattern == QueryPattern.LIST_ALL:
            complexity_score += 1
            
        if complexity_score <= 0:
            return "simple"
        elif complexity_score <= 3:
            return "moderada"
        elif complexity_score <= 6:
            return "compleja"
        else:
            return "muy_compleja"


# ------  "Calculador de confianza general" -------

    def calculate_overall_confidence(self, structure: QueryStructure) -> float:
        """Calculador de Confianza General"""
        all_components = []
        
        if structure.main_dimension:
            all_components.append(structure.main_dimension)
        
        all_components.extend(structure.operations)
        all_components.extend(structure.metrics)
        all_components.extend(structure.values)
        all_components.extend(structure.connectors)
        all_components.extend(structure.unknown_tokens)
        
        # Agregar confianza de condiciones de columna
        for condition in structure.column_conditions:
            all_components.append(QueryComponent("dummy", ComponentType.COLUMN_VALUE, condition.confidence))
        
        # Agregar confianza de filtros temporales
        for tf in structure.temporal_filters:
            all_components.append(QueryComponent("dummy", ComponentType.TEMPORAL, tf.confidence))
        
        if not all_components:
            return 0.0
        
        # Calcular promedio ponderado
        total_confidence = sum(comp.confidence for comp in all_components)
        return round(total_confidence / len(all_components), 2)


# ------  "Calculador de confianza referencial" -------

    def calculate_reference_confidence(self, structure: QueryStructure) -> float:
        """Calculador de Confianza Referencial"""
        print(f"   üîç CALCULANDO CONFIANZA PARA DATOS REFERENCIADOS:")
        
        base_confidence = 0.5  # Confianza base
        factors = []
        
        # Factor 1: Tiene dimensi√≥n (+0.15)
        if structure.main_dimension:
            base_confidence += 0.15
            factors.append("tiene_dimensi√≥n")
        
        # Factor 2: Operaci√≥n √∫nica (+0.1)
        if len(structure.operations) == 1:
            base_confidence += 0.1
            factors.append("operaci√≥n_√∫nica")
        
        # Factor 3: M√©trica √∫nica (+0.1)
        if len(structure.metrics) == 1:
            base_confidence += 0.1
            factors.append("m√©trica_√∫nica")
        
        # Factor 4: Sin filtros de columna (+0.1)
        if len(structure.column_conditions) == 0:
            base_confidence += 0.1
            factors.append("sin_filtros_columna")
        
        # Factor 5: Operaci√≥n de comparaci√≥n (+0.2)
        if structure.operations and structure.operations[0].value in ['m√°ximo', 'm√≠nimo']:
            base_confidence += 0.2
            factors.append("operaci√≥n_comparaci√≥n")
        
        # Factor 6: Operaci√≥n espec√≠fica de referencia (+0.05)
        if structure.operations:
            op_text = structure.operations[0].text.lower()
            reference_ops = ['mas', 'm√°s', 'mayor', 'mejor', 'menos', 'menor', 'peor']
            if op_text in reference_ops:
                base_confidence += 0.05
                factors.append(f"operaci√≥n_referencia_{op_text}")
        
        # Penalizaciones
        if len(structure.operations) > 1:
            base_confidence -= 0.2
            factors.append("m√∫ltiples_operaciones_-0.2")
        
        if len(structure.metrics) > 1:
            base_confidence -= 0.2
            factors.append("m√∫ltiples_m√©tricas_-0.2")
        
        if len(structure.column_conditions) > 0:
            base_confidence -= 0.15
            factors.append("filtros_columna_-0.15")
        
        # Limitar entre 0.0 y 1.0
        final_confidence = max(0.0, min(1.0, base_confidence))
        
        print(f"      üìä Factores aplicados: {factors}")
        print(f"      ‚≠ê Confianza final: {final_confidence:.2f}")
        
        return final_confidence


# ------  "Calculador de confianza compuesta" -------

    def calculate_compound_reference_confidence(self, structure: QueryStructure) -> float:
        """Calculador de Confianza Compuesta"""
        print(f"   üîç CALCULANDO CONFIANZA PARA CONSULTA COMPUESTA REFERENCIADA:")
        
        base_confidence = 0.6  # Confianza base m√°s alta para compuestas
        factors = []
        
        # Factor 1: Tiene dimensi√≥n (+0.1)
        if structure.main_dimension:
            base_confidence += 0.1
            factors.append("tiene_dimensi√≥n")
        
        # Factor 2: N√∫mero de criterios v√°lidos (+0.05 por criterio, max +0.15)
        valid_criteria = len([c for c in structure.compound_criteria if c.confidence >= 0.7])
        criteria_bonus = min(valid_criteria * 0.05, 0.15)
        base_confidence += criteria_bonus
        factors.append(f"criterios_v√°lidos_{valid_criteria}_+{criteria_bonus}")
        
        # Factor 3: Sin filtros de columna (+0.1)
        if len(structure.column_conditions) == 0:
            base_confidence += 0.1
            factors.append("sin_filtros_columna")
        
        # Factor 4: Todas las operaciones son de comparaci√≥n (+0.1)
        reference_operations = ['m√°ximo', 'm√≠nimo', 'mayor', 'menor']
        all_reference = all(
            criteria.operation.value in reference_operations 
            for criteria in structure.compound_criteria
        )
        if all_reference:
            base_confidence += 0.1
            factors.append("todas_operaciones_comparaci√≥n")
        
        # Factor 5: Calidad promedio de criterios (+0.05)
        if structure.compound_criteria:
            avg_criteria_confidence = sum(c.confidence for c in structure.compound_criteria) / len(structure.compound_criteria)
            if avg_criteria_confidence >= 0.8:
                base_confidence += 0.05
                factors.append(f"alta_calidad_criterios_{avg_criteria_confidence:.2f}")
        
        # Limitar entre 0.0 y 1.0
        final_confidence = max(0.0, min(1.0, base_confidence))
        
        print(f"      üìä Factores aplicados: {factors}")
        print(f"      ‚≠ê Confianza final: {final_confidence:.2f}")
        
        return final_confidence


# ------  "Calculador de confianza ranking" -------

    def calculate_ranking_confidence(self, structure: QueryStructure) -> float:
        """Calculador de Confianza de Ranking"""
        print(f"   üîç CALCULANDO CONFIANZA PARA CONSULTA DE RANKING:")
        
        if not structure.ranking_criteria:
            return 0.0
        
        base_confidence = structure.ranking_criteria.confidence
        factors = ['criterio_base']
        
        # Factor 1: Tiene dimensi√≥n principal (+0.1)
        if structure.main_dimension:
            base_confidence += 0.1
            factors.append("tiene_dimensi√≥n")
        
        # Factor 2: Tipo de unidad
        if structure.ranking_criteria.unit == RankingUnit.PERCENTAGE:
            base_confidence += 0.05  # Porcentajes son m√°s espec√≠ficos
            factors.append("usa_porcentaje")
        elif structure.ranking_criteria.unit == RankingUnit.COUNT:
            base_confidence += 0.03
            factors.append("usa_n√∫mero")
        
        # Factor 3: Tiene m√©trica espec√≠fica (+0.05)
        if structure.ranking_criteria.metric:
            base_confidence += 0.05
            factors.append("m√©trica_espec√≠fica")
        
        # Factor 4: Tiene filtros de exclusi√≥n (+0.02 por filtro, max +0.06)
        if structure.exclusion_filters:
            exclusion_bonus = min(len(structure.exclusion_filters) * 0.02, 0.06)
            base_confidence += exclusion_bonus
            factors.append(f"exclusiones_{len(structure.exclusion_filters)}")
        
        # Factor 5: Valor razonable
        if structure.ranking_criteria.unit == RankingUnit.COUNT and 1 <= structure.ranking_criteria.value <= 50:
            base_confidence += 0.03
            factors.append("valor_razonable_count")
        elif structure.ranking_criteria.unit == RankingUnit.PERCENTAGE and 1 <= structure.ranking_criteria.value <= 100:
            base_confidence += 0.03
            factors.append("valor_razonable_percentage")
        
        # Limitar entre 0.0 y 1.0
        final_confidence = max(0.0, min(1.0, base_confidence))
        
        print(f"      üìä Factores aplicados: {factors}")
        print(f"      ‚≠ê Confianza final: {final_confidence:.2f}")
        
        return final_confidence


    def calculate_multi_dimension_confidence(self, structure: QueryStructure) -> float:
        """Calculador de Confianza Multi-Dimensional"""
        print(f"   üîç CALCULANDO CONFIANZA PARA M√öLTIPLES DIMENSIONES:")
        
        base_confidence = 0.6
        factors = ['base_multi_dimension']
        
        # Factor 1: N√∫mero de dimensiones (+0.05 por dimensi√≥n extra)
        extra_dims = len(structure.main_dimensions) - 2
        if extra_dims > 0:
            bonus = min(extra_dims * 0.05, 0.15)
            base_confidence += bonus
            factors.append(f"dimensiones_extra_{extra_dims}")
        
        # Factor 2: Tiene operaci√≥n y m√©trica (+0.2)
        if structure.operations and structure.metrics:
            base_confidence += 0.2
            factors.append("operacion_metrica")
        
        # Factor 3: Sin filtros complejos (+0.1)
        if len(structure.column_conditions) == 0:
            base_confidence += 0.1
            factors.append("sin_filtros_complejos")
        
        final_confidence = max(0.0, min(1.0, base_confidence))
        
        print(f"      üìä Factores aplicados: {factors}")
        print(f"      ‚≠ê Confianza final: {final_confidence:.2f}")
        
        return final_confidence



        # =======================================
        # GRUPO 6: GENERACI√ìN SQL 
        # Generaci√≥n de consultas SQL optimizadas
        # =======================================



# ------  "Generador de SQL optimizado" -------



    def generate_optimized_sql(self, structure: QueryStructure) -> str:
        """Generador de SQL Optimizado - VERSI√ìN CORREGIDA PARA RANKINGS MULTI-DIMENSIONALES"""
        select_parts = []
        from_clause = "FROM datos"
        where_conditions = []
        group_by_parts = []
        order_by_parts = []
        
        # Identificar columnas temporales para evitar duplicaci√≥n
        temporal_columns = set()
        temporal_sql_added = False
        
        for tf in structure.temporal_filters:
            if tf.unit == TemporalUnit.WEEKS:
                temporal_columns.add('semana')
                temporal_columns.add('week')
            elif tf.unit == TemporalUnit.MONTHS:
                temporal_columns.add('mes')
                temporal_columns.add('month')
            elif tf.unit == TemporalUnit.DAYS:
                temporal_columns.add('dia')
                temporal_columns.add('day')
            elif tf.unit == TemporalUnit.YEARS:
                temporal_columns.add('a√±o')
                temporal_columns.add('year')
        
        print(f"üóÑÔ∏è Generando SQL optimizado:")
        print(f"   ‚è∞ Columnas temporales detectadas: {temporal_columns}")
        print(f"   üéØ Patr√≥n de consulta: {structure.query_pattern.value}")
        print(f"   üîó Es compuesta: {structure.is_compound_query}")
        print(f"   üèÜ Es ranking: {structure.is_ranking_query}")
        print(f"   üîó Es multi-dimensional: {structure.is_multi_dimension_query}")
        
        # üîß NUEVA L√ìGICA: Manejar rankings multi-dimensionales
        if (structure.is_ranking_query and 
            structure.is_multi_dimension_query and 
            len(structure.main_dimensions) >= 2):
            print(f"üèÜüîó DETECTADO: Ranking multi-dimensional ‚Üí usando generador especializado")
            return self.generate_multi_dimension_sql(structure, temporal_columns)
        
        # NUEVA L√ìGICA: Manejar consultas multi-dimensionales sin ranking
        if (structure.is_multi_dimension_query and 
            structure.query_pattern == QueryPattern.MULTI_DIMENSION):
            print(f"üîó DETECTADO: Multi-dimensional sin ranking ‚Üí usando generador especializado")
            return self.generate_multi_dimension_sql(structure, temporal_columns)
        
        # Verificar si es agregaci√≥n global
        is_global_aggregation = not structure.main_dimension and structure.operations and structure.metrics
        
        if is_global_aggregation:
            print(f"üåê Generando SQL para agregaci√≥n global")
            
            # Para agregaciones globales: solo la funci√≥n de agregaci√≥n
            if structure.operations and structure.metrics:
                operation = structure.operations[0]
                metric = structure.metrics[0]

                # OPERACIONES SQL DISPONIBLES
                if operation.value == 'm√°ximo':
                    agg_function = self._get_contextual_aggregation(structure, metric.text, operation.value)
                else:
                    sql_operations = {
                        'm√≠nimo': f'MIN({metric.text})',
                        'suma': f'SUM({metric.text})',
                        'promedio': f'AVG({metric.text})',
                        'conteo': f'COUNT({metric.text})'
                    }
                    agg_function = sql_operations.get(operation.value, f'SUM({metric.text})')
                
                if agg_function:
                    select_parts.append(agg_function)
                        
        else:
            # L√≥gica para consultas con dimensi√≥n principal
            if structure.main_dimension:
                dim_name = structure.main_dimension.text
                select_parts.append(dim_name)
                group_by_parts.append(dim_name)
            
            # CONSULTAS COMPUESTAS
            if structure.is_compound_query and structure.compound_criteria:
                print(f"üîó Procesando consulta compuesta con {len(structure.compound_criteria)} criterios:")
                
                # Agregar funciones de agregaci√≥n para cada criterio
                for i, criteria in enumerate(structure.compound_criteria):
                    operation_value = criteria.operation.value
                    metric_text = criteria.metric.text
                    
                    # Usar _get_contextual_aggregation para 'm√°ximo'
                    if operation_value == 'm√°ximo':
                        agg_function = self._get_contextual_aggregation(structure, metric_text, operation_value)
                    else:
                        sql_operations = {
                            'm√≠nimo': f'MIN({metric_text})',
                            'suma': f'SUM({metric_text})',
                            'promedio': f'AVG({metric_text})',
                            'conteo': f'COUNT({metric_text})'
                        }
                        agg_function = sql_operations.get(operation_value, f'SUM({metric_text})')
                    
                    if agg_function:
                        select_parts.append(agg_function)
                        
                        # Construir ORDER BY para m√∫ltiples criterios
                        if operation_value in ['m√°ximo', 'mayor']:
                            order_direction = "DESC"
                        elif operation_value in ['m√≠nimo', 'menor']:
                            order_direction = "ASC"
                        else:
                            order_direction = "DESC"
                        
                        order_by_parts.append(f"{agg_function} {order_direction}")
                        
                        print(f"   üîó Criterio {i+1}: {operation_value} {metric_text} ‚Üí {agg_function} {order_direction}")
                    else:
                        # Si no hay operaci√≥n SQL espec√≠fica, usar la m√©trica directamente
                        select_parts.append(metric_text)
                        order_by_parts.append(f"{metric_text} DESC")
                        print(f"   üîó Criterio {i+1}: {metric_text} ‚Üí {metric_text} DESC")
                
            # L√ìGICA TRADICIONAL
            elif structure.operations and structure.metrics:
                operation = structure.operations[0]
                metric = structure.metrics[0]
                
                # Usar _get_contextual_aggregation para 'm√°ximo'
                if operation.value == 'm√°ximo':
                    agg_function = self._get_contextual_aggregation(structure, metric.text, operation.value)
                else:
                    sql_operations = {
                        'm√≠nimo': f'MIN({metric.text})',
                        'suma': f'SUM({metric.text})',
                        'promedio': f'AVG({metric.text})',
                        'conteo': f'COUNT({metric.text})'
                    }
                    agg_function = sql_operations.get(operation.value, f'SUM({metric.text})')
                
                if agg_function:
                    select_parts.append(agg_function)
                    
                    # Para REFERENCED, ordenar por la m√©trica agregada
                    if structure.query_pattern == QueryPattern.REFERENCED:
                        if operation.value in ['m√°ximo', 'mayor']:
                            order_by_parts.append(f"{agg_function} DESC")
                        elif operation.value in ['m√≠nimo', 'menor']:
                            order_by_parts.append(f"{agg_function} ASC")
                        else:
                            order_by_parts.append(f"{agg_function} DESC")
                    else:
                        order_by_parts.append(f"{agg_function} DESC")
                else:
                    # Si no hay operaci√≥n SQL espec√≠fica, usar la m√©trica directamente
                    select_parts.append(metric.text)
                    if structure.query_pattern == QueryPattern.REFERENCED:
                        order_by_parts.append(f"{metric.text} DESC")
        
        # WHERE para condiciones de columna (excluyendo temporales duplicadas)
        for condition in structure.column_conditions:
            if condition.column_name not in temporal_columns:
                where_conditions.append(f"{condition.column_name} = '{condition.value}'")
                print(f"   ‚úÖ Condici√≥n WHERE: {condition.column_name} = '{condition.value}'")
            else:
                print(f"   ‚è∞ Excluyendo condici√≥n temporal duplicada: {condition.column_name} = '{condition.value}'")
        
        # FILTROS TEMPORALES - CORREGIDO
        # Intentar filtros temporales avanzados
        advanced_conditions = self.get_advanced_temporal_sql_conditions(structure)
        if advanced_conditions:
            where_conditions.extend(advanced_conditions)
            temporal_sql_added = True
            print(f"   ‚úÖ Usando filtros temporales avanzados: {advanced_conditions}")

        # CONSTRUCCI√ìN DEL SQL FINAL
        sql_parts = []
        
        if select_parts:
            sql_parts.append(f"SELECT {', '.join(select_parts)}")
        else:
            sql_parts.append("SELECT *")
        
        sql_parts.append(from_clause)
        
        if where_conditions:
            sql_parts.append(f"WHERE {' AND '.join(where_conditions)}")
        
        if group_by_parts:
            sql_parts.append(f"GROUP BY {', '.join(group_by_parts)}")
        
        if order_by_parts:
            sql_parts.append(f"ORDER BY {', '.join(order_by_parts)}")
        
        # LIMITAR LA DATA SEGUN EL USUARIO
        if structure.query_pattern == QueryPattern.REFERENCED:
            sql_parts.append("LIMIT 1")
            print(f"   üéØ Agregando LIMIT 1 para patr√≥n REFERENCED")
            
        elif structure.query_pattern == QueryPattern.TOP_N and structure.limit_value:
            sql_parts.append(f"LIMIT {structure.limit_value}")
            print(f"   üèÜ Agregando LIMIT {structure.limit_value} para patr√≥n TOP_N")
        
        elif structure.is_ranking_query and structure.ranking_criteria and structure.ranking_criteria.value:
            limit_value = int(structure.ranking_criteria.value)
            sql_parts.append(f"LIMIT {limit_value}")
            print(f"   üèÜ FORZANDO LIMIT {limit_value} para ranking (patr√≥n: {structure.query_pattern.value})")
        
        final_sql = " ".join(sql_parts) + ";"
        print(f"   üéØ SQL final: {final_sql}")
        
        return final_sql
            


# ------  "Generador de SQL ranking" -------

    def generate_ranking_sql(self, structure: QueryStructure, temporal_columns: set) -> str:
        """üîß Generador de SQL para Rankings Multi-Criterio - VERSI√ìN CORREGIDA"""
        print(f"üèÜ GENERANDO SQL PARA RANKING MULTI-CRITERIO:")
        
        ranking = structure.ranking_criteria
        if not ranking:
            print(f"‚ùå Error: No hay criterios de ranking")
            return "SELECT * FROM datos;"
        
        # CONSTRUIR SELECT - ‚úÖ INCLUIR TODAS LAS M√âTRICAS
        select_parts = []
        if structure.main_dimension:
            select_parts.append(structure.main_dimension.text)
        
        order_by_parts = []
        
        # üîß NUEVA L√ìGICA: Procesar TODAS las m√©tricas detectadas
        if len(structure.metrics) > 1:
            print(f"   üîó DETECTANDO RANKING MULTI-CRITERIO con {len(structure.metrics)} m√©tricas")
            
            # Mapear operaciones a m√©tricas
            operations_available = [op.text.lower() for op in structure.operations if op.text.lower() in ['mas', 'm√°s', 'mayor', 'menor', 'top']]
            metrics_available = [m.text for m in structure.metrics]
            
            print(f"   üìä M√©tricas: {metrics_available}")
            print(f"   ‚ö° Operaciones: {operations_available}")
            
            # Asumir que las operaciones se aplican en orden a las m√©tricas
            for i, metric in enumerate(metrics_available):
                # Determinar operaci√≥n para esta m√©trica
                if i < len(operations_available):
                    op = operations_available[i]
                else:
                    op = operations_available[0] if operations_available else 'mas'  # Default
                
                # Mapear operaci√≥n a funci√≥n SQL
                if op in ['mas', 'm√°s', 'mayor', 'top']:
                    agg_function = f'SUM({metric})'
                    order_direction = 'DESC'
                elif op in ['menor', 'minimo', 'm√≠nimo']:
                    agg_function = f'SUM({metric})'  # Usando SUM, pero ordenando ASC
                    order_direction = 'ASC'
                else:
                    agg_function = f'SUM({metric})'
                    order_direction = 'DESC'
                
                select_parts.append(agg_function)
                order_by_parts.append(f"{agg_function} {order_direction}")
                
                print(f"   {i+1}. {metric} ‚Üí {agg_function} {order_direction} (operaci√≥n: {op})")
                
        else:
            # üîß L√ìGICA ORIGINAL: Una sola m√©trica
            if ranking.metric:
                if ranking.operation and ranking.operation.text.lower() in ['mas', 'm√°s', 'mayor']:
                    agg_function = f'SUM({ranking.metric.text})'
                    print(f"   üèÜ Ranking: 'mas' interpretado como SUM")
                elif ranking.operation:
                    sql_operations = {
                        'm√°ximo': f'MAX({ranking.metric.text})',
                        'm√≠nimo': f'MIN({ranking.metric.text})',
                        'suma': f'SUM({ranking.metric.text})',
                        'promedio': f'AVG({ranking.metric.text})',
                        'conteo': f'COUNT({ranking.metric.text})'
                    }
                    agg_function = sql_operations.get(ranking.operation.value, f'SUM({ranking.metric.text})')
                else:
                    agg_function = f'SUM({ranking.metric.text})'
                
                if agg_function:
                    select_parts.append(agg_function)
                    
                    # Determinar direcci√≥n basada en el ranking
                    if ranking.direction == RankingDirection.TOP:
                        order_direction = "DESC"
                    else:
                        order_direction = "ASC"
                        
                    order_by_parts.append(f"{agg_function} {order_direction}")
                    print(f"   ‚úÖ Funci√≥n agregada al SELECT: {agg_function}")
        
        # CONSTRUIR WHERE (usando l√≥gica existente)
        where_conditions = []
        
        # Condiciones regulares
        for condition in structure.column_conditions:
            if condition.column_name not in temporal_columns:
                where_conditions.append(f"{condition.column_name} = '{condition.value}'")
        
        # Exclusiones
        for exclusion in structure.exclusion_filters:
            if exclusion.exclusion_type == ExclusionType.NOT_EQUALS:
                where_conditions.append(f"{exclusion.column_name} != '{exclusion.value}'")
        
        # Filtros temporales avanzados
        advanced_conditions = self.get_advanced_temporal_sql_conditions(structure)
        where_conditions.extend(advanced_conditions)
        
        # CONSTRUIR SQL FINAL
        sql_parts = [
            f"SELECT {', '.join(select_parts)}",
            "FROM datos"
        ]
        
        if where_conditions:
            sql_parts.append(f"WHERE {' AND '.join(where_conditions)}")
        
        if structure.main_dimension:
            sql_parts.append(f"GROUP BY {structure.main_dimension.text}")
        
        # üîß ORDER BY multi-criterio
        if order_by_parts:
            sql_parts.append(f"ORDER BY {', '.join(order_by_parts)}")
        
        sql_parts.append(f"LIMIT {int(ranking.value)}")
        
        final_sql = " ".join(sql_parts) + ";"
        print(f"   üéØ SQL final multi-criterio: {final_sql}")
        
        return final_sql


# --- GENERACION DE RANKING SQL ---

    def generate_multi_dimension_sql(self, structure: QueryStructure, temporal_columns: set) -> str:
        """üîß GENERADOR SQL PARA M√öLTIPLES DIMENSIONES - VERSI√ìN CORREGIDA"""
        print(f"üîó GENERANDO SQL PARA M√öLTIPLES DIMENSIONES:")
        
        select_parts = []
        group_by_parts = []
        order_by_parts = []
        where_conditions = []
        
        # PASO 1: Agregar todas las dimensiones principales
        for dimension in structure.main_dimensions:
            select_parts.append(dimension.text)
            group_by_parts.append(dimension.text)
            print(f"   üìç Dimensi√≥n agregada: {dimension.text}")
        
        # PASO 2: üîß BUSCAR LA M√âTRICA CORRECTA PARA EL RANKING
        ranking_metric = None
        operation_value = None
        
        # Prioridad 1: M√©trica especificada en ranking_criteria
        if structure.ranking_criteria and structure.ranking_criteria.metric:
            ranking_metric = structure.ranking_criteria.metric
            print(f"   üìä M√©trica del ranking: {ranking_metric.text}")
        
        # Prioridad 2: Buscar m√©tricas reales (NO convertidas de dimensiones)
        else:
            real_metrics = [
                m for m in structure.metrics 
                if not m.linguistic_info.get('converted_from') == 'dimension'
            ]
            
            if real_metrics:
                ranking_metric = real_metrics[0]
                print(f"   üìä M√©trica real encontrada: {ranking_metric.text}")
            else:
                # Fallback: usar la primera m√©trica disponible
                if structure.metrics:
                    ranking_metric = structure.metrics[0]
                    print(f"   üìä M√©trica fallback: {ranking_metric.text}")
        
        # PASO 3: Determinar operaci√≥n
        if structure.operations:
            # Buscar operaci√≥n relevante (no ranking indicators)
            relevant_operations = [
                op for op in structure.operations 
                if op.value not in ['top', 'bottom'] and op.subtype != 'ranking_indicator'
            ]
            
            if relevant_operations:
                operation = relevant_operations[0]
                operation_value = operation.value
                print(f"   ‚ö° Operaci√≥n relevante: {operation.text} ‚Üí {operation_value}")
            else:
                # Si solo hay indicadores de ranking, usar operaci√≥n por defecto
                operation_value = 'suma'  # Por defecto para rankings
                print(f"   ‚ö° Usando operaci√≥n por defecto: suma")
        else:
            operation_value = 'suma'
            print(f"   ‚ö° Sin operaciones, usando por defecto: suma")
        
        # PASO 4: Construir funci√≥n de agregaci√≥n
        if ranking_metric:
            if operation_value == 'm√°ximo':
                agg_function = self._get_contextual_aggregation(structure, ranking_metric.text, operation_value)
            else:
                sql_operations = {
                    'm√≠nimo': f'MIN({ranking_metric.text})',
                    'suma': f'SUM({ranking_metric.text})',
                    'promedio': f'AVG({ranking_metric.text})',
                    'conteo': f'COUNT({ranking_metric.text})'
                }
                agg_function = sql_operations.get(operation_value, f'SUM({ranking_metric.text})')
            
            select_parts.append(agg_function)
            
            # Determinar orden basado en ranking
            if structure.ranking_criteria:
                if structure.ranking_criteria.direction == RankingDirection.TOP:
                    order_direction = "DESC"
                else:
                    order_direction = "ASC"
            else:
                # Determinar orden basado en operaci√≥n
                if operation_value in ['m√°ximo', 'mayor']:
                    order_direction = "DESC"
                elif operation_value in ['m√≠nimo', 'menor']:
                    order_direction = "ASC"
                else:
                    order_direction = "DESC"
            
            order_by_parts.append(f"{agg_function} {order_direction}")
            print(f"   üìä Agregaci√≥n: {agg_function} {order_direction}")
        else:
            print(f"   ‚ùå No se encontr√≥ m√©trica v√°lida para el ranking")
            return "SELECT * FROM datos;"
        
        # PASO 5: WHERE conditions
        for condition in structure.column_conditions:
            if condition.column_name not in temporal_columns:
                where_conditions.append(f"{condition.column_name} = '{condition.value}'")
        
        # PASO 6: Filtros temporales
        advanced_conditions = self.get_advanced_temporal_sql_conditions(structure)
        where_conditions.extend(advanced_conditions)
        
        # PASO 7: Construir SQL final
        sql_parts = [f"SELECT {', '.join(select_parts)}", "FROM datos"]
        
        if where_conditions:
            sql_parts.append(f"WHERE {' AND '.join(where_conditions)}")
        
        if group_by_parts:
            sql_parts.append(f"GROUP BY {', '.join(group_by_parts)}")
        
        if order_by_parts:
            sql_parts.append(f"ORDER BY {', '.join(order_by_parts)}")
        
        # PASO 8: Aplicar l√≠mite
        if structure.is_ranking_query and structure.ranking_criteria:
            ranking_value = int(structure.ranking_criteria.value)
            sql_parts.append(f"LIMIT {ranking_value}")
            print(f"   üèÜ APLICANDO LIMIT de ranking: {ranking_value}")
        else:
            sql_parts.append("LIMIT 10")  # L√≠mite por defecto m√°s razonable
            print(f"   üìç APLICANDO LIMIT por defecto: 10")
        
        final_sql = " ".join(sql_parts) + ";"
        print(f"   üéØ SQL multi-dimensional: {final_sql}")
        
        return final_sql




    # Generar SQL con m√∫ltiples valores temporales
    def get_advanced_temporal_sql_conditions(self, structure: QueryStructure) -> List[str]:
        """Obtiene condiciones SQL avanzadas para filtros temporales - VERSI√ìN M√öLTIPLES VALORES"""
        sql_conditions = []
        
        if hasattr(self, 'advanced_temporal_info') and self.advanced_temporal_info:
            for advanced_info in self.advanced_temporal_info:
                
                # üÜï NUEVO: Manejar m√∫ltiples valores espec√≠ficos
                if (advanced_info.original_filter.filter_type == "multiple_values" and
                    advanced_info.start_value and advanced_info.end_value):
                    
                    if advanced_info.original_filter.unit == TemporalUnit.WEEKS:
                        sql_condition = f"week IN ({advanced_info.start_value}, {advanced_info.end_value})"
                    elif advanced_info.original_filter.unit == TemporalUnit.MONTHS:
                        sql_condition = f"month IN ({advanced_info.start_value}, {advanced_info.end_value})"
                    elif advanced_info.original_filter.unit == TemporalUnit.DAYS:
                        sql_condition = f"day IN ({advanced_info.start_value}, {advanced_info.end_value})"
                    else:
                        continue
                    
                    sql_conditions.append(sql_condition)
                    print(f"   ‚úÖ Condici√≥n SQL m√∫ltiples valores: {sql_condition}")
                    continue
                
                # L√ìGICA EXISTENTE para otros tipos de filtros temporales
                sql_condition = advanced_info.to_sql_condition()
                if sql_condition and sql_condition != "1=1":
                    sql_conditions.append(sql_condition)
                    print(f"   ‚è∞ Condici√≥n SQL avanzada: {sql_condition}")
        
        return sql_conditions
    
    

    def _get_contextual_aggregation(self, structure: QueryStructure, metric_text: str, operation: str) -> str:
        """Usar intent sem√°ntico original (pre-mapeo) para decidir SUM vs MAX"""
        
        if operation == 'm√°ximo':
            # üéØ USAR INTENT ORIGINAL (analizado ANTES del mapeo)
            original_intent = getattr(structure, 'original_semantic_intent', 'DEFAULT')
            
            if original_intent == 'MAX':
                print(f"   üéØ INTENT ORIGINAL: MAX ‚Üí MAX({metric_text}) [palabras originales singulares]")
                return f'MAX({metric_text})'
            elif original_intent == 'SUM':
                print(f"   üéØ INTENT ORIGINAL: SUM ‚Üí SUM({metric_text}) [palabras originales plurales]")
                return f'SUM({metric_text})'
            else:
                print(f"   üéØ INTENT ORIGINAL: DEFAULT ‚Üí SUM({metric_text}) [configuraci√≥n por defecto]")
                return f'SUM({metric_text})'  # Tu configuraci√≥n por defecto
        
        return f'SUM({metric_text})'   
        


        # =============================================
        # GRUPO 7: FORMATEO Y RESULTADO 
        # Formateo de salida y conversi√≥n de resultados
        # =============================================



# ------  "Generador de estructura jerarquica" -------

    def generate_hierarchical_structure(self, structure: QueryStructure) -> str:
        """üîß Generador de Estructura Jer√°rquica - VERSI√ìN MULTI-CRITERIO"""
        
        # CASO ESPECIAL: Rankings - VERSI√ìN MULTI-CRITERIO
        if structure.is_ranking_query and structure.ranking_criteria:
            ranking = structure.ranking_criteria
            direction_text = "top" if ranking.direction == RankingDirection.TOP else "worst"
            
            if ranking.unit == RankingUnit.COUNT:
                result = f"{direction_text} {int(ranking.value)} ({structure.main_dimension.text})"
            else:  # PERCENTAGE
                result = f"{direction_text} {ranking.value}% ({structure.main_dimension.text})"
            
            # üîß NUEVA L√ìGICA: Incluir m√∫ltiples criterios
            if len(structure.metrics) > 1:
                operations_available = [op.text.lower() for op in structure.operations if op.text.lower() in ['mas', 'm√°s', 'mayor', 'menor']]
                metrics_available = [m.text for m in structure.metrics]
                
                criteria_parts = []
                for i, metric in enumerate(metrics_available):
                    if i < len(operations_available):
                        op = operations_available[i]
                    else:
                        op = operations_available[0] if operations_available else 'mas'
                    
                    criteria_parts.append(f"({op} {metric})")
                
                # Combinar con " y "
                combined_criteria = " y ".join(criteria_parts)
                result += f" por {combined_criteria}"
                
            else:
                # L√ìGICA ORIGINAL: Un criterio
                result += f" por ({ranking.metric.text})"
            
            # NUEVA L√ìGICA: Agregar filtros temporales avanzados
            temporal_description = self.generate_hierarchical_structure_temporal_description(structure)
            if temporal_description:
                result += f" {temporal_description}"
            
            # NUEVA L√ìGICA: Agregar filtros de columna si existen
            if structure.column_conditions:
                filter_parts = []
                for condition in structure.column_conditions:
                    filter_parts.append(f"con {condition.column_name} = '{condition.value}'")
                
                if filter_parts:
                    result += f" {' y '.join(filter_parts)}"
            
            # NUEVA L√ìGICA: Agregar exclusiones si existen
            if structure.exclusion_filters:
                exclusion_parts = []
                for exclusion in structure.exclusion_filters:
                    exclusion_parts.append(f"excluyendo {exclusion.column_name} = '{exclusion.value}'")
                
                if exclusion_parts:
                    result += f" {' y '.join(exclusion_parts)}"
            
            print(f"   üèÜ Resultado ranking completo: {result}")
            return result
        
        # RESTO DE LA L√ìGICA ORIGINAL PARA CONSULTAS NO-RANKING
        parts = []
        
        # PASO 1: Identificar columnas temporales
        temporal_columns = set()
        for tf in structure.temporal_filters:
            if tf.unit == TemporalUnit.WEEKS:
                temporal_columns.add('semana')
                temporal_columns.add('week')
            elif tf.unit == TemporalUnit.MONTHS:
                temporal_columns.add('mes')
                temporal_columns.add('month')
            elif tf.unit == TemporalUnit.DAYS:
                temporal_columns.add('dia')
                temporal_columns.add('day')
            elif tf.unit == TemporalUnit.YEARS:
                temporal_columns.add('a√±o')
                temporal_columns.add('year')
        
        print(f"üîç Generando estructura jer√°rquica para consulta compuesta:")
        print(f"   üìç Dimensi√≥n: {structure.main_dimension.text if structure.main_dimension else 'N/A'}")
        print(f"   üîó Es compuesta: {structure.is_compound_query}")
        print(f"   üîó Criterios compuestos: {len(structure.compound_criteria)}")
        print(f"   ‚è∞ Columnas temporales: {temporal_columns}")
        
        # PASO 2: Verificar si dimensi√≥n est√° en filtros
        dimension_in_filter = False
        if structure.main_dimension and structure.column_conditions:
            main_dim_name = structure.main_dimension.text
            for condition in structure.column_conditions:
                if condition.column_name == main_dim_name:
                    dimension_in_filter = True
                    break
        
        print(f"   üîÑ ¬øDimensi√≥n en filtros? {dimension_in_filter}")
        
        # PASO 3: FILTRAR condiciones temporales duplicadas
        non_temporal_conditions = []
        for condition in structure.column_conditions:
            if condition.column_name not in temporal_columns:
                non_temporal_conditions.append(condition)
                print(f"   ‚úÖ Conservando filtro: {condition.column_name} = {condition.value}")
            else:
                print(f"   ‚è∞ EXCLUYENDO filtro temporal duplicado: {condition.column_name} = {condition.value}")
        
        # PASO 4: Construir dimensi√≥n principal
        if structure.main_dimension and not dimension_in_filter:
            main_part = f"({structure.main_dimension.text})"
            
            # CR√çTICO: Solo agregar filtros NO temporales
            if non_temporal_conditions:
                conditions = []
                for condition in non_temporal_conditions:
                    conditions.append(f"({condition.column_name} = '{condition.value}')")
                main_part += f" con {' y '.join(conditions)}"
            
            parts.append(main_part)
            print(f"   ‚úÖ Parte principal: {main_part}")
        
        # PASO 5: Filtros directos (solo NO temporales)
        elif non_temporal_conditions:
            filter_parts = []
            for condition in non_temporal_conditions:
                filter_parts.append(f"({condition.column_name} = '{condition.value}')")
            
            if len(filter_parts) == 1:
                parts.append(filter_parts[0])
            else:
                parts.append(f"({' Y '.join(filter_parts)})")
            
            print(f"   ‚úÖ Filtros directos (no temporales): {filter_parts}")
        
        # PASO 6 NUEVA L√ìGICA: Operaci√≥n y m√©trica COMPUESTA
        if structure.is_compound_query and structure.compound_criteria:
            print(f"üîó PROCESANDO ESTRUCTURA JER√ÅRQUICA COMPUESTA:")
            
            # Construir cada criterio como ((operaci√≥n) (m√©trica))
            criteria_parts = []
            for i, criteria in enumerate(structure.compound_criteria):
                criteria_part = f"(({criteria.operation.text}) ({criteria.metric.text}))"
                criteria_parts.append(criteria_part)
                print(f"   {i+1}. Criterio: {criteria_part}")
            
            # Unir criterios con " y "
            if len(criteria_parts) == 1:
                operation_part = criteria_parts[0]
            else:
                operation_part = " y ".join(criteria_parts)
            
            # NUEVA L√ìGICA: Agregar informaci√≥n temporal avanzada para compuestas
            temporal_description = self.generate_hierarchical_structure_temporal_description(structure)
            if temporal_description:
                operation_part += f" {temporal_description}"
            
            parts.append(operation_part)
            print(f"   ‚úÖ Operaci√≥n compuesta: {operation_part}")
        
        # PASO 6 L√ìGICA TRADICIONAL: Para consultas NO compuestas
        elif structure.operations and structure.metrics:
            op = structure.operations[0]
            metric = structure.metrics[0]
            operation_part = f"(({op.text}) ({metric.text}))"
            
            # NUEVA L√ìGICA: Agregar informaci√≥n temporal avanzada
            temporal_description = self.generate_hierarchical_structure_temporal_description(structure)
            if temporal_description:
                operation_part += f" {temporal_description}"
            
            parts.append(operation_part)
            print(f"   ‚úÖ Operaci√≥n+M√©trica tradicional: {operation_part}")
        
        elif structure.operations:
            op = structure.operations[0]
            parts.append(f"({op.text})")
            print(f"   ‚úÖ Solo operaci√≥n: ({op.text})")
            
        elif structure.metrics:
            # üîß Solo agregar m√©tricas que NO est√°n en filtros
            metrics_not_in_filters = []
            for metric in structure.metrics:
                used_in_filter = any(
                    cvp.column_name == metric.text 
                    for cvp in structure.column_conditions
                )
                if not used_in_filter:
                    metrics_not_in_filters.append(metric)
            
            if metrics_not_in_filters:
                metric = metrics_not_in_filters[0]
                parts.append(f"({metric.text})")
        
        # PASO 7: Combinar partes con l√≥gica correcta
        if len(parts) == 1:
            result = parts[0]
        elif len(parts) == 2:
            # Verificar si TODAS las condiciones son temporales
            all_conditions_are_temporal = all(
                condition.column_name in temporal_columns 
                for condition in structure.column_conditions
            )
            
            if all_conditions_are_temporal and structure.main_dimension:
                # Caso: dimensi√≥n + operaci√≥n temporal (sin filtros adicionales)
                result = f"{parts[0]} con {parts[1]}"
                print(f"   üîß Combinaci√≥n especial (dimensi√≥n con operaci√≥n temporal): {result}")
            else:
                # Caso: m√∫ltiples condiciones independientes
                result = f"{' Y '.join(parts)}"
                print(f"   üîß Combinaci√≥n est√°ndar (m√∫ltiples condiciones): {result}")
        elif len(parts) > 2:
            result = f"{' Y '.join(parts)}"
        else:
            result = "estructura_incompleta"
        
        print(f"   üéØ Resultado final COMPUESTO: {result}")
        return result



    def generate_hierarchical_structure_temporal_description(self, structure: QueryStructure) -> str:
        """Genera descripci√≥n temporal avanzada para estructura jer√°rquica"""
        temporal_parts = []
        
        # NUEVA L√ìGICA: Usar informaci√≥n temporal avanzada si est√° disponible
        if hasattr(self, 'advanced_temporal_info') and self.advanced_temporal_info:
            for advanced_info in self.advanced_temporal_info:
                if advanced_info.is_range_from:
                    if advanced_info.original_filter.unit == TemporalUnit.WEEKS:
                        temporal_parts.append(f"desde semana {advanced_info.start_value}")
                    elif advanced_info.original_filter.unit == TemporalUnit.MONTHS:
                        temporal_parts.append(f"desde mes {advanced_info.start_value}")
                    elif advanced_info.original_filter.unit == TemporalUnit.DAYS:
                        temporal_parts.append(f"desde d√≠a {advanced_info.start_value}")
                elif advanced_info.is_range_between:
                    if advanced_info.original_filter.unit == TemporalUnit.WEEKS:
                        temporal_parts.append(f"de semana {advanced_info.start_value} a {advanced_info.end_value}")
                    elif advanced_info.original_filter.unit == TemporalUnit.MONTHS:
                        temporal_parts.append(f"de mes {advanced_info.start_value} a {advanced_info.end_value}")
                    elif advanced_info.original_filter.unit == TemporalUnit.DAYS:
                        temporal_parts.append(f"de d√≠a {advanced_info.start_value} a {advanced_info.end_value}")
                elif advanced_info.is_range_to:
                    if advanced_info.original_filter.unit == TemporalUnit.WEEKS:
                        temporal_parts.append(f"hasta semana {advanced_info.end_value}")
                    elif advanced_info.original_filter.unit == TemporalUnit.MONTHS:
                        temporal_parts.append(f"hasta mes {advanced_info.end_value}")
                    elif advanced_info.original_filter.unit == TemporalUnit.DAYS:
                        temporal_parts.append(f"hasta d√≠a {advanced_info.end_value}")
                else:
                    # Filtros tradicionales existentes
                    tf = advanced_info.original_filter
                    if tf.filter_type == "specific":
                        if tf.unit == TemporalUnit.WEEKS:
                            temporal_parts.append(f"en semana {tf.quantity}")
                        elif tf.unit == TemporalUnit.MONTHS:
                            temporal_parts.append(f"en mes {tf.quantity}")
                        elif tf.unit == TemporalUnit.DAYS:
                            temporal_parts.append(f"en d√≠a {tf.quantity}")
                    else:
                        temporal_parts.append(f"en las {tf.indicator} {tf.quantity} {tf.unit.value}")
        else:
            # FALLBACK: Usar filtros temporales tradicionales (para compatibilidad)
            for tf in structure.temporal_filters:
                if tf.filter_type == "specific":
                    if tf.unit == TemporalUnit.WEEKS:
                        temporal_parts.append(f"en semana {tf.quantity}")
                    elif tf.unit == TemporalUnit.MONTHS:
                        temporal_parts.append(f"en mes {tf.quantity}")
                    elif tf.unit == TemporalUnit.DAYS:
                        temporal_parts.append(f"en d√≠a {tf.quantity}")
                else:
                    temporal_parts.append(f"en las {tf.indicator} {tf.quantity} {tf.unit.value}")
        
        return ' y '.join(temporal_parts) if temporal_parts else ""

# ------  "Debugger de estructura jerarquica" -------

    def debug_hierarchical_structure(self, structure: QueryStructure) -> Dict:
        """Debugger de Estructura Jer√°rquica"""
        debug_info = {
            'main_dimension': structure.main_dimension.text if structure.main_dimension else None,
            'column_conditions': [f"{cvp.column_name} = '{cvp.value}'" for cvp in structure.column_conditions],
            'operations': [op.text for op in structure.operations],
            'metrics': [m.text for m in structure.metrics],
            'temporal_filters': [f"{tf.indicator} {tf.quantity} {tf.unit.value}" for tf in structure.temporal_filters]
        }
        
        # Verificar si dimensi√≥n est√° en filtros
        dimension_in_filter = False
        if structure.main_dimension and structure.column_conditions:
            main_dim_name = structure.main_dimension.text
            for condition in structure.column_conditions:
                if condition.column_name == main_dim_name:
                    dimension_in_filter = True
                    break
        
        debug_info['dimension_in_filter'] = dimension_in_filter
        
        # Construir paso a paso
        construction_steps = []
        
        if structure.main_dimension and not dimension_in_filter:
            construction_steps.append(f"PASO 1: Dimensi√≥n principal ‚Üí ({structure.main_dimension.text})")
            if structure.column_conditions:
                conditions = [f"({cvp.column_name} = '{cvp.value}')" for cvp in structure.column_conditions]
                construction_steps.append(f"PASO 2: Agregar filtros ‚Üí con {' y '.join(conditions)}")
        elif structure.column_conditions:
            filters = [f"({cvp.column_name} = '{cvp.value}')" for cvp in structure.column_conditions]
            construction_steps.append(f"PASO 1: Filtros directos ‚Üí {' Y '.join(filters)}")
        
        if structure.operations and structure.metrics:
            op = structure.operations[0]
            metric = structure.metrics[0]
            construction_steps.append(f"PASO FINAL: Operaci√≥n + M√©trica ‚Üí (({op.text}) ({metric.text}))")
        
        debug_info['construction_steps'] = construction_steps
        
        return debug_info


# ------  "Generador de interpretacion natural" -------

    def generate_natural_interpretation(self, structure: QueryStructure) -> str:
        """üîß Generador de Interpretaci√≥n Natural - VERSI√ìN MULTI-CRITERIO"""
        
        # CASO ESPECIAL: Rankings - L√ìGICA MULTI-CRITERIO CORREGIDA
        if structure.is_ranking_query and structure.ranking_criteria:
            ranking = structure.ranking_criteria
            parts = []
            
            # Construir interpretaci√≥n espec√≠fica para rankings
            direction_text = "los mejores" if ranking.direction == RankingDirection.TOP else "los peores"
            
            if ranking.unit == RankingUnit.COUNT:
                parts.append(f"Encontrar {direction_text} {int(ranking.value)} {structure.main_dimension.text}")
            else:  # PERCENTAGE
                parts.append(f"Encontrar {direction_text} {ranking.value}% de {structure.main_dimension.text}")
            
            # üîß NUEVA L√ìGICA: Describir TODOS los criterios
            if len(structure.metrics) > 1:
                print(f"   üîó Generando interpretaci√≥n multi-criterio")
                
                # Obtener operaciones disponibles
                operations_available = [op.text.lower() for op in structure.operations if op.text.lower() in ['mas', 'm√°s', 'mayor', 'menor']]
                metrics_available = [m.text for m in structure.metrics]
                
                criteria_descriptions = []
                for i, metric in enumerate(metrics_available):
                    if i < len(operations_available):
                        op = operations_available[i]
                    else:
                        op = operations_available[0] if operations_available else 'mas'
                    
                    if op in ['mas', 'm√°s', 'mayor']:
                        criteria_descriptions.append(f"mayor {metric}")
                    elif op in ['menor', 'minimo', 'm√≠nimo']:
                        criteria_descriptions.append(f"menor {metric}")
                    else:
                        criteria_descriptions.append(f"{op} {metric}")
                
                # Combinar criterios con "y"
                if len(criteria_descriptions) == 2:
                    combined_criteria = f" y ".join(criteria_descriptions)
                else:
                    combined_criteria = ", ".join(criteria_descriptions[:-1]) + f" y {criteria_descriptions[-1]}"
                
                parts.append(f"basado en {combined_criteria}")
                
            else:
                # L√ìGICA ORIGINAL: Un solo criterio
                if ranking.metric:
                    if ranking.operation and ranking.operation.text.lower() in ['mas', 'm√°s', 'mayor']:
                        parts.append(f"con mayor volumen total de {ranking.metric.text}")
                    elif ranking.operation and ranking.operation.text.lower() in ['menos', 'menor']:
                        parts.append(f"con menor volumen total de {ranking.metric.text}")
                    else:
                        parts.append(f"basado en {ranking.metric.text}")
            
            # Agregar filtros temporales (l√≥gica existente)
            if structure.temporal_filters:
                for tf in structure.temporal_filters:
                    if tf.filter_type == "range_between":
                        # Usar informaci√≥n temporal avanzada si est√° disponible
                        if hasattr(self, 'advanced_temporal_info') and self.advanced_temporal_info:
                            for advanced_info in self.advanced_temporal_info:
                                if advanced_info.is_range_between:
                                    if tf.unit == TemporalUnit.WEEKS:
                                        parts.append(f"entre semana {advanced_info.start_value} y {advanced_info.end_value}")
                                    elif tf.unit == TemporalUnit.MONTHS:
                                        parts.append(f"entre mes {advanced_info.start_value} y {advanced_info.end_value}")
                        else:
                            parts.append(f"en rango temporal")
                    elif tf.filter_type == "specific":
                        if tf.unit == TemporalUnit.WEEKS:
                            parts.append(f"en la semana n√∫mero {tf.quantity}")
                        elif tf.unit == TemporalUnit.MONTHS:
                            parts.append(f"en el mes n√∫mero {tf.quantity}")
                    else:
                        parts.append(f"en las {tf.indicator} {tf.quantity} {tf.unit.value}")
            
            # Agregar otros filtros (l√≥gica existente)
            if structure.column_conditions:
                conditions = []
                for condition in structure.column_conditions:
                    conditions.append(f"donde {condition.column_name} = '{condition.value}'")
                parts.extend(conditions)
            
            interpretation = ", ".join(parts)
            return interpretation.capitalize() if interpretation else "Consulta de ranking sin interpretaci√≥n clara"
        
        
        # L√ìGICA ORIGINAL PARA CONSULTAS NO-RANKING
        parts = []
        
        # Parte principal
        if structure.main_dimension:
            parts.append(f"Encontrar {structure.main_dimension.text}")
        
        # Condiciones de columna
        if structure.column_conditions:
            conditions = []
            for condition in structure.column_conditions:
                conditions.append(f"donde {condition.column_name} = '{condition.value}'")
            parts.append(", ".join(conditions))
        
        # Operaci√≥n y m√©trica
        if structure.operations and structure.metrics:
            operation = structure.operations[0]
            metric = structure.metrics[0]
            
            if operation.value == 'm√°ximo':
                parts.append(f"con el mayor valor en {metric.text}")
            elif operation.value == 'm√≠nimo':
                parts.append(f"con el menor valor en {metric.text}")
            else:
                parts.append(f"calculando {operation.value} de {metric.text}")
        elif structure.operations:
            operation = structure.operations[0]
            parts.append(f"con {operation.value}")
        elif structure.metrics:
            metric = structure.metrics[0]
            parts.append(f"relacionado con {metric.text}")
        
        # Filtros temporales
        if structure.temporal_filters:
            for tf in structure.temporal_filters:
                if tf.filter_type == "specific":
                    if tf.unit == TemporalUnit.WEEKS:
                        parts.append(f"en la semana n√∫mero {tf.quantity}")
                    elif tf.unit == TemporalUnit.MONTHS:
                        parts.append(f"en el mes n√∫mero {tf.quantity}")
                    elif tf.unit == TemporalUnit.DAYS:
                        parts.append(f"en el d√≠a n√∫mero {tf.quantity}")
                else:
                    parts.append(f"en las {tf.indicator} {tf.quantity} {tf.unit.value}")
        
        interpretation = ", ".join(parts)
        return interpretation.capitalize() if interpretation else "Consulta sin interpretaci√≥n clara"


# ------  "Convertidor de estructura a diccionario" -------

    def structure_to_dict(self, structure: QueryStructure) -> Dict:
        """Convertidor de Estructura a Diccionario"""
        return {
            'main_dimension': self.component_to_dict(structure.main_dimension) if structure.main_dimension else None,
            'operations': [self.component_to_dict(op) for op in structure.operations],
            'metrics': [self.component_to_dict(m) for m in structure.metrics],
            'column_conditions': [self.cvp_to_dict(cvp) for cvp in structure.column_conditions],
            'temporal_filters': [self.temporal_to_dict(tf) for tf in structure.temporal_filters],
            'values': [self.component_to_dict(v) for v in structure.values],
            'connectors': [self.component_to_dict(c) for c in structure.connectors],
            'unknown_tokens': [self.component_to_dict(u) for u in structure.unknown_tokens],
            'complexity_level': structure.get_complexity_level()
        }


# ------  "Convertidor de componente a diccionario" -------

    def component_to_dict(self, component: QueryComponent) -> Dict:
        """Convertidor de Componente a Diccionario"""
        if not component:
            return None
        
        return {
            'text': component.text,
            'type': component.type.value,
            'confidence': component.confidence,
            'subtype': component.subtype,
            'value': component.value,
            'column_name': component.column_name,
            'linguistic_info': component.linguistic_info
        }


# ------  "Convertidor de par columna-valor" -------

    def cvp_to_dict(self, cvp: ColumnValuePair) -> Dict:
        """Convertidor de Par Columna-Valor"""
        return {
            'column_name': cvp.column_name,
            'value': cvp.value,
            'confidence': cvp.confidence,
            'raw_text': cvp.raw_text
        }


# ------  "Convertidor de filtro temporal" -------

    def temporal_to_dict(self, tf: TemporalFilter) -> Dict:
        """Convertidor de Filtro Temporal"""
        return {
            'indicator': tf.indicator,
            'quantity': tf.quantity,
            'unit': tf.unit.value,
            'confidence': tf.confidence,
            'filter_type': tf.filter_type
        }


# ------  "Inferidor de dimension por defecto" -------

    def infer_default_dimension_for_ranking(self, ranking_criteria: RankingCriteria) -> Optional[QueryComponent]:
        """Inferidor de Dimensi√≥n por Defecto"""
        # Dimensiones comunes por m√©trica
        metric_to_dimension = {
            'ventas': 'account',
            'venta': 'account', 
            'inventario': 'product',
            'margen': 'product',
            'revenue': 'account',
            'sales': 'account'
        }
        
        if ranking_criteria and ranking_criteria.metric:
            metric_text = ranking_criteria.metric.text.lower()
            if metric_text in metric_to_dimension:
                inferred_dim = metric_to_dimension[metric_text]
                
                return QueryComponent(
                    text=inferred_dim,
                    type=ComponentType.DIMENSION,
                    confidence=0.75,  # Confianza media por ser inferida
                    subtype='inferred',
                    linguistic_info={'source': 'inferred_for_ranking'}
                )
        
        return None




        # ========================================
        # GRUPO 8: INTERFAZ DE USUARIO 
        # Interfaz de usuario y sesi√≥n interactiva
        # ========================================



# ------  "Mostrador de resultados unificados" -------

    def display_unified_result(self, result: Dict):
        """Mostrar resultado unificado con informaci√≥n de schema mapping"""
        
        if result['success']:
            print("‚úÖ CONSULTA PROCESADA EXITOSAMENTE")
            print("="*80)
            print(f"üìù Input Original: '{result['original_input']}'")
            print(f"üîÑ Consulta Normalizada: '{result['normalized_query']}'")
            print(f"‚≠ê Confianza General: {result['confidence']}")
            print(f"üìä Complejidad: {result['complexity_level'].upper()}")
            
    # üÜï MOSTRAR AMBOS SQLS
            if 'conceptual_sql' in result:
                print(f"\nüîß SQL CONCEPTUAL:")
                print(f"   {result['conceptual_sql']}")
            
            print(f"\nüóÑÔ∏è SQL NORMALIZADO:")
            print(f"   {result['sql_query']}")
            
    # üÜï MOSTRAR ESTAD√çSTICAS DE MAPEO
            if 'schema_mapping_stats' in result:
                stats = result['schema_mapping_stats']
                print(f"\nüìä SCHEMA MAPPING STATS:")
                print(f"   üìç Dimension anchors: {stats['total_dimension_anchors']}")
                print(f"   üìà Metric anchors: {stats['total_metric_anchors']}")
                print(f"   üîÑ Total mappings: {stats['total_reverse_mappings']}")
            if not result.get('success', False):
                print("\n‚ùå ERROR EN LA CONSULTA")
                print("="*70)
                print(f"üìù Input: '{result.get('original_input', 'N/A')}'")
                print(f"‚ùå Error: {result.get('error', 'Error desconocido')}")
                
    # üÜï MOSTRAR INFORMACI√ìN DE PALABRAS DESCONOCIDAS
                if result.get('error_type') == 'unknown_words':
                    self._display_unknown_words_error(result)
                
                elif result.get('suggestions'):
                    print("\nüí° SUGERENCIAS:")
                    for i, suggestion in enumerate(result['suggestions'], 1):
                        print(f"  {i}. {suggestion}")
                
                return
            
    # üÜï MOSTRAR ADVERTENCIAS DE PALABRAS SOSPECHOSAS
            if result.get('unknown_words_detected', 0) > 0:
                print(f"\n‚ö†Ô∏è ADVERTENCIA: {result['unknown_words_detected']} palabras con baja confianza detectadas")
            
            # TU C√ìDIGO EXISTENTE PARA MOSTRAR RESULTADOS EXITOSOS...
            print("\n‚úÖ CONSULTA PROCESADA EXITOSAMENTE")
            print("="*80)
            print(f"üìù Input Original: '{result['original_input']}'")
            print(f"üîÑ Consulta Normalizada: '{result['normalized_query']}'")
            print(f"‚≠ê Confianza General: {result.get('confidence', 0):.2f}")
            print(f"üìä Complejidad: {result.get('complexity_level', 'desconocida').upper()}")
            
    # ESTRUCTURA JER√ÅRQUICA
            print(f"\nüèóÔ∏è  ESTRUCTURA JER√ÅRQUICA:")
            print(f"   {result.get('hierarchical_structure', 'N/A')}")
            
    # DESGLOSE DETALLADO DE COMPONENTES
            self.show_detailed_component_breakdown(result)
            
    # SQL Y INTERPRETACI√ìN
            print(f"\nüóÑÔ∏è  SQL GENERADO:")
            print(f"   {result.get('sql_query', 'N/A')}")
            
            print(f"\nüí° INTERPRETACI√ìN NATURAL:")
            print(f"   {result.get('interpretation', 'N/A')}")
            
            print("="*80)
        
    
    
# ------  "Mostrador de palabras desconocidas encontradas"--------
    
    def _display_unknown_words_error(self, result: Dict):
        """Mostrar detalles de error por palabras desconocidas"""
        feedback = result.get('unknown_words_feedback', {})
        
        print(f"\nüö® PALABRAS NO RECONOCIDAS DETECTADAS:")
        print(f"   üìä Total: {result.get('unknown_words_count', 0)}")
        
        if feedback.get('unknown_words'):
            print(f"\nüìã DETALLES:")
            for word_info in feedback['unknown_words']:
                severity_icon = "üö®" if word_info['severity'] == 'critical' else "‚ö†Ô∏è"
                print(f"   {severity_icon} '{word_info['word']}' en posici√≥n {word_info['position']}")
                print(f"      Contexto: {word_info['context']}")
                print(f"      Confianza: {word_info['confidence']:.2f}")
        
        if feedback.get('similar_words'):
            print(f"\nüí° PALABRAS SIMILARES ENCONTRADAS:")
            for similar in feedback['similar_words']:
                print(f"   üîÑ '{similar['original']}' ‚Üí ¬ø'{similar['suggested']}'?")
        
        print(f"\nüí° SUGERENCIAS:")
        for suggestion in feedback.get('suggestions', []):
            print(f"   ‚Ä¢ {suggestion}")


# ------  "Mostrador de estadisticas de palabras desconocidas" -----

    def show_unknown_words_statistics(self):
        """üìä MOSTRAR ESTAD√çSTICAS DE PALABRAS DESCONOCIDAS"""
        stats = self.unknown_words_log['statistics']
        
        print(f"\nüìä ESTAD√çSTICAS DE PALABRAS DESCONOCIDAS")
        print("="*60)
        print(f"üìà Total consultas fallidas: {stats.get('total_failures', 0)}")
        print(f"üìã Consultas registradas: {len(self.unknown_words_log['failures'])}")
        
        common_words = stats.get('most_common_unknown_words', {})
        if common_words:
            print(f"\nüîù TOP PALABRAS DESCONOCIDAS:")
            sorted_words = sorted(common_words.items(), key=lambda x: x[1]['count'], reverse=True)
            for i, (word, info) in enumerate(sorted_words[:10], 1):
                print(f"  {i:2d}. '{word}' ‚Üí {info['count']} veces")
        
        print("="*60)


# ------  "Mostrador de desglose detallado" -------

    def show_detailed_component_breakdown(self, result: Dict):
        """Mostrador de Desglose Detallado"""
        print(f"\nüîç DESGLOSE DETALLADO DE COMPONENTES:")
        print("-" * 60)
        
        structure = result.get('query_structure', {})
        
        # Dimensi√≥n principal
        main_dim = structure.get('main_dimension')
        if main_dim:
            print(f"\nüéØ DIMENSI√ìN PRINCIPAL:")
            print(f"  ‚úÖ '{main_dim['text']}' ‚Üí {main_dim['type']} (confianza: {main_dim['confidence']:.2f})")
        
        # Operaciones
        operations = structure.get('operations', [])
        if operations:
            print(f"\n‚ö° OPERACIONES ({len(operations)}):")
            for op in operations:
                print(f"  ‚úÖ '{op['text']}' ‚Üí {op['value']} (confianza: {op['confidence']:.2f})")
        
        # M√©tricas
        metrics = structure.get('metrics', [])
        if metrics:
            print(f"\nüìä M√âTRICAS ({len(metrics)}):")
            for metric in metrics:
                print(f"  ‚úÖ '{metric['text']}' ‚Üí medida a analizar (confianza: {metric['confidence']:.2f})")
        
        # Condiciones de columna
        column_conditions = structure.get('column_conditions', [])
        if column_conditions:
            print(f"\nüéõÔ∏è  CONDICIONES DE COLUMNA ({len(column_conditions)}):")
            for condition in column_conditions:
                print(f"  ‚úÖ '{condition['raw_text']}' ‚Üí WHERE {condition['column_name']} = '{condition['value']}' (confianza: {condition['confidence']:.2f})")
        
        # Filtros temporales
        temporal_filters = structure.get('temporal_filters', [])
        if temporal_filters:
            print(f"\n‚è∞ FILTROS TEMPORALES ({len(temporal_filters)}):")
            for tf in temporal_filters:
                filter_type_desc = "espec√≠fico" if tf['filter_type'] == "specific" else "rango"
                print(f"  ‚úÖ '{tf['indicator']} {tf['quantity']} {tf['unit']}' ‚Üí {filter_type_desc} (confianza: {tf['confidence']:.2f})")
        
        # Tokens no reconocidos
        unknown = structure.get('unknown_tokens', [])
        if unknown:
            print(f"\n‚ùì TOKENS NO RECONOCIDOS ({len(unknown)}):")
            for token in unknown:
                print(f"  ‚ö†Ô∏è  '{token['text']}' (confianza: {token['confidence']:.2f})")


# ------  "Mostrador de estadisticas de sesion" -------

    def show_session_stats(self):
        """Mostrador de Estad√≠sticas de Sesi√≥n"""
        print("\nüìä ESTAD√çSTICAS DE LA SESI√ìN")
        print("="*50)
        
        duration = datetime.now() - self.session_stats['session_start']
        success_rate = 0
        if self.session_stats['total_queries'] > 0:
            success_rate = (self.session_stats['successful_queries'] / self.session_stats['total_queries']) * 100
        
        print(f"‚è±Ô∏è  Duraci√≥n: {duration}")
        print(f"üìà Total consultas: {self.session_stats['total_queries']}")
        print(f"‚úÖ Exitosas: {self.session_stats['successful_queries']}")
        print(f"‚ùå Fallidas: {self.session_stats['failed_queries']}")
        print(f"üéØ Tasa de √©xito: {success_rate:.1f}%")
        print(f"üìù Consultas simples: {self.session_stats['simple_queries']}")
        print(f"üîß Consultas complejas: {self.session_stats['complex_queries']}")
        
        # Informaci√≥n adicional si hay historial
        if self.query_history:
            print(f"\nüìã HISTORIAL RECIENTE:")
            recent_queries = self.query_history[-5:]  # √öltimas 5 consultas
            for i, entry in enumerate(recent_queries, 1):
                status = "‚úÖ" if entry.get('processed', False) else "‚ùå"
                print(f"  {i}. {status} [{entry['timestamp']}] '{entry['input'][:50]}{'...' if len(entry['input']) > 50 else ''}'")
        
        print("="*50)


# ------  "Ejecutor de sesion interactiva" -------

    def run_interactive_session(self):
        """Ejecutor de Sesi√≥n Interactiva - VERSI√ìN MEJORADA"""
        print("\nü§ñ PARSER NLP UNIFICADO - SESI√ìN INTERACTIVA")
        print("="*60)
        print("‚úÖ Sistema de detecci√≥n de palabras desconocidas ACTIVADO")
        print("üö® Las consultas problem√°ticas se detendr√°n autom√°ticamente")
        print("="*60)
        
        while True:
            try:
                print(f"\n[Consultas: {self.session_stats['total_queries']}] ", end="")
                user_input = input("üîç Ingresa tu consulta: ").strip()
                
                if not user_input:
                    continue
                
                command = user_input.lower()
                
                # COMANDOS ESPECIALES EXISTENTES...
                if command in ['salir', 'exit', 'quit']:
                    print("\nüëã ¬°Gracias por usar el Parser NLP Unificado!")
                    self.show_session_stats()
                    self.show_unknown_words_statistics()  # üÜï MOSTRAR ESTAD√çSTICAS ADICIONALES
                    break
                
                elif command in ['unknown', 'desconocidas', 'stats_unknown']:
                    self.show_unknown_words_statistics()
                    continue
            
                
                elif command in ['ayuda', 'help']:
                    self.show_help()
                    continue
                
                elif command in ['stats', 'estadisticas']:
                    self.show_session_stats()
                    continue
                
                elif command in ['diccionarios', 'dict']:
                    try:
                        self.dictionaries.show_dictionary_info()
                    except AttributeError:
                        print("üìö Informaci√≥n de diccionarios no disponible")
                    continue
                
                elif command in ['historial', 'history']:
                    self._show_query_history()
                    continue
                
                elif command in ['limpiar', 'clear']:
                    self._clear_session()
                    continue
                
                elif command in ['test', 'prueba']:
                    self._run_test_queries()
                    continue
                
                # PROCESAR CONSULTA NORMAL
                print("\nüîç Procesando consulta unificada...")
                result = self.process_user_input(user_input)
                self.display_unified_result(result)
                
            except KeyboardInterrupt:
                print("\n\nüëã Sesi√≥n interrumpida por el usuario")
                break
            except Exception as e:
                print(f"\n‚ùå Error inesperado: {e}")


# ------  "Ayuda del sistema" -------

    def show_help(self):
        """Mostrador de Ayuda del Sistema"""
        print("\nü§ñ PARSER NLP UNIFICADO - AYUDA")
        print("="*50)
        print("Procesamiento autom√°tico de consultas simples y complejas")
        
        print("\nüìã COMANDOS DISPONIBLES:")
        print("  ‚Ä¢ Escribe cualquier consulta en lenguaje natural")
        print("  ‚Ä¢ 'stats' - Ver estad√≠sticas de la sesi√≥n")
        print("  ‚Ä¢ 'diccionarios' - Ver informaci√≥n de diccionarios")
        print("  ‚Ä¢ 'historial' - Ver historial de consultas")
        print("  ‚Ä¢ 'limpiar' - Limpiar sesi√≥n y estad√≠sticas")
        print("  ‚Ä¢ 'test' - Ejecutar consultas de prueba")
        print("  ‚Ä¢ 'ayuda' - Mostrar esta ayuda")
        print("  ‚Ä¢ 'salir' - Terminar sesi√≥n")
        
        print("\nüéØ TIPOS DE CONSULTAS SOPORTADOS:")
        print("  üìù SIMPLES: 'partner code con mayor ventas'")
        print("  üîß COMPLEJAS: 'customer id con sell out mayor sales amount'")
        print("  üìä CON VALORES: 'product group con estado A mayor precio'")
        print("  ‚è∞ CON TIEMPO ESPEC√çFICO: 'vendor code mayor venta semana 8'")
        print("  ‚è∞ CON RANGO TEMPORAL: 'account code suma ventas ultimas 3 semanas'")
        print("  üèÜ RANKINGS: 'top 5 accounts por ventas'")
        print("  üîó COMPUESTAS: 'account con mas inventario y menor venta'")
        
        print("\n‚úÖ FRASES COMPUESTAS SOPORTADAS:")
        print("  üè∑Ô∏è  partner code, customer code, vendor code")
        print("  üÜî partner id, customer id, user id")
        print("  üìä sales amount, total amount, sell out")
        print("  üè¢ product group, cost center, sales area")
        
        print("\nüí° EJEMPLOS DE CONSULTAS:")
        print("  üîπ 'partner code con mas ventas'")
        print("  üîπ 'top 5 products por sales amount'")
        print("  üîπ 'account region A con mayor inventario'")
        print("  üîπ 'customer con mas revenue y menor costo'")
        print("  üîπ 'suma ventas ultimas 8 semanas'")
        
        print("\nüö® CONSEJOS:")
        print("  ‚Ä¢ Usa frases compuestas (partner_code, no partner code)")
        print("  ‚Ä¢ S√© espec√≠fico con operaciones (mas, mayor, suma)")
        print("  ‚Ä¢ Para rankings usa: top, mejores, primeros + n√∫mero")
        print("  ‚Ä¢ Para filtros: entidad + valor (region A, estado ACTIVO)")
        
        print("="*50)


    def _show_query_history(self):
        """Mostrador de Historial de Consultas"""
        print("\nüìã HISTORIAL DE CONSULTAS")
        print("-" * 60)
        
        if not self.query_history:
            print("üìù No hay consultas en el historial")
            return
        
        for i, entry in enumerate(self.query_history, 1):
            status = "‚úÖ EXITOSA" if entry.get('processed', False) else "‚ùå FALLIDA"
            print(f"\n{i}. [{entry['timestamp']}] {status}")
            print(f"   üìù Input: '{entry['input']}'")
            
            if entry.get('processed', False) and entry.get('result'):
                result = entry['result']
                print(f"   üèóÔ∏è Estructura: {result.get('hierarchical_structure', 'N/A')}")
                print(f"   üìä Complejidad: {result.get('complexity_level', 'N/A')}")
                print(f"   ‚≠ê Confianza: {result.get('confidence', 0):.2f}")
            elif entry.get('error'):
                print(f"   ‚ùå Error: {entry['error']}")
        
        print(f"\nüìä Total: {len(self.query_history)} consultas")


    def _clear_session(self):
        """Limpiador de Sesi√≥n"""
        print("\nüßπ LIMPIANDO SESI√ìN...")
        
        # Reiniciar estad√≠sticas
        self.session_stats = {
            'total_queries': 0,
            'successful_queries': 0,
            'failed_queries': 0,
            'simple_queries': 0,
            'complex_queries': 0,
            'session_start': datetime.now()
        }
        
        # Limpiar historial
        self.query_history = []
        
        print("‚úÖ Sesi√≥n limpiada exitosamente")
        print("üìä Estad√≠sticas reiniciadas")
        print("üìã Historial borrado")


    def _run_test_queries(self):
        """Ejecutor de Consultas de Prueba"""
        print("\nüß™ EJECUTANDO CONSULTAS DE PRUEBA")
        print("="*50)
        
        test_queries = [
            "partner code con mas ventas",
            "top 5 accounts por revenue",
            "product group con estado A mayor precio",
            "customer con mas inventario y menor costo",
            "suma sales amount ultimas 4 semanas",
            "account region norte con mayor margen",
            "mejores 10% vendors por sell out"
        ]
        
        for i, query in enumerate(test_queries, 1):
            print(f"\nüî¨ PRUEBA {i}: '{query}'")
            print("-" * 40)
            
            try:
                result = self.process_user_input(query)
                
                if result.get('success'):
                    print(f"‚úÖ EXITOSA")
                    print(f"üìä Complejidad: {result.get('complexity_level', 'N/A')}")
                    print(f"‚≠ê Confianza: {result.get('confidence', 0):.2f}")
                    print(f"üóÑÔ∏è SQL: {result.get('sql_query', 'N/A')}")
                else:
                    print(f"‚ùå FALLIDA: {result.get('error', 'Error desconocido')}")
                    
            except Exception as e:
                print(f"üö® ERROR: {str(e)}")
        
        print(f"\nüìä PRUEBAS COMPLETADAS")
        print("="*50)




    # =============================================
    # MAPEADOR DE TOKENS CON DICCIONARIOS COMPLEJOS
    # =============================================


class SQLSchemaMapper:
    """
    Mapea SQL conceptual a SQL con nombres reales de columnas usando diccionarios anchor.
    √öltimo paso del pipeline NLP para normalizar consultas SQL.
    """
    
    def __init__(self):
        """Inicializa el mapeador cargando diccionarios de anchor"""
        self.dimension_anchors = {}
        self.metric_anchors = {}
        self.reverse_mapping = {}  # Para b√∫squeda r√°pida: palabra ‚Üí anchor
        
        # Rutas de los diccionarios anchor
        self.dimension_path = Path("diccionarios/complejos/anchors/dimension_anchors.json")
        self.metric_path = Path("diccionarios/complejos/anchors/metric_anchors.json")
        
        # Cargar diccionarios
        self._load_anchor_dictionaries()
        
        print(f"üîó SQLSchemaMapper inicializado:")
        print(f"   üìç Dimensiones: {len(self.dimension_anchors)} anchors")
        print(f"   üìä M√©tricas: {len(self.metric_anchors)} anchors")
        print(f"   üîÑ Mapeos reversos: {len(self.reverse_mapping)} palabras")
    
    
    def _load_anchor_dictionaries(self):
        """Carga los diccionarios de anchor desde archivos JSON"""
        try:
            # Cargar dimension anchors
            if self.dimension_path.exists():
                with open(self.dimension_path, 'r', encoding='utf-8') as f:
                    self.dimension_anchors = json.load(f)
                    print(f"‚úÖ Cargado dimension_anchors.json: {len(self.dimension_anchors)} entradas")
            else:
                print(f"‚ö†Ô∏è No encontrado: {self.dimension_path}")
            
            # Cargar metric anchors
            if self.metric_path.exists():
                with open(self.metric_path, 'r', encoding='utf-8') as f:
                    self.metric_anchors = json.load(f)
                    print(f"‚úÖ Cargado metric_anchors.json: {len(self.metric_anchors)} entradas")
            else:
                print(f"‚ö†Ô∏è No encontrado: {self.metric_path}")
            
            # Construir mapeo reverso para b√∫squeda r√°pida
            self._build_reverse_mapping()
            
        except Exception as e:
            print(f"‚ùå Error cargando diccionarios anchor: {e}")
            print("üîÑ Continuando con diccionarios vac√≠os")
    
    
    def _build_reverse_mapping(self):
        """Construye mapeo reverso: palabra ‚Üí nombre_anchor para b√∫squeda r√°pida"""
        self.reverse_mapping = {}
        
        # Procesar dimension anchors
        for anchor_name, synonyms in self.dimension_anchors.items():
            for synonym in synonyms:
                synonym_lower = synonym.lower().strip()
                if synonym_lower:
                    self.reverse_mapping[synonym_lower] = {
                        'anchor': anchor_name,
                        'type': 'dimension'
                    }
        
        # Procesar metric anchors
        for anchor_name, synonyms in self.metric_anchors.items():
            for synonym in synonyms:
                synonym_lower = synonym.lower().strip()
                if synonym_lower:
                    self.reverse_mapping[synonym_lower] = {
                        'anchor': anchor_name,
                        'type': 'metric'
                    }
        
        print(f"üîÑ Mapeo reverso construido: {len(self.reverse_mapping)} palabras mapeadas")
    
    
    def normalize_sql(self, conceptual_sql: str) -> str:
        """
        üîß NORMALIZADOR SQL CON MANEJO DE ERRORES ROBUSTO
        """
        
        print(f"üîó NORMALIZANDO SQL (Enhanced - Robust):")
        print(f"   üì• Input: {conceptual_sql}")
        
        try:
            sql = conceptual_sql
            replacements_made = 0
            
            # PASO 1: NORMALIZAR COLUMNAS DENTRO DE FUNCIONES SQL
            import re
            
            function_pattern = r'(\w+)\s*\(\s*(DISTINCT\s+)?([a-zA-Z_][a-zA-Z0-9_]*)\s*\)'
            
            def replace_function_column(match):
                nonlocal replacements_made
                try:
                    function_name = match.group(1)
                    distinct_part = match.group(2) or ""
                    column_name = match.group(3)
                    
                    print(f"   üîç Function found: {function_name}({distinct_part}{column_name})")
                    
                    normalized_column = self._find_column_mapping_anchors_only(column_name)
                    
                    if normalized_column:
                        new_function = f'{function_name}({distinct_part}{normalized_column})'
                        print(f"   üîÑ Function mapping: {function_name}({distinct_part}{column_name}) ‚Üí {new_function}")
                        replacements_made += 1
                        return new_function
                    else:
                        print(f"   ‚ùì Function column '{column_name}' no mapping found")
                        return match.group(0)
                        
                except Exception as e:
                    print(f"   ‚ùå Error in replace_function_column: {e}")
                    return match.group(0)
            
            # Aplicar reemplazos en funciones
            sql = re.sub(function_pattern, replace_function_column, sql, flags=re.IGNORECASE)
            
            # PASO 2: NORMALIZAR COLUMNAS INDEPENDIENTES
            standalone_columns = self._find_standalone_columns(sql)
            
            for column in standalone_columns:
                try:
                    normalized_column = self._find_column_mapping_anchors_only(column)
                    if normalized_column and column != normalized_column:
                        pattern = r'\b' + re.escape(column) + r'\b(?!\s*\))'
                        sql = re.sub(pattern, normalized_column, sql, flags=re.IGNORECASE)
                        replacements_made += 1
                        print(f"   üîÑ Standalone mapping: '{column}' ‚Üí {normalized_column}")
                except Exception as e:
                    print(f"   ‚ùå Error mapping column '{column}': {e}")
            
            # PASO 3: AGREGAR COMILLAS
            try:
                sql = self._add_quotes_to_columns_enhanced(sql)
            except Exception as e:
                print(f"   ‚ùå Error adding quotes: {e}")
            
            print(f"   üì§ Output: {sql}")
            print(f"   üìä Reemplazos realizados: {replacements_made}")
            
            return sql
            
        except Exception as e:
            print(f"   ‚ùå CRITICAL ERROR in normalize_sql: {e}")
            print(f"   üìã Returning original SQL as fallback")
            return conceptual_sql


    def _find_column_mapping_anchors_only(self, column_name: str) -> Optional[str]:
        """
        üîç BUSCADOR ROBUSTO - MANEJA DIFERENTES ESTRUCTURAS DE ANCHORS
        """
        
        column_lower = column_name.lower()
        
        print(f"      üîç Searching for '{column_name}' in anchors...")
        
        # PASO 1: Buscar en dimension anchors
        if hasattr(self, 'dimension_anchors'):
            print(f"      üìç Checking dimension_anchors (type: {type(self.dimension_anchors)})")
            
            try:
                if isinstance(self.dimension_anchors, dict):
                    for anchor_key, anchor_data in self.dimension_anchors.items():
                        print(f"         üîç Checking anchor_key: '{anchor_key}' (data type: {type(anchor_data)})")
                        
                        # Manejar diferentes estructuras de anchor_data
                        if isinstance(anchor_data, dict):
                            # Estructura esperada: {"normalized_name": "Store", "variants": [...]}
                            variants = anchor_data.get('variants', [])
                            normalized = anchor_data.get('normalized_name', anchor_key)
                            
                            if isinstance(variants, list):
                                if column_lower in [v.lower() for v in variants]:
                                    print(f"      ‚úÖ Dimension match: '{column_name}' ‚Üí '{normalized}' (via variants)")
                                    return normalized
                            
                            # Tambi√©n verificar si la clave coincide directamente
                            if anchor_key.lower() == column_lower:
                                print(f"      ‚úÖ Dimension match: '{column_name}' ‚Üí '{normalized}' (direct key)")
                                return normalized
                                
                        elif isinstance(anchor_data, list):
                            # Estructura: [variant1, variant2, ...]
                            if column_lower in [v.lower() for v in anchor_data]:
                                normalized = anchor_key.title()  # Capitalizar clave como normalized
                                print(f"      ‚úÖ Dimension match: '{column_name}' ‚Üí '{normalized}' (list structure)")
                                return normalized
                                
                        elif isinstance(anchor_data, str):
                            # Estructura: "normalized_name"
                            if anchor_key.lower() == column_lower:
                                print(f"      ‚úÖ Dimension match: '{column_name}' ‚Üí '{anchor_data}' (string structure)")
                                return anchor_data
                                
                else:
                    print(f"      ‚ö†Ô∏è dimension_anchors is not a dict: {type(self.dimension_anchors)}")
                    
            except Exception as e:
                print(f"      ‚ùå Error processing dimension_anchors: {e}")
        
        # PASO 2: Buscar en metric anchors (misma l√≥gica robusta)
        if hasattr(self, 'metric_anchors'):
            print(f"      üìä Checking metric_anchors (type: {type(self.metric_anchors)})")
            
            try:
                if isinstance(self.metric_anchors, dict):
                    for anchor_key, anchor_data in self.metric_anchors.items():
                        print(f"         üîç Checking metric anchor_key: '{anchor_key}' (data type: {type(anchor_data)})")
                        
                        # Manejar diferentes estructuras de anchor_data
                        if isinstance(anchor_data, dict):
                            variants = anchor_data.get('variants', [])
                            normalized = anchor_data.get('normalized_name', anchor_key)
                            
                            if isinstance(variants, list):
                                if column_lower in [v.lower() for v in variants]:
                                    print(f"      ‚úÖ Metric match: '{column_name}' ‚Üí '{normalized}' (via variants)")
                                    return normalized
                            
                            if anchor_key.lower() == column_lower:
                                print(f"      ‚úÖ Metric match: '{column_name}' ‚Üí '{normalized}' (direct key)")
                                return normalized
                                
                        elif isinstance(anchor_data, list):
                            if column_lower in [v.lower() for v in anchor_data]:
                                normalized = anchor_key.title()
                                print(f"      ‚úÖ Metric match: '{column_name}' ‚Üí '{normalized}' (list structure)")
                                return normalized
                                
                        elif isinstance(anchor_data, str):
                            if anchor_key.lower() == column_lower:
                                print(f"      ‚úÖ Metric match: '{column_name}' ‚Üí '{anchor_data}' (string structure)")
                                return anchor_data
                                
                else:
                    print(f"      ‚ö†Ô∏è metric_anchors is not a dict: {type(self.metric_anchors)}")
                    
            except Exception as e:
                print(f"      ‚ùå Error processing metric_anchors: {e}")
        
        # PASO 3: Debug - mostrar contenido de anchors para diagn√≥stico
        print(f"      ‚ùå No mapping found for '{column_name}'")
        
        # Debug info para diagnosticar estructura
        if hasattr(self, 'dimension_anchors') and self.dimension_anchors:
            print(f"      üîç DEBUG - Sample dimension_anchors structure:")
            sample_keys = list(self.dimension_anchors.keys())[:3]  # Primeras 3 claves
            for key in sample_keys:
                print(f"         '{key}': {type(self.dimension_anchors[key])} = {self.dimension_anchors[key]}")
        
        return None


    def debug_anchors_structure(self):
        """
        üîç M√âTODO PARA DEBUGGEAR LA ESTRUCTURA DE ANCHORS
        Llamar este m√©todo para ver c√≥mo est√°n estructurados tus anchors
        """
        
        print(f"\nüîç DEBUGGING ANCHORS STRUCTURE:")
        
        if hasattr(self, 'dimension_anchors'):
            print(f"üìç DIMENSION_ANCHORS (type: {type(self.dimension_anchors)}):")
            if isinstance(self.dimension_anchors, dict):
                for i, (key, value) in enumerate(self.dimension_anchors.items()):
                    if i < 5:  # Solo mostrar primeros 5
                        print(f"   '{key}': {type(value)} = {value}")
                    elif i == 5:
                        print(f"   ... and {len(self.dimension_anchors) - 5} more")
                        break
            else:
                print(f"   ‚ö†Ô∏è Not a dict: {self.dimension_anchors}")
        
        if hasattr(self, 'metric_anchors'):
            print(f"üìä METRIC_ANCHORS (type: {type(self.metric_anchors)}):")
            if isinstance(self.metric_anchors, dict):
                for i, (key, value) in enumerate(self.metric_anchors.items()):
                    if i < 5:  # Solo mostrar primeros 5
                        print(f"   '{key}': {type(value)} = {value}")
                    elif i == 5:
                        print(f"   ... and {len(self.metric_anchors) - 5} more")
                        break
            else:
                print(f"   ‚ö†Ô∏è Not a dict: {self.metric_anchors}")


    def _find_standalone_columns(self, sql: str) -> List[str]:
        """
        üîç ENCUENTRA COLUMNAS QUE NO EST√ÅN DENTRO DE FUNCIONES
        """
        import re
        
        # Encontrar todas las palabras que podr√≠an ser columnas
        # Excluir palabras SQL reservadas y funciones
        sql_keywords = {
            'select', 'from', 'where', 'group', 'by', 'order', 'limit',
            'and', 'or', 'not', 'in', 'exists', 'between', 'like',
            'count', 'sum', 'max', 'min', 'avg', 'distinct',
            'datos', 'desc', 'asc'
        }
        
        # Encontrar palabras alfanum√©ricas que no est√°n dentro de funciones o comillas
        word_pattern = r'\b[a-zA-Z_][a-zA-Z0-9_]*\b'
        words = re.findall(word_pattern, sql)
        
        standalone_columns = []
        for word in words:
            if (word.lower() not in sql_keywords and 
                not word.startswith('"') and 
                not self._is_inside_function(word, sql)):
                standalone_columns.append(word)
        
        # Remover duplicados manteniendo orden
        seen = set()
        unique_columns = []
        for col in standalone_columns:
            if col not in seen:
                seen.add(col)
                unique_columns.append(col)
        
        return unique_columns


    def _is_inside_function(self, word: str, sql: str) -> bool:
        """
        üîç VERIFICA SI UNA PALABRA EST√Å DENTRO DE UNA FUNCI√ìN
        """
        import re
        
        # Buscar si la palabra est√° dentro de par√©ntesis de funci√≥n
        function_pattern = r'\w+\s*\([^)]*\b' + re.escape(word) + r'\b[^)]*\)'
        return bool(re.search(function_pattern, sql, re.IGNORECASE))


    def _add_quotes_to_columns_enhanced(self, sql: str) -> str:
        """
        üîß AGREGADOR DE COMILLAS DIN√ÅMICO - VERSI√ìN CORREGIDA
        Usa word boundaries para evitar reemplazos parciales
        """
        
        print(f"üîß AGREGANDO COMILLAS (Dynamic - Using Anchors):")
        print(f"   üì• Input: {sql}")
        
        import re
        result = sql
        replacements_made = 0
        
        # PASO 1: Obtener todas las columnas normalizadas de los anchors
        normalized_columns = set()
        
        # Extraer de dimension_anchors
        if hasattr(self, 'dimension_anchors') and isinstance(self.dimension_anchors, dict):
            for anchor_key, anchor_data in self.dimension_anchors.items():
                if isinstance(anchor_data, dict):
                    normalized_name = anchor_data.get('normalized_name', anchor_key)
                    normalized_columns.add(normalized_name)
                elif isinstance(anchor_data, str):
                    normalized_columns.add(anchor_data)
                else:
                    normalized_columns.add(anchor_key.title())
        
        # Extraer de metric_anchors
        if hasattr(self, 'metric_anchors') and isinstance(self.metric_anchors, dict):
            for anchor_key, anchor_data in self.metric_anchors.items():
                if isinstance(anchor_data, dict):
                    normalized_name = anchor_data.get('normalized_name', anchor_key)
                    normalized_columns.add(normalized_name)
                elif isinstance(anchor_data, str):
                    normalized_columns.add(anchor_data)
                else:
                    normalized_columns.add(anchor_key.title())
        
        print(f"   üìä Normalized columns from anchors: {sorted(normalized_columns)}")
        
        # PASO 2: Ordenar columnas por longitud (m√°s largas primero)
        # Esto evita que "Inventory" se procese antes que "Dead_Inventory"
        sorted_columns = sorted(normalized_columns, key=len, reverse=True)
        
        # PASO 3: Agregar comillas usando regex con word boundaries
        for column in sorted_columns:
            # Skip si ya tiene comillas
            if f'"{column}"' in result or f"'{column}'" in result:
                continue
            
            # Crear patr√≥n regex que busque la columna como palabra completa
            # \b funciona con letras/n√∫meros pero no con underscore al final
            # Por eso usamos lookahead/lookbehind m√°s complejos
            pattern = r'(?<!["\w])' + re.escape(column) + r'(?!["\w])'
            
            # Buscar todas las coincidencias
            matches = list(re.finditer(pattern, result))
            
            if matches:
                # Reemplazar de atr√°s hacia adelante para no afectar las posiciones
                for match in reversed(matches):
                    start, end = match.span()
                    # Verificar contexto para decidir si agregar comillas
                    context_before = result[max(0, start-10):start]
                    context_after = result[end:min(len(result), end+10)]
                    
                    # No agregar comillas si ya est√° entre comillas
                    if '"' in context_before[-1:] or '"' in context_after[:1]:
                        continue
                    
                    # No agregar comillas si est√° dentro de comillas simples (valores)
                    if "'" in context_before[-1:] or "'" in context_after[:1]:
                        continue
                    
                    # Reemplazar
                    result = result[:start] + f'"{column}"' + result[end:]
                    replacements_made += 1
                    print(f"      üìù Added quotes: {column} ‚Üí \"{column}\" at position {start}")
        
        # PASO 4: Verificaci√≥n final - asegurar que no haya patrones rotos
        broken_pattern = r'(\w+)_"(\w+)"'
        broken_matches = re.findall(broken_pattern, result)
        
        if broken_matches:
            print(f"   ‚ö†Ô∏è WARNING: Found broken patterns that need fixing:")
            for match in broken_matches:
                broken = f'{match[0]}_"{match[1]}"'
                fixed = f'"{match[0]}_{match[1]}"'
                result = result.replace(broken, fixed)
                print(f"      üîß Fixed: {broken} ‚Üí {fixed}")
                replacements_made += 1
        
        print(f"   üì§ Output: {result}")
        print(f"   üìä Reemplazos realizados: {replacements_made}")
        
        return result
                
        
    def extract_columns_from_sql(self, sql: str) -> List[str]:
        """
        Extrae nombres de columnas del SQL usando expresiones regulares
        Busca en SELECT, GROUP BY, ORDER BY, WHERE
        """
        columns = set()
        
        # Normalizar SQL para an√°lisis
        sql_clean = sql.replace('\n', ' ').replace('\t', ' ')
        sql_clean = re.sub(r'\s+', ' ', sql_clean).strip()
        
# PATR√ìN 1: SELECT columns (incluyendo funciones)
        # SELECT tienda, MAX(ventas), SUM(inventario) FROM...
        select_pattern = r'SELECT\s+(.*?)\s+FROM'
        select_match = re.search(select_pattern, sql_clean, re.IGNORECASE)
        if select_match:
            select_part = select_match.group(1)
            # Extraer columnas dentro de funciones y directas
            select_columns = self._extract_columns_from_select(select_part)
            columns.update(select_columns)
        
# PATR√ìN 2: GROUP BY columns
        # GROUP BY tienda, region
        group_by_pattern = r'GROUP\s+BY\s+(.*?)(?:\s+ORDER\s+BY|\s+HAVING|\s+LIMIT|\s*;|\s*$)'
        group_by_match = re.search(group_by_pattern, sql_clean, re.IGNORECASE)
        if group_by_match:
            group_by_part = group_by_match.group(1).strip()
            group_by_columns = [col.strip() for col in group_by_part.split(',')]
            columns.update(group_by_columns)
        
# PATR√ìN 3: ORDER BY columns (incluyendo funciones)
        # ORDER BY MAX(ventas) DESC, tienda ASC
        order_by_pattern = r'ORDER\s+BY\s+(.*?)(?:\s+LIMIT|\s*;|\s*$)'
        order_by_match = re.search(order_by_pattern, sql_clean, re.IGNORECASE)
        if order_by_match:
            order_by_part = order_by_match.group(1).strip()
            order_by_columns = self._extract_columns_from_order_by(order_by_part)
            columns.update(order_by_columns)
        
# PATR√ìN 4: WHERE conditions
        # WHERE tienda = 'valor' AND ventas > 100
        where_pattern = r'WHERE\s+(.*?)(?:\s+GROUP\s+BY|\s+ORDER\s+BY|\s+LIMIT|\s*;|\s*$)'
        where_match = re.search(where_pattern, sql_clean, re.IGNORECASE)
        if where_match:
            where_part = where_match.group(1).strip()
            where_columns = self._extract_columns_from_where(where_part)
            columns.update(where_columns)
        
        # Limpiar y filtrar columnas
        cleaned_columns = []
        for col in columns:
            col_clean = col.strip().strip(',').strip()
            if col_clean and col_clean.lower() not in ['desc', 'asc', 'and', 'or']:
                cleaned_columns.append(col_clean)
        
        return list(set(cleaned_columns))  # Eliminar duplicados
    
    
    def _extract_columns_from_select(self, select_part: str) -> Set[str]:
        """Extrae columnas de la parte SELECT, incluyendo funciones"""
        columns = set()
        
        # Dividir por comas, pero respetando par√©ntesis
        items = self._split_respecting_parentheses(select_part, ',')
        
        for item in items:
            item = item.strip()
            
            # Si contiene funci√≥n: MAX(ventas) ‚Üí extraer 'ventas'
            function_match = re.search(r'\w+\s*\(\s*([^)]+)\s*\)', item)
            if function_match:
                column_in_function = function_match.group(1)
                columns.add(column_in_function)
            else:
                # Columna simple: tienda
                if re.match(r'^\w+$', item):
                    columns.add(item)
        
        return columns
    
    
    def _extract_columns_from_order_by(self, order_by_part: str) -> Set[str]:
        """Extrae columnas de ORDER BY, incluyendo funciones"""
        columns = set()
        
        # Dividir por comas
        items = order_by_part.split(',')
        
        for item in items:
            item = item.strip()
            # Remover DESC/ASC
            item = re.sub(r'\s+(DESC|ASC)\s*$', '', item, flags=re.IGNORECASE).strip()
            
            # Si contiene funci√≥n: MAX(ventas) ‚Üí extraer 'ventas'
            function_match = re.search(r'\w+\s*\(\s*(\w+)\s*\)', item)
            if function_match:
                column_in_function = function_match.group(1)
                columns.add(column_in_function)
            else:
                # Columna simple
                if re.match(r'^\w+$', item):
                    columns.add(item)
        
        return columns
    
    
    def _extract_columns_from_where(self, where_part: str) -> Set[str]:
        """Extrae columnas de condiciones WHERE"""
        columns = set()
        
        # Buscar patrones: columna = valor, columna > valor, etc.
        column_patterns = [
            r'(\w+)\s*=\s*[\'"]?[\w\s]+[\'"]?',
            r'(\w+)\s*!=\s*[\'"]?[\w\s]+[\'"]?',
            r'(\w+)\s*>\s*[\d\.]+',
            r'(\w+)\s*<\s*[\d\.]+',
            r'(\w+)\s*>=\s*[\d\.]+',
            r'(\w+)\s*<=\s*[\d\.]+',
            r'(\w+)\s+BETWEEN\s+',
            r'(\w+)\s+IN\s*\(',
        ]
        
        for pattern in column_patterns:
            matches = re.findall(pattern, where_part, re.IGNORECASE)
            for match in matches:
                if isinstance(match, tuple):
                    columns.add(match[0])
                else:
                    columns.add(match)
        
        return columns
    
    
    def _split_respecting_parentheses(self, text: str, delimiter: str) -> List[str]:
        """Divide texto por delimitador respetando par√©ntesis"""
        parts = []
        current_part = ""
        paren_count = 0
        
        for char in text:
            if char == '(':
                paren_count += 1
            elif char == ')':
                paren_count -= 1
            elif char == delimiter and paren_count == 0:
                parts.append(current_part.strip())
                current_part = ""
                continue
            
            current_part += char
        
        if current_part.strip():
            parts.append(current_part.strip())
        
        return parts
    
    
    def find_column_mapping(self, conceptual_word: str) -> Optional[str]:
        """
        Busca el mapeo de una palabra conceptual a su anchor correspondiente
        
        Args:
            conceptual_word: Palabra conceptual (ej: 'tienda', 'ventas')
            
        Returns:
            Nombre del anchor (ej: 'store_name', 'Sell-Out') o None si no se encuentra
        """
        word_lower = conceptual_word.lower().strip()
        
        # Buscar en mapeo reverso
        if word_lower in self.reverse_mapping:
            mapping = self.reverse_mapping[word_lower]
            anchor_name = mapping['anchor']
            mapping_type = mapping['type']
            
            print(f"      üéØ '{conceptual_word}' encontrado en {mapping_type}: '{anchor_name}'")
            return anchor_name
        
        # Si no se encuentra, retornar la palabra original
        print(f"      ‚ùì '{conceptual_word}' no encontrado en anchors")
        return None
    
    
    def _replace_column_in_sql(self, sql: str, old_column: str, new_column: str) -> str:
        """
        Reemplaza todas las ocurrencias de una columna en el SQL
        Usa regex para evitar reemplazos parciales incorrectos
        """
        # Patr√≥n que busca la columna como palabra completa
        pattern = r'\b' + re.escape(old_column) + r'\b'
        
        # Reemplazar todas las ocurrencias
        new_sql = re.sub(pattern, new_column, sql, flags=re.IGNORECASE)
        
        return new_sql
    
    
    def get_mapping_statistics(self) -> Dict:
        """Devuelve estad√≠sticas de los mapeos cargados"""
        return {
            'total_dimension_anchors': len(self.dimension_anchors),
            'total_metric_anchors': len(self.metric_anchors),
            'total_reverse_mappings': len(self.reverse_mapping),
            'dimension_anchors': list(self.dimension_anchors.keys()),
            'metric_anchors': list(self.metric_anchors.keys())
        }
    
    
    def debug_column_extraction(self, sql: str):
        """M√©todo de debug para ver c√≥mo se extraen las columnas"""
        print(f"\nüîç DEBUG: Extracci√≥n de columnas")
        print(f"SQL: {sql}")
        
        columns = self.extract_columns_from_sql(sql)
        print(f"Columnas extra√≠das: {columns}")
        
        for column in columns:
            mapping = self.find_column_mapping(column)
            print(f"  {column} ‚Üí {mapping if mapping else 'Sin mapeo'}")



    def add_quotes_to_all_columns(regex_columns, sql: str) -> str:
        result_sql = sql
        quoted_columns = []

        for column in regex_columns:
            if column.startswith('"') and column.endswith('"'):
                continue

            quoted = f'"{column}"'
            esc = re.escape(column)

            pattern = rf'''
                (?<![\w"'])           # no precedido por letra, n√∫mero, _ o comilla
                {esc}                 # nombre completo de columna
                (?![\w"'])            # no seguido por letra, n√∫mero, _ o comilla
            '''

            if re.search(pattern, result_sql, re.IGNORECASE | re.VERBOSE):
                new_sql = re.sub(pattern, quoted, result_sql, flags=re.IGNORECASE | re.VERBOSE)
                if new_sql != result_sql:
                    quoted_columns.append(f"{column} ‚Üí {quoted}")
                    result_sql = new_sql

        return result_sql, quoted_columns




# ===================================================================
# -------------- ANALIZADOR SEM√ÅNTICO PRE-MAPEO ---------------------
# ===================================================================


class PreMappingSemanticAnalyzer:
    """
    Analiza sem√°ntica en palabras ORIGINALES del usuario
    ANTES de que se mapeen a palabras anchor
    """
    
    def __init__(self):
        # Sin diccionarios - solo reglas generales del espa√±ol
        pass
    
    def analyze_original_intent(self, original_tokens: List[str]) -> str:
        """
        üéØ AN√ÅLISIS PRINCIPAL: Detectar intenci√≥n en tokens ORIGINALES
        
        Args:
            original_tokens: Palabras exactas del usuario ANTES del mapeo
            
        Returns:
            'SUM' - para volumen total (plural context)
            'MAX' - para transacci√≥n individual (singular + magnitud)
            'DEFAULT' - usar configuraci√≥n por defecto
        """
        if not original_tokens:
            return 'DEFAULT'
        
        print(f"üîç AN√ÅLISIS PRE-MAPEO de tokens originales: {original_tokens}")
        
        # REGLA 1: Detectar contexto plural (indica volumen total)
        if self._has_plural_context(original_tokens):
            return 'SUM'
        
        # REGLA 2: Detectar contexto individual (indica transacci√≥n espec√≠fica)
        if self._has_individual_context(original_tokens):
            return 'MAX'
        
        return 'DEFAULT'
    
    def _has_plural_context(self, tokens: List[str]) -> bool:
        """Detectar contexto plural en palabras ORIGINALES"""
        
        for token in tokens:
            # REGLA MORFOL√ìGICA: Detectar plurales del espa√±ol
            if self._is_spanish_plural(token):
                print(f"   üìä PLURAL ORIGINAL detectado: '{token}' ‚Üí SUM")
                return True
        
        # REGLA CONTEXTUAL: Detectar cuantificadores de volumen
        volume_indicators = {'total', 'suma', 'conjunto', 'todos', 'todas', 'cantidad'}
        for token in tokens:
            if token.lower() in volume_indicators:
                print(f"   üìä CUANTIFICADOR de volumen: '{token}' ‚Üí SUM")
                return True
        
        return False
    
    def _has_individual_context(self, tokens: List[str]) -> bool:
        """Detectar contexto individual en palabras ORIGINALES"""
        
        # REGLA 1: Buscar palabras de magnitud
        magnitude_words = self._find_magnitude_words(tokens)
        if magnitude_words:
            
            # REGLA 2: Verificar que haya sustantivos singulares cerca
            singular_nouns = self._find_singular_nouns(tokens)
            if singular_nouns:
                print(f"   üéØ MAGNITUD + SINGULAR: {magnitude_words} + {singular_nouns} ‚Üí MAX")
                return True
            
            # REGLA 3: Si hay magnitud sin plural expl√≠cito, asumir individual
            if not self._has_explicit_plural(tokens):
                print(f"   üéØ MAGNITUD sin plural expl√≠cito: {magnitude_words} ‚Üí MAX")
                return True
        
        return False
    
    def _is_spanish_plural(self, word: str) -> bool:
        """Detectar plurales usando reglas morfol√≥gicas del espa√±ol"""
        if len(word) <= 2:
            return False
        
        word_lower = word.lower()
        
        # Excepciones comunes (palabras que terminan en 's' pero no son plurales)
        exceptions = {
            'mas', 'm√°s', 'menos', 'entonces', 'adem√°s', 'antes', 'despu√©s',
            'lunes', 'martes', 'mi√©rcoles', 'jueves', 'viernes', 'an√°lisis',
            'crisis', 'tesis', 'dosis', 'oasis'
        }
        
        if word_lower in exceptions:
            return False
        
        # REGLAS MORFOL√ìGICAS DEL ESPA√ëOL:
        
        # Regla 1: Terminaci√≥n en -s (no acentuada en la √∫ltima s√≠laba)
        if word_lower.endswith('s') and not word_lower.endswith('√°s'):
            return True
        
        # Regla 2: Terminaci√≥n en -es  
        if word_lower.endswith('es'):
            return True
        
        return False
    
    def _find_magnitude_words(self, tokens: List[str]) -> List[str]:
        """Encontrar palabras de magnitud en tokens originales"""
        magnitude_words = []
        
        for token in tokens:
            token_lower = token.lower()
            
            # DETECTAR POR MORFOLOG√çA (sufijos comunes)
            magnitude_suffixes = ['imo', 'ima', 'or', 'nde', 'to', 'ta']
            for suffix in magnitude_suffixes:
                if token_lower.endswith(suffix) and len(token) > 3:
                    magnitude_words.append(token)
                    break
            
            # DETECTAR PALABRAS ESPEC√çFICAS DE MAGNITUD
            magnitude_specific = {
                'grande', 'peque√±o', 'enorme', 'gigante', 'masivo',
                'alto', 'bajo', 'elevado', 'superior', 'inferior'
            }
            if token_lower in magnitude_specific:
                magnitude_words.append(token)
        
        return magnitude_words
    
    def _find_singular_nouns(self, tokens: List[str]) -> List[str]:
        """Encontrar sustantivos singulares en tokens originales"""
        singular_nouns = []
        
        for token in tokens:
            # HEUR√çSTICA: Palabras que NO son plurales y podr√≠an ser sustantivos
            if (not self._is_spanish_plural(token) and 
                len(token) > 3 and 
                token.lower() not in {'con', 'mas', 'm√°s', 'menor', 'mayor', 'para', 'por'}):
                singular_nouns.append(token)
        
        return singular_nouns
    
    def _has_explicit_plural(self, tokens: List[str]) -> bool:
        """Verificar si hay plurales expl√≠citos en los tokens"""
        return any(self._is_spanish_plural(token) for token in tokens)
        


        # ===============================
        # FUNCI√ìN PRINCIPAL DE EJECUCI√ìN 
        # ===============================f


# ------  "Funcion de ejecucion principal" -------

def main():
    """Funci√≥n Principal de Ejecuci√≥n"""
    try:
        parser = UnifiedNLPParser()
        
        # Prueba espec√≠fica del problema
        print("üö® PRUEBA ESPEC√çFICA: partner code Y")
        print("="*50)
        
        query = "cual es el partner code Y con mas ventas"
        result = parser.process_user_input(query)
        parser.display_unified_result(result)
        
        print("\nüöÄ Iniciando sesi√≥n interactiva...")
        parser.run_interactive_session()
        
    except Exception as e:
        print(f"‚ùå Error al inicializar: {e}")
        print("\nüîß POSIBLES SOLUCIONES:")
        print("1. Verifica que diccionario_sinonimos_2.py est√© en el mismo directorio")
        print("2. Revisa que todas las dependencias est√©n instaladas")
        print("3. Verifica la sintaxis del c√≥digo")

if __name__ == "__main__":
    main()



